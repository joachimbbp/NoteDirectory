{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import summarize\n",
    "import markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "note_path = \"/Users/joachimpfefferkorn/repos/daily_note_organizer/test_media/2024-09-03.md\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "Your max_length is set to 256, but your input_length is only 247. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=123)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLEANING\n",
      "max: 256, cleaned: 1751\n",
      " Main goal fMRI sequence from Open Neuro project which shows unblurred brain anatomy . This dataset is pretty good looking https://openneuroorgdatasetsdsversions .\n"
     ]
    }
   ],
   "source": [
    "print(summarize(note_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2838aa6f917a4aaeb402466473931475",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.49k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0027f65da2bf462e93ded0caaddc2eaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26c2ed2d94a94a22b04ac04caaad8565",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a735d48ad66410cbf6a303624282ea0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "734a00d95cc449cb8a12deb9f41c1cbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c437de49ce314b08ac6df69295f403f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b28f3f4c6524f348e98b78495be8fda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (851 > 512). Running this sequence through the model will result in indexing errors\n",
      "Your max_length is set to 1000, but your input_length is only 851. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=425)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'summary_text': 'Hugging Face has emerged as a prominent and innovative force in NLP . From its inception to its role in democratizing AI, the company has left an indelible mark on the industry . The name \"Hugging Face\" was chosen to reflect the company\\'s mission of making AI models more accessible and friendly to humans .'}]\n"
     ]
    }
   ],
   "source": [
    "#COPYPASTA\n",
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"Falconsai/text_summarization\")\n",
    "\n",
    "ARTICLE = \"\"\" \n",
    "Hugging Face: Revolutionizing Natural Language Processing\n",
    "Introduction\n",
    "In the rapidly evolving field of Natural Language Processing (NLP), Hugging Face has emerged as a prominent and innovative force. This article will explore the story and significance of Hugging Face, a company that has made remarkable contributions to NLP and AI as a whole. From its inception to its role in democratizing AI, Hugging Face has left an indelible mark on the industry.\n",
    "The Birth of Hugging Face\n",
    "Hugging Face was founded in 2016 by ClÃ©ment Delangue, Julien Chaumond, and Thomas Wolf. The name \"Hugging Face\" was chosen to reflect the company's mission of making AI models more accessible and friendly to humans, much like a comforting hug. Initially, they began as a chatbot company but later shifted their focus to NLP, driven by their belief in the transformative potential of this technology.\n",
    "Transformative Innovations\n",
    "Hugging Face is best known for its open-source contributions, particularly the \"Transformers\" library. This library has become the de facto standard for NLP and enables researchers, developers, and organizations to easily access and utilize state-of-the-art pre-trained language models, such as BERT, GPT-3, and more. These models have countless applications, from chatbots and virtual assistants to language translation and sentiment analysis.\n",
    "Key Contributions:\n",
    "1. **Transformers Library:** The Transformers library provides a unified interface for more than 50 pre-trained models, simplifying the development of NLP applications. It allows users to fine-tune these models for specific tasks, making it accessible to a wider audience.\n",
    "2. **Model Hub:** Hugging Face's Model Hub is a treasure trove of pre-trained models, making it simple for anyone to access, experiment with, and fine-tune models. Researchers and developers around the world can collaborate and share their models through this platform.\n",
    "3. **Hugging Face Transformers Community:** Hugging Face has fostered a vibrant online community where developers, researchers, and AI enthusiasts can share their knowledge, code, and insights. This collaborative spirit has accelerated the growth of NLP.\n",
    "Democratizing AI\n",
    "Hugging Face's most significant impact has been the democratization of AI and NLP. Their commitment to open-source development has made powerful AI models accessible to individuals, startups, and established organizations. This approach contrasts with the traditional proprietary AI model market, which often limits access to those with substantial resources.\n",
    "By providing open-source models and tools, Hugging Face has empowered a diverse array of users to innovate and create their own NLP applications. This shift has fostered inclusivity, allowing a broader range of voices to contribute to AI research and development.\n",
    "Industry Adoption\n",
    "The success and impact of Hugging Face are evident in its widespread adoption. Numerous companies and institutions, from startups to tech giants, leverage Hugging Face's technology for their AI applications. This includes industries as varied as healthcare, finance, and entertainment, showcasing the versatility of NLP and Hugging Face's contributions.\n",
    "Future Directions\n",
    "Hugging Face's journey is far from over. As of my last knowledge update in September 2021, the company was actively pursuing research into ethical AI, bias reduction in models, and more. Given their track record of innovation and commitment to the AI community, it is likely that they will continue to lead in ethical AI development and promote responsible use of NLP technologies.\n",
    "Conclusion\n",
    "Hugging Face's story is one of transformation, collaboration, and empowerment. Their open-source contributions have reshaped the NLP landscape and democratized access to AI. As they continue to push the boundaries of AI research, we can expect Hugging Face to remain at the forefront of innovation, contributing to a more inclusive and ethical AI future. Their journey reminds us that the power of open-source collaboration can lead to groundbreaking advancements in technology and bring AI within the reach of many.\n",
    "\"\"\"\n",
    "print(summarizer(ARTICLE, max_length=1000, min_length=30, do_sample=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "/var/folders/m4/rtcmkx_17lv03n9tvdf76ycr0000gn/T/ipykernel_95808/1362939293.py:15: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  text = ''.join(soup.findAll(text=True))\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1025 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1>To-Do</h1>\n",
      "<ul>\n",
      "<li>[x] Start Dev for [[Neurovolume]] in [[Docker]] file<ul>\n",
      "<li>[ ] If this takes a while to build, work on [[Introduction to Statistics]]</li>\n",
      "<li>[ ] Main goal: [[fMRI]] sequence from Open Neuro project which shows un-blurred brain anatomy</li>\n",
      "<li>This dataset is pretty good looking: https://openneuro.org/datasets/ds003548/versions/1.0.1<ul>\n",
      "<li>[ ] Use [[VDB]] color and emission to show activations, not a layered VDB</li>\n",
      "</ul>\n",
      "</li>\n",
      "</ul>\n",
      "</li>\n",
      "<li>[ ] Ask in Zulip about running/funding OSS stuff<ul>\n",
      "<li>What is the typical plan for running an open source project? I know from @filippo's <a href=\"https://words.filippo.io/full-time-maintainer/\">blogpost</a> that it is possible to build out a career, but he's pretty experienced and had a lot more projects under his belt. Is it the sort of thing that just helps getting jobs?</li>\n",
      "</ul>\n",
      "</li>\n",
      "</ul>\n",
      "<h1>Status as of Lunch</h1>\n",
      "<ul>\n",
      "<li>Dockerfile does work but needed setup within vscode. I wonder what the best way to maintain all of this with the two githubs linked? Perhaps a question for Zach Lipp</li>\n",
      "<li>Forgot to add <code>nilearn</code> to neurovolume deps,</li>\n",
      "</ul>\n",
      "<h1>To-Do Afternoon</h1>\n",
      "<ul>\n",
      "<li>[x] Add nilearn to docker file</li>\n",
      "<li>[x] Push these changes to openvdb_docker</li>\n",
      "<li>[ ] Continue building fMRI sequence in open neuro with this vdb integrated<ul>\n",
      "<li>[ ] 86 the blender scripts and binaries, integrate this into your code base</li>\n",
      "</ul>\n",
      "</li>\n",
      "<li>[ ] Follow up with Zach if/when the numpy support build works</li>\n",
      "</ul>\n",
      "<p>TRAOT\n",
      "throwing rocks at oncoming traffic</p>\n",
      "<h1>Notes</h1>\n",
      "<ul>\n",
      "<li>\n",
      "<p>[[Docker]] cp <code>docker cp ./some_file CONTAINER:/work</code></p>\n",
      "<ul>\n",
      "<li>https://docs.docker.com/reference/cli/docker/container/cp/</li>\n",
      "</ul>\n",
      "</li>\n",
      "<li>\n",
      "<p><code>nib.load()</code> indeed does not like <code>.nii.gz</code> files, despite the <a href=\"https://nipy.org/nibabel/gettingstarted.html\">documentation</a></p>\n",
      "</li>\n",
      "<li>Perhaps you didn't download correctly. Trying again with <code>openneuro download --snapshot 1.0.1 ds003548 ds003548-download/</code></li>\n",
      "<li>\n",
      "<p><strong>This is indeed the case, the new download command above worked perfectly!</strong></p>\n",
      "</li>\n",
      "<li>\n",
      "<p>Rebuilding container with numpy support, this kind of thing is a big-ish question for dev-ops re: Zach Zulip messages.</p>\n",
      "</li>\n",
      "</ul>\n",
      "[{'summary_text': 'li>[x] Start Dev for [[Neurovolume]] in [[Docker]] fileul> Li>p>Rebuilding container with numpy support, this kind of thing is a big-ish question for dev-ops re: Zach Zulip messages ./strong>/p>, /ul>, and status as of Lunch/h1> .'}]\n"
     ]
    }
   ],
   "source": [
    "summarizer = pipeline(\"summarization\", model=\"Falconsai/text_summarization\")\n",
    "\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "with open(note_path, 'r') as NOTE:\n",
    "    html = markdown.markdown(NOTE.read())\n",
    "\n",
    "    html = re.sub(r'<pre>(.*?)</pre>', ' ', html)\n",
    "    html = re.sub(r'<code>(.*?)</code >', ' ', html)\n",
    "\n",
    "    # extract text\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    text = ''.join(soup.findAll(text=True))\n",
    "    print(text)\n",
    "    \n",
    "    print(summarizer(text, max_length=1000, min_length=30, do_sample=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
