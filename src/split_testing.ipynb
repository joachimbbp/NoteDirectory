{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goals\n",
    "Models are either capped at `1024` or `512` tokens. To summarize our notes, let's split each note into appropriate sections by reading in the headers. Each section will be summarized separately,and those strings will then be concatenated together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import markdown\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "from functions import summarize, summarize_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_HEADER_DEPTH = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_note = \"/Users/joachimpfefferkorn/repos/daily_note_organizer/test_media/2023-10-17.md\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_md(md_content: str):\n",
    "    #TODO\n",
    "    # Remove hyperlinks\n",
    "    # Disregard edge cases, anything encapsulated in a code block\n",
    "    # use a look up table of strings to replace instead of this shadowed variable\n",
    "    clean_note = md_content\n",
    "    clean_note = clean_note.replace('- [x]', 'Completed:').replace('- [ ]','To Do:')\n",
    "    clean_note = clean_note.replace('[[', '').replace(']]','')\n",
    "    clean_note = clean_note.replace('![[', 'Image file:')\n",
    "    return clean_note\n",
    "\n",
    "def prepare_note(md_path):\n",
    "    with open(md_path, 'r') as note:\n",
    "        md_content = note.read()\n",
    "        cleaned_md = clean_md(md_content)\n",
    "        return cleaned_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_note = prepare_note(str(long_note))\n",
    "# print(type(prepared_note))\n",
    "#print(prepared_note)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_note_sections(note):\n",
    "    \"\"\"\n",
    "    Splits the note into preamble and body, returns a list to be used in splitting functions.\n",
    "    Preamble consists of everything before the first header, body is everything after the first header\n",
    "    \"\"\"\n",
    "    preamble = \"\"\n",
    "    for i, line in enumerate(note.split(\"\\n\")):\n",
    "        if line.startswith(\"# \" or \"## \" or \"### \" or \"#### \" or \"##### \" or \"###### \") == False: #TODO dry\n",
    "            preamble += line + \" \" #TODO why no newline chars here?\n",
    "\n",
    "        else:\n",
    "            body = note[len(preamble):]\n",
    "            break\n",
    "    return [preamble, body]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The longest header is: '#### '\n"
     ]
    }
   ],
   "source": [
    "#GPT copypasta\n",
    "def find_longest_header(text):\n",
    "    lines = text.splitlines()  # Split the text into lines\n",
    "    # Filter lines that start with \"#\" and are followed by a space after the \"#\" characters\n",
    "    headers = [line for line in lines if line.startswith('#') and line.lstrip('#').startswith(' ')]\n",
    "    \n",
    "    if not headers:\n",
    "        return None  # Return None if no headers are found\n",
    "    \n",
    "    # Find the longest header based on the number of \"#\" characters before the space\n",
    "    longest_header = max(headers, key=lambda header: len(header.split()[0]))\n",
    "    \n",
    "    # Return only the header part (sequence of \"#\" followed by space)\n",
    "    return longest_header.split(' ')[0] + ' '\n",
    "\n",
    "# Example usage\n",
    "text = \"\"\"\n",
    "# Header 1\n",
    "## Header 2\n",
    "### Header 3\n",
    "#### Header 4\n",
    "#thisgarbageisa really long hashtag\n",
    "# Short Header\n",
    "\"\"\"\n",
    "\n",
    "longest_header = find_longest_header(text)\n",
    "print(f\"The longest header is: '{longest_header}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joachimpfefferkorn/repos/daily_note_organizer/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2718 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "model_name = \"knkarthick/MEETING_SUMMARY\"\n",
    "summarizer = pipeline(\"summarization\", model=model_name)\n",
    "TOKENIZER = AutoTokenizer.from_pretrained(model_name)\n",
    "MAX_TOKENS = TOKENIZER.model_max_length\n",
    "\n",
    "#print(f\"Max Tokens: {max_tokens}\")\n",
    "\n",
    "# Tokenize the note\n",
    "num_tokens = len(TOKENIZER(prepared_note)[0]) #Index 0 is tokens, index 1 is the attention mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_subsections(header_positions, lines):\n",
    "    print(\"üè≠ Building Subsections\")\n",
    "    print(\"üëë Header positions: \", header_positions)\n",
    "    \n",
    "    output = []\n",
    "    for h, _ in enumerate(header_positions[:-1]):\n",
    "        start = header_positions[h]\n",
    "        end = header_positions[h+1]\n",
    "        subsection = ' '.join(lines[start:end])\n",
    "        print(\"üé¨ START\", start, \"üîö END\", end)\n",
    "        print(\"ü™Ö SUBSECTION: \\n\", subsection)\n",
    "        output.append(subsection)\n",
    "    return output\n",
    "\n",
    "def tokens_small_enough(sections):\n",
    "    largest_section_length = 0\n",
    "    for section in sections:\n",
    "        print(\"ü™ô SECTION to tokenize: \", section)\n",
    "        tokens = TOKENIZER(section)\n",
    "        num_tokens = len(tokens['input_ids'])\n",
    "        print(f\"üí∂ section has {num_tokens}\")\n",
    "        if num_tokens > largest_section_length:\n",
    "            largest_section_length = num_tokens\n",
    "    if largest_section_length > MAX_TOKENS:\n",
    "        print(f\"üêò Largest Token size is {largest_section_length}, which is Too Big (max tokens are {MAX_TOKENS})\")\n",
    "        return False\n",
    "    else:\n",
    "        print(f\"ü¶ã Largest Tokens Size is {largest_section_length}, which is Small Enough (max tokens are {MAX_TOKENS})\")\n",
    "        return True\n",
    "\n",
    "\n",
    "def split_at_header(body: str, header_hashes: str) -> tuple: #should return list of subnotes\n",
    "    print(f\"\\n‚õìÔ∏è‚Äçüí• Splitting headers starting at hashes üü©{header_hashes}üü©\")\n",
    "\n",
    "    header_positions = []\n",
    "    lines = []\n",
    "\n",
    "    for line in body.split(\"\\n\"):\n",
    "        lines += [line]\n",
    "    print(\"INITIAL LINES:\", len(lines))\n",
    "\n",
    "\n",
    "    for length, hash in enumerate(header_hashes):\n",
    "        header = header_hashes[:length+1] + ' '\n",
    "        print(f\"üóø header: üü©{header}üü©\")\n",
    "        for linenum, line in enumerate(lines):\n",
    "            if line.startswith(header):\n",
    "                header_positions += [linenum]\n",
    "        subsections = build_subsections(header_positions, lines)\n",
    "        if tokens_small_enough(subsections):\n",
    "            break\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    return subsections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚õëÔ∏è 2718 is greater than 1024, we need to split up this note\n",
      "üèÑ Longest header: üü©##### üü©\n",
      "\n",
      "‚õìÔ∏è‚Äçüí• Splitting headers starting at hashes üü©#####üü©\n",
      "INITIAL LINES: 199\n",
      "üóø header: üü©# üü©\n",
      "üè≠ Building Subsections\n",
      "üëë Header positions:  [0, 13, 186]\n",
      "üé¨ START 0 üîö END 13\n",
      "ü™Ö SUBSECTION: \n",
      " # Afternoon To-Do #meta #todoList  ## Priority Completed: Complete Python module Completed: Complete Bias and Variance ## Bonus/tomorrow: Completed: Note cleanup with Aliases (see Career and Study To - Do) To Do: Linear Algebra Note Migration Completed: Integrate with this daily note \t\tTo Do: Training, Test, and Dev Sets \t\tTo Do: regularization (#merge and integrate) \t\tTo Do: Cost and Loss Completed: Big overhaul of Bias and Variance with regards to todays notes. \n",
      "üé¨ START 13 üîö END 186\n",
      "ü™Ö SUBSECTION: \n",
      " # Machine Learning Specialization Notes: These notes will be integrated into other notes but kept here in their entirety. ## Bias and Variance Machine Learning Specialization Advanced Machine Learning Algorithms, Week 3  Models almost never work the first time you try them out. Let's see how we can fix them.  ### Diagnosing Bias/Variance See Bias and Variance  If you have more features, you can't visualize Bias and Variance  A more systematic way to see if you have high bias or high variance is to look at the performance of the algorithm on the Training, Test, and Dev Sets|training set and dev set  A characteristic of a High Bias (under-fit) model is that $Jtrain$ (cost of the Training, Test, and Dev Sets|training set) is high. A characteristic of a High Variance (overfit) model is that $J_{cv}$ is high but $J_{train}$ is low. #### Slide !Screenshot 2023-10-17 at 1.26.00 PM.png  ### Understanding Bias and Variance Take degree of polynomial $d$ As $d$ increases, that is to say we add degrees to the polynomial $J_{train}({\\vec{w},b})$  will decrease. $J_{cv}(\\vec{w},b)$ dips towards the middle, showing the ideal degree for the polynomial.  #### Slide !Screenshot 2023-10-17 at 1.29.21 PM.png  ### Technique for Diagnosing Bias and Variance How do you tell if your algorithm has a high bias or variance problem?  One key takeaway is that high bias means it is not even doing well on the training set and high variance means it does much worse on the cross validation set and the training set. #### High Bias (under-fit) In an under-fit model $J_{train}$ will be high and $J_{train} \\approx J_{cv}$  #### High Variance (overfit) In an over-fit model $J_{cv} \\gg J_{train}$ and $J_{train}$ will be low. #### High Bias and High Variance In a model with both high bias and high variance $J_{train}$ will be high and $J_{cv} \\gg J_{train}$  This is rare, and doesn't really happen for linear models with one $d$, but it does happen. This is when it overfits for some part of the input and under-fits for another part of the input. #### Slide !Screenshot 2023-10-17 at 1.38.51 PM.png  ## Regularization and bias/variance ### Linear Regression with Regularization Regularized Linear Regression model: Lambda $\\lambda$ is the regularization parameter (Big #merge and cleanup needed here in the regularization, Regularized Linear Regression, cost, Cost and Loss, loss, Loss Function, etc. Aliases are needed in many of these)  Take this model: $$f_{\\vec{w},b}(x)=w_1+w_2x^2 + w_4x^4+b$$ With this regularization|regularized Cost and Loss $$J(\\vec{w},b)=\\frac{1}{2m} \\sum_{i=1}^{m}(f_{\\vec{w},{b}}(\\vec{x}^{(i)}-y^{(i)})^2+\\frac{\\lambda}{2m} \\sum_{j=1}^nw_j^2$$  If lambda is large, say $\\lambda = 10,000$ than $w_1 \\approx 0, W_2 \\approx 0$ and $f_{\\vec{x},b}\\vec{x} \\approx b$, thus the model creates a flat line. This is underfit, $J_{train}()  On the other hand, if we set $\\lambda=0$, then we have a forth order polynomial with no regularization. We end up with a very overfit curve.  So, how do we find a good value for $\\lambda$? ##### Slide !Screenshot 2023-10-17 at 3.56.53 PM.png ### Choosing the Regularization parameter $\\lambda$   This will be similar to choosing $d$ with cross validation Try multiple values for $\\lambda$ and then choose the option with the lowest cost. ### Slide !Screenshot 2023-10-17 at 4.15.27 PM.png ### Bias and Variance as a Function of regularization parameter $\\lambda$  Cross Validation tries out many versions of $\\lambda$ and then chooses the one with the lowest cost. Cross validation will help find us a a good value of $d$ as well as $\\lambda$  #### Slide !Screenshot 2023-10-17 at 4.14.07 PM.png   ## Establishing A Baseline Level of Performance ### Speech Recognition example Job is to take in audio and output the text of what a person is saying  Training error $J_{train}$ is percentage of audio clips that the program does not transcribe correctly in it's entirety. Lets say: \tHuman Level Performance: 10.6 \t$J_{train}$: 10.8% \t$J_{cv}$: 14.8%  Why is human level error so high? There is lots of noise in the audio. It seems unfair to expect a learning algorithm to do much better. It is thus is more useful to measure the training error against the human error. So, looking at these results, $J_{train}$ is only 0.2$ higher than the human level performance whereas $J_{cv}$ is a full 4.2% higher. We can thus conclude that this algorithm has more of a variance problem than a bias problem. ### Establishing a baseline level of performance What is the level of error you can reasonably hope to get to? - Human level performance - Competing algorithms performance - Guess based on experience ### Bias Variance Examples Gap between baseline and training error shows high bias. A gap between training error and cross validation error shows high variance.  If your goal is perfection, the baseline would be zero. But for a lot of real world examples, like audio recognition, there is a lot of noise in the data, so you need a higher baseline.  If there is a gap between all three is high you have both high bias and high variance. #### Slide !Screenshot 2023-10-17 at 4.42.49 PM.png   ## Learning Curves  Noted in Learning Curves ### Overview Learning curves help understand how your learning algorithm is doing as a function of the amount of experience it has. Experience being the number of training examples it has.  The bigger the training set the harder it is to fit all examples perfectly. Thus; as the training set increases so does the training error $J_{train}(\\vec{w},b)$   Plotting a learning curve by training different models based on different subsets of training data is computationally expensive, so in practice it isn't done that often. But, it's a good mental visualization. #### Slides !Screenshot 2023-10-17 at 5.40.04 PM.png    ### High Bias Example If a learning algorithm suffers from high bias, getting more training data will not (by itself) help that much. #### Slide for High Bias !Screenshot 2023-10-17 at 5.44.29 PM.png  ### High Variance Example If a learning algorithm suffers from high variance, getting more training data is likely to help. #### Slide for High Variance !Screenshot 2023-10-17 at 5.49.45 PM.png    ## Deciding what to try next revisited ### Examples when Debugging an Algorithm: Get more training examples: fixes high variance Try smaller set of features: fixes high variance Try getting additional features: fixes high bias \tExamples; an algorithm that lacks information wont even do well on the training set Adding polynomial features ($x_1^2,X_2^2, etc$): Fixes high bias Decreasing $\\lambda$ Fixes high bias Increasing $\\lambda$ Fixes high variance \tForces the algorithm to force a smoother function.  **Note!** Don't randomly throw away training examples just to fix a high bias problem. ### Takeaway #merge with Bias and Variance ? If your algorithm has high variance, try simplifying your model or getting more training data. Simplification can mean a smaller set of features or an increased regularization If your algorithm has high bias, that is to say its not even doing well on the training set, you mainly need to make your model more powerful and flexible to fit more complex functions. To do so you can give it additional features, add polynomial features, or decrease $\\lambda$   ## Bias/Variance and Neural Networks ### The bias variance tradeoff Simple model = high bias Complex model = high variance Before neural networks, we had to worry about balancing this complexity between bias and variance. With neural networks we now are mostly worried about high variance.  ### Neural Networks and bias variance Large Neural Network|neural networks are low bias machines. If you make your neural network large enough you can almost always fit your training set well. #### Recipe for decreasing bias with a neural network 1. Train a neural network 2.  If the training set error $J_{train}(\\vec{w},b)$ is high relative to your baseline, increase the size of the neural network by adding hidden layers. 3. Once $J_{train}(\\vec{w},b)$ is low enough, see if it does well on the cross validation set 4.  If the cross validation set $J_{cv}(\\vec{w},b)$ is too high, add more data, then test again from step 2. 5. Repeat until $J_{cv}(\\vec{w},b)$  is low enough for your liking. ##### Slide Illustration !Screenshot 2023-10-17 at 6.38.46 PM.png ### Limitations and Notes Bigger networks are restricted by your computing power, data is restricted to the amount of data you have.  Sometimes you will pingpong back between high bias and high variance as you move through this recipe and develop a machine learning algorithm. Use these observations to shape what you do next in the process.  ### Neural Networks and Regularization A large neural network will usually do as well or better than a smaller one so long as regularization is chosen appropriately. Of course, larger neural networks are more computationally expensive. ### Neural Network Regularization #important  #function and TensorFlow implementation: !Screenshot 2023-10-17 at 6.46.28 PM.png*Note usually don't regularize B, it doesn't really affect anything*\n",
      "ü™ô SECTION to tokenize:  # Afternoon To-Do #meta #todoList  ## Priority Completed: Complete Python module Completed: Complete Bias and Variance ## Bonus/tomorrow: Completed: Note cleanup with Aliases (see Career and Study To - Do) To Do: Linear Algebra Note Migration Completed: Integrate with this daily note \t\tTo Do: Training, Test, and Dev Sets \t\tTo Do: regularization (#merge and integrate) \t\tTo Do: Cost and Loss Completed: Big overhaul of Bias and Variance with regards to todays notes. \n",
      "üí∂ section has 122\n",
      "ü™ô SECTION to tokenize:  # Machine Learning Specialization Notes: These notes will be integrated into other notes but kept here in their entirety. ## Bias and Variance Machine Learning Specialization Advanced Machine Learning Algorithms, Week 3  Models almost never work the first time you try them out. Let's see how we can fix them.  ### Diagnosing Bias/Variance See Bias and Variance  If you have more features, you can't visualize Bias and Variance  A more systematic way to see if you have high bias or high variance is to look at the performance of the algorithm on the Training, Test, and Dev Sets|training set and dev set  A characteristic of a High Bias (under-fit) model is that $Jtrain$ (cost of the Training, Test, and Dev Sets|training set) is high. A characteristic of a High Variance (overfit) model is that $J_{cv}$ is high but $J_{train}$ is low. #### Slide !Screenshot 2023-10-17 at 1.26.00 PM.png  ### Understanding Bias and Variance Take degree of polynomial $d$ As $d$ increases, that is to say we add degrees to the polynomial $J_{train}({\\vec{w},b})$  will decrease. $J_{cv}(\\vec{w},b)$ dips towards the middle, showing the ideal degree for the polynomial.  #### Slide !Screenshot 2023-10-17 at 1.29.21 PM.png  ### Technique for Diagnosing Bias and Variance How do you tell if your algorithm has a high bias or variance problem?  One key takeaway is that high bias means it is not even doing well on the training set and high variance means it does much worse on the cross validation set and the training set. #### High Bias (under-fit) In an under-fit model $J_{train}$ will be high and $J_{train} \\approx J_{cv}$  #### High Variance (overfit) In an over-fit model $J_{cv} \\gg J_{train}$ and $J_{train}$ will be low. #### High Bias and High Variance In a model with both high bias and high variance $J_{train}$ will be high and $J_{cv} \\gg J_{train}$  This is rare, and doesn't really happen for linear models with one $d$, but it does happen. This is when it overfits for some part of the input and under-fits for another part of the input. #### Slide !Screenshot 2023-10-17 at 1.38.51 PM.png  ## Regularization and bias/variance ### Linear Regression with Regularization Regularized Linear Regression model: Lambda $\\lambda$ is the regularization parameter (Big #merge and cleanup needed here in the regularization, Regularized Linear Regression, cost, Cost and Loss, loss, Loss Function, etc. Aliases are needed in many of these)  Take this model: $$f_{\\vec{w},b}(x)=w_1+w_2x^2 + w_4x^4+b$$ With this regularization|regularized Cost and Loss $$J(\\vec{w},b)=\\frac{1}{2m} \\sum_{i=1}^{m}(f_{\\vec{w},{b}}(\\vec{x}^{(i)}-y^{(i)})^2+\\frac{\\lambda}{2m} \\sum_{j=1}^nw_j^2$$  If lambda is large, say $\\lambda = 10,000$ than $w_1 \\approx 0, W_2 \\approx 0$ and $f_{\\vec{x},b}\\vec{x} \\approx b$, thus the model creates a flat line. This is underfit, $J_{train}()  On the other hand, if we set $\\lambda=0$, then we have a forth order polynomial with no regularization. We end up with a very overfit curve.  So, how do we find a good value for $\\lambda$? ##### Slide !Screenshot 2023-10-17 at 3.56.53 PM.png ### Choosing the Regularization parameter $\\lambda$   This will be similar to choosing $d$ with cross validation Try multiple values for $\\lambda$ and then choose the option with the lowest cost. ### Slide !Screenshot 2023-10-17 at 4.15.27 PM.png ### Bias and Variance as a Function of regularization parameter $\\lambda$  Cross Validation tries out many versions of $\\lambda$ and then chooses the one with the lowest cost. Cross validation will help find us a a good value of $d$ as well as $\\lambda$  #### Slide !Screenshot 2023-10-17 at 4.14.07 PM.png   ## Establishing A Baseline Level of Performance ### Speech Recognition example Job is to take in audio and output the text of what a person is saying  Training error $J_{train}$ is percentage of audio clips that the program does not transcribe correctly in it's entirety. Lets say: \tHuman Level Performance: 10.6 \t$J_{train}$: 10.8% \t$J_{cv}$: 14.8%  Why is human level error so high? There is lots of noise in the audio. It seems unfair to expect a learning algorithm to do much better. It is thus is more useful to measure the training error against the human error. So, looking at these results, $J_{train}$ is only 0.2$ higher than the human level performance whereas $J_{cv}$ is a full 4.2% higher. We can thus conclude that this algorithm has more of a variance problem than a bias problem. ### Establishing a baseline level of performance What is the level of error you can reasonably hope to get to? - Human level performance - Competing algorithms performance - Guess based on experience ### Bias Variance Examples Gap between baseline and training error shows high bias. A gap between training error and cross validation error shows high variance.  If your goal is perfection, the baseline would be zero. But for a lot of real world examples, like audio recognition, there is a lot of noise in the data, so you need a higher baseline.  If there is a gap between all three is high you have both high bias and high variance. #### Slide !Screenshot 2023-10-17 at 4.42.49 PM.png   ## Learning Curves  Noted in Learning Curves ### Overview Learning curves help understand how your learning algorithm is doing as a function of the amount of experience it has. Experience being the number of training examples it has.  The bigger the training set the harder it is to fit all examples perfectly. Thus; as the training set increases so does the training error $J_{train}(\\vec{w},b)$   Plotting a learning curve by training different models based on different subsets of training data is computationally expensive, so in practice it isn't done that often. But, it's a good mental visualization. #### Slides !Screenshot 2023-10-17 at 5.40.04 PM.png    ### High Bias Example If a learning algorithm suffers from high bias, getting more training data will not (by itself) help that much. #### Slide for High Bias !Screenshot 2023-10-17 at 5.44.29 PM.png  ### High Variance Example If a learning algorithm suffers from high variance, getting more training data is likely to help. #### Slide for High Variance !Screenshot 2023-10-17 at 5.49.45 PM.png    ## Deciding what to try next revisited ### Examples when Debugging an Algorithm: Get more training examples: fixes high variance Try smaller set of features: fixes high variance Try getting additional features: fixes high bias \tExamples; an algorithm that lacks information wont even do well on the training set Adding polynomial features ($x_1^2,X_2^2, etc$): Fixes high bias Decreasing $\\lambda$ Fixes high bias Increasing $\\lambda$ Fixes high variance \tForces the algorithm to force a smoother function.  **Note!** Don't randomly throw away training examples just to fix a high bias problem. ### Takeaway #merge with Bias and Variance ? If your algorithm has high variance, try simplifying your model or getting more training data. Simplification can mean a smaller set of features or an increased regularization If your algorithm has high bias, that is to say its not even doing well on the training set, you mainly need to make your model more powerful and flexible to fit more complex functions. To do so you can give it additional features, add polynomial features, or decrease $\\lambda$   ## Bias/Variance and Neural Networks ### The bias variance tradeoff Simple model = high bias Complex model = high variance Before neural networks, we had to worry about balancing this complexity between bias and variance. With neural networks we now are mostly worried about high variance.  ### Neural Networks and bias variance Large Neural Network|neural networks are low bias machines. If you make your neural network large enough you can almost always fit your training set well. #### Recipe for decreasing bias with a neural network 1. Train a neural network 2.  If the training set error $J_{train}(\\vec{w},b)$ is high relative to your baseline, increase the size of the neural network by adding hidden layers. 3. Once $J_{train}(\\vec{w},b)$ is low enough, see if it does well on the cross validation set 4.  If the cross validation set $J_{cv}(\\vec{w},b)$ is too high, add more data, then test again from step 2. 5. Repeat until $J_{cv}(\\vec{w},b)$  is low enough for your liking. ##### Slide Illustration !Screenshot 2023-10-17 at 6.38.46 PM.png ### Limitations and Notes Bigger networks are restricted by your computing power, data is restricted to the amount of data you have.  Sometimes you will pingpong back between high bias and high variance as you move through this recipe and develop a machine learning algorithm. Use these observations to shape what you do next in the process.  ### Neural Networks and Regularization A large neural network will usually do as well or better than a smaller one so long as regularization is chosen appropriately. Of course, larger neural networks are more computationally expensive. ### Neural Network Regularization #important  #function and TensorFlow implementation: !Screenshot 2023-10-17 at 6.46.28 PM.png*Note usually don't regularize B, it doesn't really affect anything*\n",
      "üí∂ section has 2323\n",
      "üêò Largest Token size is 2323, which is Too Big (max tokens are 1024)\n",
      "üóø header: üü©## üü©\n",
      "üè≠ Building Subsections\n",
      "üëë Header positions:  [0, 13, 186, 2, 5, 15, 56, 86, 116, 142, 159, 187, 191]\n",
      "üé¨ START 0 üîö END 13\n",
      "ü™Ö SUBSECTION: \n",
      " # Afternoon To-Do #meta #todoList  ## Priority Completed: Complete Python module Completed: Complete Bias and Variance ## Bonus/tomorrow: Completed: Note cleanup with Aliases (see Career and Study To - Do) To Do: Linear Algebra Note Migration Completed: Integrate with this daily note \t\tTo Do: Training, Test, and Dev Sets \t\tTo Do: regularization (#merge and integrate) \t\tTo Do: Cost and Loss Completed: Big overhaul of Bias and Variance with regards to todays notes. \n",
      "üé¨ START 13 üîö END 186\n",
      "ü™Ö SUBSECTION: \n",
      " # Machine Learning Specialization Notes: These notes will be integrated into other notes but kept here in their entirety. ## Bias and Variance Machine Learning Specialization Advanced Machine Learning Algorithms, Week 3  Models almost never work the first time you try them out. Let's see how we can fix them.  ### Diagnosing Bias/Variance See Bias and Variance  If you have more features, you can't visualize Bias and Variance  A more systematic way to see if you have high bias or high variance is to look at the performance of the algorithm on the Training, Test, and Dev Sets|training set and dev set  A characteristic of a High Bias (under-fit) model is that $Jtrain$ (cost of the Training, Test, and Dev Sets|training set) is high. A characteristic of a High Variance (overfit) model is that $J_{cv}$ is high but $J_{train}$ is low. #### Slide !Screenshot 2023-10-17 at 1.26.00 PM.png  ### Understanding Bias and Variance Take degree of polynomial $d$ As $d$ increases, that is to say we add degrees to the polynomial $J_{train}({\\vec{w},b})$  will decrease. $J_{cv}(\\vec{w},b)$ dips towards the middle, showing the ideal degree for the polynomial.  #### Slide !Screenshot 2023-10-17 at 1.29.21 PM.png  ### Technique for Diagnosing Bias and Variance How do you tell if your algorithm has a high bias or variance problem?  One key takeaway is that high bias means it is not even doing well on the training set and high variance means it does much worse on the cross validation set and the training set. #### High Bias (under-fit) In an under-fit model $J_{train}$ will be high and $J_{train} \\approx J_{cv}$  #### High Variance (overfit) In an over-fit model $J_{cv} \\gg J_{train}$ and $J_{train}$ will be low. #### High Bias and High Variance In a model with both high bias and high variance $J_{train}$ will be high and $J_{cv} \\gg J_{train}$  This is rare, and doesn't really happen for linear models with one $d$, but it does happen. This is when it overfits for some part of the input and under-fits for another part of the input. #### Slide !Screenshot 2023-10-17 at 1.38.51 PM.png  ## Regularization and bias/variance ### Linear Regression with Regularization Regularized Linear Regression model: Lambda $\\lambda$ is the regularization parameter (Big #merge and cleanup needed here in the regularization, Regularized Linear Regression, cost, Cost and Loss, loss, Loss Function, etc. Aliases are needed in many of these)  Take this model: $$f_{\\vec{w},b}(x)=w_1+w_2x^2 + w_4x^4+b$$ With this regularization|regularized Cost and Loss $$J(\\vec{w},b)=\\frac{1}{2m} \\sum_{i=1}^{m}(f_{\\vec{w},{b}}(\\vec{x}^{(i)}-y^{(i)})^2+\\frac{\\lambda}{2m} \\sum_{j=1}^nw_j^2$$  If lambda is large, say $\\lambda = 10,000$ than $w_1 \\approx 0, W_2 \\approx 0$ and $f_{\\vec{x},b}\\vec{x} \\approx b$, thus the model creates a flat line. This is underfit, $J_{train}()  On the other hand, if we set $\\lambda=0$, then we have a forth order polynomial with no regularization. We end up with a very overfit curve.  So, how do we find a good value for $\\lambda$? ##### Slide !Screenshot 2023-10-17 at 3.56.53 PM.png ### Choosing the Regularization parameter $\\lambda$   This will be similar to choosing $d$ with cross validation Try multiple values for $\\lambda$ and then choose the option with the lowest cost. ### Slide !Screenshot 2023-10-17 at 4.15.27 PM.png ### Bias and Variance as a Function of regularization parameter $\\lambda$  Cross Validation tries out many versions of $\\lambda$ and then chooses the one with the lowest cost. Cross validation will help find us a a good value of $d$ as well as $\\lambda$  #### Slide !Screenshot 2023-10-17 at 4.14.07 PM.png   ## Establishing A Baseline Level of Performance ### Speech Recognition example Job is to take in audio and output the text of what a person is saying  Training error $J_{train}$ is percentage of audio clips that the program does not transcribe correctly in it's entirety. Lets say: \tHuman Level Performance: 10.6 \t$J_{train}$: 10.8% \t$J_{cv}$: 14.8%  Why is human level error so high? There is lots of noise in the audio. It seems unfair to expect a learning algorithm to do much better. It is thus is more useful to measure the training error against the human error. So, looking at these results, $J_{train}$ is only 0.2$ higher than the human level performance whereas $J_{cv}$ is a full 4.2% higher. We can thus conclude that this algorithm has more of a variance problem than a bias problem. ### Establishing a baseline level of performance What is the level of error you can reasonably hope to get to? - Human level performance - Competing algorithms performance - Guess based on experience ### Bias Variance Examples Gap between baseline and training error shows high bias. A gap between training error and cross validation error shows high variance.  If your goal is perfection, the baseline would be zero. But for a lot of real world examples, like audio recognition, there is a lot of noise in the data, so you need a higher baseline.  If there is a gap between all three is high you have both high bias and high variance. #### Slide !Screenshot 2023-10-17 at 4.42.49 PM.png   ## Learning Curves  Noted in Learning Curves ### Overview Learning curves help understand how your learning algorithm is doing as a function of the amount of experience it has. Experience being the number of training examples it has.  The bigger the training set the harder it is to fit all examples perfectly. Thus; as the training set increases so does the training error $J_{train}(\\vec{w},b)$   Plotting a learning curve by training different models based on different subsets of training data is computationally expensive, so in practice it isn't done that often. But, it's a good mental visualization. #### Slides !Screenshot 2023-10-17 at 5.40.04 PM.png    ### High Bias Example If a learning algorithm suffers from high bias, getting more training data will not (by itself) help that much. #### Slide for High Bias !Screenshot 2023-10-17 at 5.44.29 PM.png  ### High Variance Example If a learning algorithm suffers from high variance, getting more training data is likely to help. #### Slide for High Variance !Screenshot 2023-10-17 at 5.49.45 PM.png    ## Deciding what to try next revisited ### Examples when Debugging an Algorithm: Get more training examples: fixes high variance Try smaller set of features: fixes high variance Try getting additional features: fixes high bias \tExamples; an algorithm that lacks information wont even do well on the training set Adding polynomial features ($x_1^2,X_2^2, etc$): Fixes high bias Decreasing $\\lambda$ Fixes high bias Increasing $\\lambda$ Fixes high variance \tForces the algorithm to force a smoother function.  **Note!** Don't randomly throw away training examples just to fix a high bias problem. ### Takeaway #merge with Bias and Variance ? If your algorithm has high variance, try simplifying your model or getting more training data. Simplification can mean a smaller set of features or an increased regularization If your algorithm has high bias, that is to say its not even doing well on the training set, you mainly need to make your model more powerful and flexible to fit more complex functions. To do so you can give it additional features, add polynomial features, or decrease $\\lambda$   ## Bias/Variance and Neural Networks ### The bias variance tradeoff Simple model = high bias Complex model = high variance Before neural networks, we had to worry about balancing this complexity between bias and variance. With neural networks we now are mostly worried about high variance.  ### Neural Networks and bias variance Large Neural Network|neural networks are low bias machines. If you make your neural network large enough you can almost always fit your training set well. #### Recipe for decreasing bias with a neural network 1. Train a neural network 2.  If the training set error $J_{train}(\\vec{w},b)$ is high relative to your baseline, increase the size of the neural network by adding hidden layers. 3. Once $J_{train}(\\vec{w},b)$ is low enough, see if it does well on the cross validation set 4.  If the cross validation set $J_{cv}(\\vec{w},b)$ is too high, add more data, then test again from step 2. 5. Repeat until $J_{cv}(\\vec{w},b)$  is low enough for your liking. ##### Slide Illustration !Screenshot 2023-10-17 at 6.38.46 PM.png ### Limitations and Notes Bigger networks are restricted by your computing power, data is restricted to the amount of data you have.  Sometimes you will pingpong back between high bias and high variance as you move through this recipe and develop a machine learning algorithm. Use these observations to shape what you do next in the process.  ### Neural Networks and Regularization A large neural network will usually do as well or better than a smaller one so long as regularization is chosen appropriately. Of course, larger neural networks are more computationally expensive. ### Neural Network Regularization #important  #function and TensorFlow implementation: !Screenshot 2023-10-17 at 6.46.28 PM.png*Note usually don't regularize B, it doesn't really affect anything*\n",
      "üé¨ START 186 üîö END 2\n",
      "ü™Ö SUBSECTION: \n",
      " \n",
      "üé¨ START 2 üîö END 5\n",
      "ü™Ö SUBSECTION: \n",
      " ## Priority Completed: Complete Python module Completed: Complete Bias and Variance\n",
      "üé¨ START 5 üîö END 15\n",
      "ü™Ö SUBSECTION: \n",
      " ## Bonus/tomorrow: Completed: Note cleanup with Aliases (see Career and Study To - Do) To Do: Linear Algebra Note Migration Completed: Integrate with this daily note \t\tTo Do: Training, Test, and Dev Sets \t\tTo Do: regularization (#merge and integrate) \t\tTo Do: Cost and Loss Completed: Big overhaul of Bias and Variance with regards to todays notes.  # Machine Learning Specialization Notes: These notes will be integrated into other notes but kept here in their entirety.\n",
      "üé¨ START 15 üîö END 56\n",
      "ü™Ö SUBSECTION: \n",
      " ## Bias and Variance Machine Learning Specialization Advanced Machine Learning Algorithms, Week 3  Models almost never work the first time you try them out. Let's see how we can fix them.  ### Diagnosing Bias/Variance See Bias and Variance  If you have more features, you can't visualize Bias and Variance  A more systematic way to see if you have high bias or high variance is to look at the performance of the algorithm on the Training, Test, and Dev Sets|training set and dev set  A characteristic of a High Bias (under-fit) model is that $Jtrain$ (cost of the Training, Test, and Dev Sets|training set) is high. A characteristic of a High Variance (overfit) model is that $J_{cv}$ is high but $J_{train}$ is low. #### Slide !Screenshot 2023-10-17 at 1.26.00 PM.png  ### Understanding Bias and Variance Take degree of polynomial $d$ As $d$ increases, that is to say we add degrees to the polynomial $J_{train}({\\vec{w},b})$  will decrease. $J_{cv}(\\vec{w},b)$ dips towards the middle, showing the ideal degree for the polynomial.  #### Slide !Screenshot 2023-10-17 at 1.29.21 PM.png  ### Technique for Diagnosing Bias and Variance How do you tell if your algorithm has a high bias or variance problem?  One key takeaway is that high bias means it is not even doing well on the training set and high variance means it does much worse on the cross validation set and the training set. #### High Bias (under-fit) In an under-fit model $J_{train}$ will be high and $J_{train} \\approx J_{cv}$  #### High Variance (overfit) In an over-fit model $J_{cv} \\gg J_{train}$ and $J_{train}$ will be low. #### High Bias and High Variance In a model with both high bias and high variance $J_{train}$ will be high and $J_{cv} \\gg J_{train}$  This is rare, and doesn't really happen for linear models with one $d$, but it does happen. This is when it overfits for some part of the input and under-fits for another part of the input. #### Slide !Screenshot 2023-10-17 at 1.38.51 PM.png \n",
      "üé¨ START 56 üîö END 86\n",
      "ü™Ö SUBSECTION: \n",
      " ## Regularization and bias/variance ### Linear Regression with Regularization Regularized Linear Regression model: Lambda $\\lambda$ is the regularization parameter (Big #merge and cleanup needed here in the regularization, Regularized Linear Regression, cost, Cost and Loss, loss, Loss Function, etc. Aliases are needed in many of these)  Take this model: $$f_{\\vec{w},b}(x)=w_1+w_2x^2 + w_4x^4+b$$ With this regularization|regularized Cost and Loss $$J(\\vec{w},b)=\\frac{1}{2m} \\sum_{i=1}^{m}(f_{\\vec{w},{b}}(\\vec{x}^{(i)}-y^{(i)})^2+\\frac{\\lambda}{2m} \\sum_{j=1}^nw_j^2$$  If lambda is large, say $\\lambda = 10,000$ than $w_1 \\approx 0, W_2 \\approx 0$ and $f_{\\vec{x},b}\\vec{x} \\approx b$, thus the model creates a flat line. This is underfit, $J_{train}()  On the other hand, if we set $\\lambda=0$, then we have a forth order polynomial with no regularization. We end up with a very overfit curve.  So, how do we find a good value for $\\lambda$? ##### Slide !Screenshot 2023-10-17 at 3.56.53 PM.png ### Choosing the Regularization parameter $\\lambda$   This will be similar to choosing $d$ with cross validation Try multiple values for $\\lambda$ and then choose the option with the lowest cost. ### Slide !Screenshot 2023-10-17 at 4.15.27 PM.png ### Bias and Variance as a Function of regularization parameter $\\lambda$  Cross Validation tries out many versions of $\\lambda$ and then chooses the one with the lowest cost. Cross validation will help find us a a good value of $d$ as well as $\\lambda$  #### Slide !Screenshot 2023-10-17 at 4.14.07 PM.png  \n",
      "üé¨ START 86 üîö END 116\n",
      "ü™Ö SUBSECTION: \n",
      " ## Establishing A Baseline Level of Performance ### Speech Recognition example Job is to take in audio and output the text of what a person is saying  Training error $J_{train}$ is percentage of audio clips that the program does not transcribe correctly in it's entirety. Lets say: \tHuman Level Performance: 10.6 \t$J_{train}$: 10.8% \t$J_{cv}$: 14.8%  Why is human level error so high? There is lots of noise in the audio. It seems unfair to expect a learning algorithm to do much better. It is thus is more useful to measure the training error against the human error. So, looking at these results, $J_{train}$ is only 0.2$ higher than the human level performance whereas $J_{cv}$ is a full 4.2% higher. We can thus conclude that this algorithm has more of a variance problem than a bias problem. ### Establishing a baseline level of performance What is the level of error you can reasonably hope to get to? - Human level performance - Competing algorithms performance - Guess based on experience ### Bias Variance Examples Gap between baseline and training error shows high bias. A gap between training error and cross validation error shows high variance.  If your goal is perfection, the baseline would be zero. But for a lot of real world examples, like audio recognition, there is a lot of noise in the data, so you need a higher baseline.  If there is a gap between all three is high you have both high bias and high variance. #### Slide !Screenshot 2023-10-17 at 4.42.49 PM.png  \n",
      "üé¨ START 116 üîö END 142\n",
      "ü™Ö SUBSECTION: \n",
      " ## Learning Curves  Noted in Learning Curves ### Overview Learning curves help understand how your learning algorithm is doing as a function of the amount of experience it has. Experience being the number of training examples it has.  The bigger the training set the harder it is to fit all examples perfectly. Thus; as the training set increases so does the training error $J_{train}(\\vec{w},b)$   Plotting a learning curve by training different models based on different subsets of training data is computationally expensive, so in practice it isn't done that often. But, it's a good mental visualization. #### Slides !Screenshot 2023-10-17 at 5.40.04 PM.png    ### High Bias Example If a learning algorithm suffers from high bias, getting more training data will not (by itself) help that much. #### Slide for High Bias !Screenshot 2023-10-17 at 5.44.29 PM.png  ### High Variance Example If a learning algorithm suffers from high variance, getting more training data is likely to help. #### Slide for High Variance !Screenshot 2023-10-17 at 5.49.45 PM.png   \n",
      "üé¨ START 142 üîö END 159\n",
      "ü™Ö SUBSECTION: \n",
      " ## Deciding what to try next revisited ### Examples when Debugging an Algorithm: Get more training examples: fixes high variance Try smaller set of features: fixes high variance Try getting additional features: fixes high bias \tExamples; an algorithm that lacks information wont even do well on the training set Adding polynomial features ($x_1^2,X_2^2, etc$): Fixes high bias Decreasing $\\lambda$ Fixes high bias Increasing $\\lambda$ Fixes high variance \tForces the algorithm to force a smoother function.  **Note!** Don't randomly throw away training examples just to fix a high bias problem. ### Takeaway #merge with Bias and Variance ? If your algorithm has high variance, try simplifying your model or getting more training data. Simplification can mean a smaller set of features or an increased regularization If your algorithm has high bias, that is to say its not even doing well on the training set, you mainly need to make your model more powerful and flexible to fit more complex functions. To do so you can give it additional features, add polynomial features, or decrease $\\lambda$  \n",
      "üé¨ START 159 üîö END 187\n",
      "ü™Ö SUBSECTION: \n",
      " ## Bias/Variance and Neural Networks ### The bias variance tradeoff Simple model = high bias Complex model = high variance Before neural networks, we had to worry about balancing this complexity between bias and variance. With neural networks we now are mostly worried about high variance.  ### Neural Networks and bias variance Large Neural Network|neural networks are low bias machines. If you make your neural network large enough you can almost always fit your training set well. #### Recipe for decreasing bias with a neural network 1. Train a neural network 2.  If the training set error $J_{train}(\\vec{w},b)$ is high relative to your baseline, increase the size of the neural network by adding hidden layers. 3. Once $J_{train}(\\vec{w},b)$ is low enough, see if it does well on the cross validation set 4.  If the cross validation set $J_{cv}(\\vec{w},b)$ is too high, add more data, then test again from step 2. 5. Repeat until $J_{cv}(\\vec{w},b)$  is low enough for your liking. ##### Slide Illustration !Screenshot 2023-10-17 at 6.38.46 PM.png ### Limitations and Notes Bigger networks are restricted by your computing power, data is restricted to the amount of data you have.  Sometimes you will pingpong back between high bias and high variance as you move through this recipe and develop a machine learning algorithm. Use these observations to shape what you do next in the process.  ### Neural Networks and Regularization A large neural network will usually do as well or better than a smaller one so long as regularization is chosen appropriately. Of course, larger neural networks are more computationally expensive. ### Neural Network Regularization #important  #function and TensorFlow implementation: !Screenshot 2023-10-17 at 6.46.28 PM.png*Note usually don't regularize B, it doesn't really affect anything* # Python Notes\n",
      "üé¨ START 187 üîö END 191\n",
      "ü™Ö SUBSECTION: \n",
      " ## Keywords From codecademy - Continue Keyword: used inside a loop to skip the remaining loop code block and begin the next loop iteration. - Break keyword escapes the loop, regardless of the iteration number. Once break executes the program will continue to execute after the loop.\n",
      "ü™ô SECTION to tokenize:  # Afternoon To-Do #meta #todoList  ## Priority Completed: Complete Python module Completed: Complete Bias and Variance ## Bonus/tomorrow: Completed: Note cleanup with Aliases (see Career and Study To - Do) To Do: Linear Algebra Note Migration Completed: Integrate with this daily note \t\tTo Do: Training, Test, and Dev Sets \t\tTo Do: regularization (#merge and integrate) \t\tTo Do: Cost and Loss Completed: Big overhaul of Bias and Variance with regards to todays notes. \n",
      "üí∂ section has 122\n",
      "ü™ô SECTION to tokenize:  # Machine Learning Specialization Notes: These notes will be integrated into other notes but kept here in their entirety. ## Bias and Variance Machine Learning Specialization Advanced Machine Learning Algorithms, Week 3  Models almost never work the first time you try them out. Let's see how we can fix them.  ### Diagnosing Bias/Variance See Bias and Variance  If you have more features, you can't visualize Bias and Variance  A more systematic way to see if you have high bias or high variance is to look at the performance of the algorithm on the Training, Test, and Dev Sets|training set and dev set  A characteristic of a High Bias (under-fit) model is that $Jtrain$ (cost of the Training, Test, and Dev Sets|training set) is high. A characteristic of a High Variance (overfit) model is that $J_{cv}$ is high but $J_{train}$ is low. #### Slide !Screenshot 2023-10-17 at 1.26.00 PM.png  ### Understanding Bias and Variance Take degree of polynomial $d$ As $d$ increases, that is to say we add degrees to the polynomial $J_{train}({\\vec{w},b})$  will decrease. $J_{cv}(\\vec{w},b)$ dips towards the middle, showing the ideal degree for the polynomial.  #### Slide !Screenshot 2023-10-17 at 1.29.21 PM.png  ### Technique for Diagnosing Bias and Variance How do you tell if your algorithm has a high bias or variance problem?  One key takeaway is that high bias means it is not even doing well on the training set and high variance means it does much worse on the cross validation set and the training set. #### High Bias (under-fit) In an under-fit model $J_{train}$ will be high and $J_{train} \\approx J_{cv}$  #### High Variance (overfit) In an over-fit model $J_{cv} \\gg J_{train}$ and $J_{train}$ will be low. #### High Bias and High Variance In a model with both high bias and high variance $J_{train}$ will be high and $J_{cv} \\gg J_{train}$  This is rare, and doesn't really happen for linear models with one $d$, but it does happen. This is when it overfits for some part of the input and under-fits for another part of the input. #### Slide !Screenshot 2023-10-17 at 1.38.51 PM.png  ## Regularization and bias/variance ### Linear Regression with Regularization Regularized Linear Regression model: Lambda $\\lambda$ is the regularization parameter (Big #merge and cleanup needed here in the regularization, Regularized Linear Regression, cost, Cost and Loss, loss, Loss Function, etc. Aliases are needed in many of these)  Take this model: $$f_{\\vec{w},b}(x)=w_1+w_2x^2 + w_4x^4+b$$ With this regularization|regularized Cost and Loss $$J(\\vec{w},b)=\\frac{1}{2m} \\sum_{i=1}^{m}(f_{\\vec{w},{b}}(\\vec{x}^{(i)}-y^{(i)})^2+\\frac{\\lambda}{2m} \\sum_{j=1}^nw_j^2$$  If lambda is large, say $\\lambda = 10,000$ than $w_1 \\approx 0, W_2 \\approx 0$ and $f_{\\vec{x},b}\\vec{x} \\approx b$, thus the model creates a flat line. This is underfit, $J_{train}()  On the other hand, if we set $\\lambda=0$, then we have a forth order polynomial with no regularization. We end up with a very overfit curve.  So, how do we find a good value for $\\lambda$? ##### Slide !Screenshot 2023-10-17 at 3.56.53 PM.png ### Choosing the Regularization parameter $\\lambda$   This will be similar to choosing $d$ with cross validation Try multiple values for $\\lambda$ and then choose the option with the lowest cost. ### Slide !Screenshot 2023-10-17 at 4.15.27 PM.png ### Bias and Variance as a Function of regularization parameter $\\lambda$  Cross Validation tries out many versions of $\\lambda$ and then chooses the one with the lowest cost. Cross validation will help find us a a good value of $d$ as well as $\\lambda$  #### Slide !Screenshot 2023-10-17 at 4.14.07 PM.png   ## Establishing A Baseline Level of Performance ### Speech Recognition example Job is to take in audio and output the text of what a person is saying  Training error $J_{train}$ is percentage of audio clips that the program does not transcribe correctly in it's entirety. Lets say: \tHuman Level Performance: 10.6 \t$J_{train}$: 10.8% \t$J_{cv}$: 14.8%  Why is human level error so high? There is lots of noise in the audio. It seems unfair to expect a learning algorithm to do much better. It is thus is more useful to measure the training error against the human error. So, looking at these results, $J_{train}$ is only 0.2$ higher than the human level performance whereas $J_{cv}$ is a full 4.2% higher. We can thus conclude that this algorithm has more of a variance problem than a bias problem. ### Establishing a baseline level of performance What is the level of error you can reasonably hope to get to? - Human level performance - Competing algorithms performance - Guess based on experience ### Bias Variance Examples Gap between baseline and training error shows high bias. A gap between training error and cross validation error shows high variance.  If your goal is perfection, the baseline would be zero. But for a lot of real world examples, like audio recognition, there is a lot of noise in the data, so you need a higher baseline.  If there is a gap between all three is high you have both high bias and high variance. #### Slide !Screenshot 2023-10-17 at 4.42.49 PM.png   ## Learning Curves  Noted in Learning Curves ### Overview Learning curves help understand how your learning algorithm is doing as a function of the amount of experience it has. Experience being the number of training examples it has.  The bigger the training set the harder it is to fit all examples perfectly. Thus; as the training set increases so does the training error $J_{train}(\\vec{w},b)$   Plotting a learning curve by training different models based on different subsets of training data is computationally expensive, so in practice it isn't done that often. But, it's a good mental visualization. #### Slides !Screenshot 2023-10-17 at 5.40.04 PM.png    ### High Bias Example If a learning algorithm suffers from high bias, getting more training data will not (by itself) help that much. #### Slide for High Bias !Screenshot 2023-10-17 at 5.44.29 PM.png  ### High Variance Example If a learning algorithm suffers from high variance, getting more training data is likely to help. #### Slide for High Variance !Screenshot 2023-10-17 at 5.49.45 PM.png    ## Deciding what to try next revisited ### Examples when Debugging an Algorithm: Get more training examples: fixes high variance Try smaller set of features: fixes high variance Try getting additional features: fixes high bias \tExamples; an algorithm that lacks information wont even do well on the training set Adding polynomial features ($x_1^2,X_2^2, etc$): Fixes high bias Decreasing $\\lambda$ Fixes high bias Increasing $\\lambda$ Fixes high variance \tForces the algorithm to force a smoother function.  **Note!** Don't randomly throw away training examples just to fix a high bias problem. ### Takeaway #merge with Bias and Variance ? If your algorithm has high variance, try simplifying your model or getting more training data. Simplification can mean a smaller set of features or an increased regularization If your algorithm has high bias, that is to say its not even doing well on the training set, you mainly need to make your model more powerful and flexible to fit more complex functions. To do so you can give it additional features, add polynomial features, or decrease $\\lambda$   ## Bias/Variance and Neural Networks ### The bias variance tradeoff Simple model = high bias Complex model = high variance Before neural networks, we had to worry about balancing this complexity between bias and variance. With neural networks we now are mostly worried about high variance.  ### Neural Networks and bias variance Large Neural Network|neural networks are low bias machines. If you make your neural network large enough you can almost always fit your training set well. #### Recipe for decreasing bias with a neural network 1. Train a neural network 2.  If the training set error $J_{train}(\\vec{w},b)$ is high relative to your baseline, increase the size of the neural network by adding hidden layers. 3. Once $J_{train}(\\vec{w},b)$ is low enough, see if it does well on the cross validation set 4.  If the cross validation set $J_{cv}(\\vec{w},b)$ is too high, add more data, then test again from step 2. 5. Repeat until $J_{cv}(\\vec{w},b)$  is low enough for your liking. ##### Slide Illustration !Screenshot 2023-10-17 at 6.38.46 PM.png ### Limitations and Notes Bigger networks are restricted by your computing power, data is restricted to the amount of data you have.  Sometimes you will pingpong back between high bias and high variance as you move through this recipe and develop a machine learning algorithm. Use these observations to shape what you do next in the process.  ### Neural Networks and Regularization A large neural network will usually do as well or better than a smaller one so long as regularization is chosen appropriately. Of course, larger neural networks are more computationally expensive. ### Neural Network Regularization #important  #function and TensorFlow implementation: !Screenshot 2023-10-17 at 6.46.28 PM.png*Note usually don't regularize B, it doesn't really affect anything*\n",
      "üí∂ section has 2323\n",
      "ü™ô SECTION to tokenize:  \n",
      "üí∂ section has 2\n",
      "ü™ô SECTION to tokenize:  ## Priority Completed: Complete Python module Completed: Complete Bias and Variance\n",
      "üí∂ section has 17\n",
      "ü™ô SECTION to tokenize:  ## Bonus/tomorrow: Completed: Note cleanup with Aliases (see Career and Study To - Do) To Do: Linear Algebra Note Migration Completed: Integrate with this daily note \t\tTo Do: Training, Test, and Dev Sets \t\tTo Do: regularization (#merge and integrate) \t\tTo Do: Cost and Loss Completed: Big overhaul of Bias and Variance with regards to todays notes.  # Machine Learning Specialization Notes: These notes will be integrated into other notes but kept here in their entirety.\n",
      "üí∂ section has 116\n",
      "ü™ô SECTION to tokenize:  ## Bias and Variance Machine Learning Specialization Advanced Machine Learning Algorithms, Week 3  Models almost never work the first time you try them out. Let's see how we can fix them.  ### Diagnosing Bias/Variance See Bias and Variance  If you have more features, you can't visualize Bias and Variance  A more systematic way to see if you have high bias or high variance is to look at the performance of the algorithm on the Training, Test, and Dev Sets|training set and dev set  A characteristic of a High Bias (under-fit) model is that $Jtrain$ (cost of the Training, Test, and Dev Sets|training set) is high. A characteristic of a High Variance (overfit) model is that $J_{cv}$ is high but $J_{train}$ is low. #### Slide !Screenshot 2023-10-17 at 1.26.00 PM.png  ### Understanding Bias and Variance Take degree of polynomial $d$ As $d$ increases, that is to say we add degrees to the polynomial $J_{train}({\\vec{w},b})$  will decrease. $J_{cv}(\\vec{w},b)$ dips towards the middle, showing the ideal degree for the polynomial.  #### Slide !Screenshot 2023-10-17 at 1.29.21 PM.png  ### Technique for Diagnosing Bias and Variance How do you tell if your algorithm has a high bias or variance problem?  One key takeaway is that high bias means it is not even doing well on the training set and high variance means it does much worse on the cross validation set and the training set. #### High Bias (under-fit) In an under-fit model $J_{train}$ will be high and $J_{train} \\approx J_{cv}$  #### High Variance (overfit) In an over-fit model $J_{cv} \\gg J_{train}$ and $J_{train}$ will be low. #### High Bias and High Variance In a model with both high bias and high variance $J_{train}$ will be high and $J_{cv} \\gg J_{train}$  This is rare, and doesn't really happen for linear models with one $d$, but it does happen. This is when it overfits for some part of the input and under-fits for another part of the input. #### Slide !Screenshot 2023-10-17 at 1.38.51 PM.png \n",
      "üí∂ section has 554\n",
      "ü™ô SECTION to tokenize:  ## Regularization and bias/variance ### Linear Regression with Regularization Regularized Linear Regression model: Lambda $\\lambda$ is the regularization parameter (Big #merge and cleanup needed here in the regularization, Regularized Linear Regression, cost, Cost and Loss, loss, Loss Function, etc. Aliases are needed in many of these)  Take this model: $$f_{\\vec{w},b}(x)=w_1+w_2x^2 + w_4x^4+b$$ With this regularization|regularized Cost and Loss $$J(\\vec{w},b)=\\frac{1}{2m} \\sum_{i=1}^{m}(f_{\\vec{w},{b}}(\\vec{x}^{(i)}-y^{(i)})^2+\\frac{\\lambda}{2m} \\sum_{j=1}^nw_j^2$$  If lambda is large, say $\\lambda = 10,000$ than $w_1 \\approx 0, W_2 \\approx 0$ and $f_{\\vec{x},b}\\vec{x} \\approx b$, thus the model creates a flat line. This is underfit, $J_{train}()  On the other hand, if we set $\\lambda=0$, then we have a forth order polynomial with no regularization. We end up with a very overfit curve.  So, how do we find a good value for $\\lambda$? ##### Slide !Screenshot 2023-10-17 at 3.56.53 PM.png ### Choosing the Regularization parameter $\\lambda$   This will be similar to choosing $d$ with cross validation Try multiple values for $\\lambda$ and then choose the option with the lowest cost. ### Slide !Screenshot 2023-10-17 at 4.15.27 PM.png ### Bias and Variance as a Function of regularization parameter $\\lambda$  Cross Validation tries out many versions of $\\lambda$ and then chooses the one with the lowest cost. Cross validation will help find us a a good value of $d$ as well as $\\lambda$  #### Slide !Screenshot 2023-10-17 at 4.14.07 PM.png  \n",
      "üí∂ section has 487\n",
      "ü™ô SECTION to tokenize:  ## Establishing A Baseline Level of Performance ### Speech Recognition example Job is to take in audio and output the text of what a person is saying  Training error $J_{train}$ is percentage of audio clips that the program does not transcribe correctly in it's entirety. Lets say: \tHuman Level Performance: 10.6 \t$J_{train}$: 10.8% \t$J_{cv}$: 14.8%  Why is human level error so high? There is lots of noise in the audio. It seems unfair to expect a learning algorithm to do much better. It is thus is more useful to measure the training error against the human error. So, looking at these results, $J_{train}$ is only 0.2$ higher than the human level performance whereas $J_{cv}$ is a full 4.2% higher. We can thus conclude that this algorithm has more of a variance problem than a bias problem. ### Establishing a baseline level of performance What is the level of error you can reasonably hope to get to? - Human level performance - Competing algorithms performance - Guess based on experience ### Bias Variance Examples Gap between baseline and training error shows high bias. A gap between training error and cross validation error shows high variance.  If your goal is perfection, the baseline would be zero. But for a lot of real world examples, like audio recognition, there is a lot of noise in the data, so you need a higher baseline.  If there is a gap between all three is high you have both high bias and high variance. #### Slide !Screenshot 2023-10-17 at 4.42.49 PM.png  \n",
      "üí∂ section has 355\n",
      "ü™ô SECTION to tokenize:  ## Learning Curves  Noted in Learning Curves ### Overview Learning curves help understand how your learning algorithm is doing as a function of the amount of experience it has. Experience being the number of training examples it has.  The bigger the training set the harder it is to fit all examples perfectly. Thus; as the training set increases so does the training error $J_{train}(\\vec{w},b)$   Plotting a learning curve by training different models based on different subsets of training data is computationally expensive, so in practice it isn't done that often. But, it's a good mental visualization. #### Slides !Screenshot 2023-10-17 at 5.40.04 PM.png    ### High Bias Example If a learning algorithm suffers from high bias, getting more training data will not (by itself) help that much. #### Slide for High Bias !Screenshot 2023-10-17 at 5.44.29 PM.png  ### High Variance Example If a learning algorithm suffers from high variance, getting more training data is likely to help. #### Slide for High Variance !Screenshot 2023-10-17 at 5.49.45 PM.png   \n",
      "üí∂ section has 257\n",
      "ü™ô SECTION to tokenize:  ## Deciding what to try next revisited ### Examples when Debugging an Algorithm: Get more training examples: fixes high variance Try smaller set of features: fixes high variance Try getting additional features: fixes high bias \tExamples; an algorithm that lacks information wont even do well on the training set Adding polynomial features ($x_1^2,X_2^2, etc$): Fixes high bias Decreasing $\\lambda$ Fixes high bias Increasing $\\lambda$ Fixes high variance \tForces the algorithm to force a smoother function.  **Note!** Don't randomly throw away training examples just to fix a high bias problem. ### Takeaway #merge with Bias and Variance ? If your algorithm has high variance, try simplifying your model or getting more training data. Simplification can mean a smaller set of features or an increased regularization If your algorithm has high bias, that is to say its not even doing well on the training set, you mainly need to make your model more powerful and flexible to fit more complex functions. To do so you can give it additional features, add polynomial features, or decrease $\\lambda$  \n",
      "üí∂ section has 239\n",
      "ü™ô SECTION to tokenize:  ## Bias/Variance and Neural Networks ### The bias variance tradeoff Simple model = high bias Complex model = high variance Before neural networks, we had to worry about balancing this complexity between bias and variance. With neural networks we now are mostly worried about high variance.  ### Neural Networks and bias variance Large Neural Network|neural networks are low bias machines. If you make your neural network large enough you can almost always fit your training set well. #### Recipe for decreasing bias with a neural network 1. Train a neural network 2.  If the training set error $J_{train}(\\vec{w},b)$ is high relative to your baseline, increase the size of the neural network by adding hidden layers. 3. Once $J_{train}(\\vec{w},b)$ is low enough, see if it does well on the cross validation set 4.  If the cross validation set $J_{cv}(\\vec{w},b)$ is too high, add more data, then test again from step 2. 5. Repeat until $J_{cv}(\\vec{w},b)$  is low enough for your liking. ##### Slide Illustration !Screenshot 2023-10-17 at 6.38.46 PM.png ### Limitations and Notes Bigger networks are restricted by your computing power, data is restricted to the amount of data you have.  Sometimes you will pingpong back between high bias and high variance as you move through this recipe and develop a machine learning algorithm. Use these observations to shape what you do next in the process.  ### Neural Networks and Regularization A large neural network will usually do as well or better than a smaller one so long as regularization is chosen appropriately. Of course, larger neural networks are more computationally expensive. ### Neural Network Regularization #important  #function and TensorFlow implementation: !Screenshot 2023-10-17 at 6.46.28 PM.png*Note usually don't regularize B, it doesn't really affect anything* # Python Notes\n",
      "üí∂ section has 422\n",
      "ü™ô SECTION to tokenize:  ## Keywords From codecademy - Continue Keyword: used inside a loop to skip the remaining loop code block and begin the next loop iteration. - Break keyword escapes the loop, regardless of the iteration number. Once break executes the program will continue to execute after the loop.\n",
      "üí∂ section has 58\n",
      "üêò Largest Token size is 2323, which is Too Big (max tokens are 1024)\n",
      "üóø header: üü©### üü©\n",
      "üè≠ Building Subsections\n",
      "üëë Header positions:  [0, 13, 186, 2, 5, 15, 56, 86, 116, 142, 159, 187, 191, 20, 32, 41, 57, 73, 77, 79, 87, 100, 105, 119, 130, 135, 143, 154, 160, 164, 175, 180, 183]\n",
      "üé¨ START 0 üîö END 13\n",
      "ü™Ö SUBSECTION: \n",
      " # Afternoon To-Do #meta #todoList  ## Priority Completed: Complete Python module Completed: Complete Bias and Variance ## Bonus/tomorrow: Completed: Note cleanup with Aliases (see Career and Study To - Do) To Do: Linear Algebra Note Migration Completed: Integrate with this daily note \t\tTo Do: Training, Test, and Dev Sets \t\tTo Do: regularization (#merge and integrate) \t\tTo Do: Cost and Loss Completed: Big overhaul of Bias and Variance with regards to todays notes. \n",
      "üé¨ START 13 üîö END 186\n",
      "ü™Ö SUBSECTION: \n",
      " # Machine Learning Specialization Notes: These notes will be integrated into other notes but kept here in their entirety. ## Bias and Variance Machine Learning Specialization Advanced Machine Learning Algorithms, Week 3  Models almost never work the first time you try them out. Let's see how we can fix them.  ### Diagnosing Bias/Variance See Bias and Variance  If you have more features, you can't visualize Bias and Variance  A more systematic way to see if you have high bias or high variance is to look at the performance of the algorithm on the Training, Test, and Dev Sets|training set and dev set  A characteristic of a High Bias (under-fit) model is that $Jtrain$ (cost of the Training, Test, and Dev Sets|training set) is high. A characteristic of a High Variance (overfit) model is that $J_{cv}$ is high but $J_{train}$ is low. #### Slide !Screenshot 2023-10-17 at 1.26.00 PM.png  ### Understanding Bias and Variance Take degree of polynomial $d$ As $d$ increases, that is to say we add degrees to the polynomial $J_{train}({\\vec{w},b})$  will decrease. $J_{cv}(\\vec{w},b)$ dips towards the middle, showing the ideal degree for the polynomial.  #### Slide !Screenshot 2023-10-17 at 1.29.21 PM.png  ### Technique for Diagnosing Bias and Variance How do you tell if your algorithm has a high bias or variance problem?  One key takeaway is that high bias means it is not even doing well on the training set and high variance means it does much worse on the cross validation set and the training set. #### High Bias (under-fit) In an under-fit model $J_{train}$ will be high and $J_{train} \\approx J_{cv}$  #### High Variance (overfit) In an over-fit model $J_{cv} \\gg J_{train}$ and $J_{train}$ will be low. #### High Bias and High Variance In a model with both high bias and high variance $J_{train}$ will be high and $J_{cv} \\gg J_{train}$  This is rare, and doesn't really happen for linear models with one $d$, but it does happen. This is when it overfits for some part of the input and under-fits for another part of the input. #### Slide !Screenshot 2023-10-17 at 1.38.51 PM.png  ## Regularization and bias/variance ### Linear Regression with Regularization Regularized Linear Regression model: Lambda $\\lambda$ is the regularization parameter (Big #merge and cleanup needed here in the regularization, Regularized Linear Regression, cost, Cost and Loss, loss, Loss Function, etc. Aliases are needed in many of these)  Take this model: $$f_{\\vec{w},b}(x)=w_1+w_2x^2 + w_4x^4+b$$ With this regularization|regularized Cost and Loss $$J(\\vec{w},b)=\\frac{1}{2m} \\sum_{i=1}^{m}(f_{\\vec{w},{b}}(\\vec{x}^{(i)}-y^{(i)})^2+\\frac{\\lambda}{2m} \\sum_{j=1}^nw_j^2$$  If lambda is large, say $\\lambda = 10,000$ than $w_1 \\approx 0, W_2 \\approx 0$ and $f_{\\vec{x},b}\\vec{x} \\approx b$, thus the model creates a flat line. This is underfit, $J_{train}()  On the other hand, if we set $\\lambda=0$, then we have a forth order polynomial with no regularization. We end up with a very overfit curve.  So, how do we find a good value for $\\lambda$? ##### Slide !Screenshot 2023-10-17 at 3.56.53 PM.png ### Choosing the Regularization parameter $\\lambda$   This will be similar to choosing $d$ with cross validation Try multiple values for $\\lambda$ and then choose the option with the lowest cost. ### Slide !Screenshot 2023-10-17 at 4.15.27 PM.png ### Bias and Variance as a Function of regularization parameter $\\lambda$  Cross Validation tries out many versions of $\\lambda$ and then chooses the one with the lowest cost. Cross validation will help find us a a good value of $d$ as well as $\\lambda$  #### Slide !Screenshot 2023-10-17 at 4.14.07 PM.png   ## Establishing A Baseline Level of Performance ### Speech Recognition example Job is to take in audio and output the text of what a person is saying  Training error $J_{train}$ is percentage of audio clips that the program does not transcribe correctly in it's entirety. Lets say: \tHuman Level Performance: 10.6 \t$J_{train}$: 10.8% \t$J_{cv}$: 14.8%  Why is human level error so high? There is lots of noise in the audio. It seems unfair to expect a learning algorithm to do much better. It is thus is more useful to measure the training error against the human error. So, looking at these results, $J_{train}$ is only 0.2$ higher than the human level performance whereas $J_{cv}$ is a full 4.2% higher. We can thus conclude that this algorithm has more of a variance problem than a bias problem. ### Establishing a baseline level of performance What is the level of error you can reasonably hope to get to? - Human level performance - Competing algorithms performance - Guess based on experience ### Bias Variance Examples Gap between baseline and training error shows high bias. A gap between training error and cross validation error shows high variance.  If your goal is perfection, the baseline would be zero. But for a lot of real world examples, like audio recognition, there is a lot of noise in the data, so you need a higher baseline.  If there is a gap between all three is high you have both high bias and high variance. #### Slide !Screenshot 2023-10-17 at 4.42.49 PM.png   ## Learning Curves  Noted in Learning Curves ### Overview Learning curves help understand how your learning algorithm is doing as a function of the amount of experience it has. Experience being the number of training examples it has.  The bigger the training set the harder it is to fit all examples perfectly. Thus; as the training set increases so does the training error $J_{train}(\\vec{w},b)$   Plotting a learning curve by training different models based on different subsets of training data is computationally expensive, so in practice it isn't done that often. But, it's a good mental visualization. #### Slides !Screenshot 2023-10-17 at 5.40.04 PM.png    ### High Bias Example If a learning algorithm suffers from high bias, getting more training data will not (by itself) help that much. #### Slide for High Bias !Screenshot 2023-10-17 at 5.44.29 PM.png  ### High Variance Example If a learning algorithm suffers from high variance, getting more training data is likely to help. #### Slide for High Variance !Screenshot 2023-10-17 at 5.49.45 PM.png    ## Deciding what to try next revisited ### Examples when Debugging an Algorithm: Get more training examples: fixes high variance Try smaller set of features: fixes high variance Try getting additional features: fixes high bias \tExamples; an algorithm that lacks information wont even do well on the training set Adding polynomial features ($x_1^2,X_2^2, etc$): Fixes high bias Decreasing $\\lambda$ Fixes high bias Increasing $\\lambda$ Fixes high variance \tForces the algorithm to force a smoother function.  **Note!** Don't randomly throw away training examples just to fix a high bias problem. ### Takeaway #merge with Bias and Variance ? If your algorithm has high variance, try simplifying your model or getting more training data. Simplification can mean a smaller set of features or an increased regularization If your algorithm has high bias, that is to say its not even doing well on the training set, you mainly need to make your model more powerful and flexible to fit more complex functions. To do so you can give it additional features, add polynomial features, or decrease $\\lambda$   ## Bias/Variance and Neural Networks ### The bias variance tradeoff Simple model = high bias Complex model = high variance Before neural networks, we had to worry about balancing this complexity between bias and variance. With neural networks we now are mostly worried about high variance.  ### Neural Networks and bias variance Large Neural Network|neural networks are low bias machines. If you make your neural network large enough you can almost always fit your training set well. #### Recipe for decreasing bias with a neural network 1. Train a neural network 2.  If the training set error $J_{train}(\\vec{w},b)$ is high relative to your baseline, increase the size of the neural network by adding hidden layers. 3. Once $J_{train}(\\vec{w},b)$ is low enough, see if it does well on the cross validation set 4.  If the cross validation set $J_{cv}(\\vec{w},b)$ is too high, add more data, then test again from step 2. 5. Repeat until $J_{cv}(\\vec{w},b)$  is low enough for your liking. ##### Slide Illustration !Screenshot 2023-10-17 at 6.38.46 PM.png ### Limitations and Notes Bigger networks are restricted by your computing power, data is restricted to the amount of data you have.  Sometimes you will pingpong back between high bias and high variance as you move through this recipe and develop a machine learning algorithm. Use these observations to shape what you do next in the process.  ### Neural Networks and Regularization A large neural network will usually do as well or better than a smaller one so long as regularization is chosen appropriately. Of course, larger neural networks are more computationally expensive. ### Neural Network Regularization #important  #function and TensorFlow implementation: !Screenshot 2023-10-17 at 6.46.28 PM.png*Note usually don't regularize B, it doesn't really affect anything*\n",
      "üé¨ START 186 üîö END 2\n",
      "ü™Ö SUBSECTION: \n",
      " \n",
      "üé¨ START 2 üîö END 5\n",
      "ü™Ö SUBSECTION: \n",
      " ## Priority Completed: Complete Python module Completed: Complete Bias and Variance\n",
      "üé¨ START 5 üîö END 15\n",
      "ü™Ö SUBSECTION: \n",
      " ## Bonus/tomorrow: Completed: Note cleanup with Aliases (see Career and Study To - Do) To Do: Linear Algebra Note Migration Completed: Integrate with this daily note \t\tTo Do: Training, Test, and Dev Sets \t\tTo Do: regularization (#merge and integrate) \t\tTo Do: Cost and Loss Completed: Big overhaul of Bias and Variance with regards to todays notes.  # Machine Learning Specialization Notes: These notes will be integrated into other notes but kept here in their entirety.\n",
      "üé¨ START 15 üîö END 56\n",
      "ü™Ö SUBSECTION: \n",
      " ## Bias and Variance Machine Learning Specialization Advanced Machine Learning Algorithms, Week 3  Models almost never work the first time you try them out. Let's see how we can fix them.  ### Diagnosing Bias/Variance See Bias and Variance  If you have more features, you can't visualize Bias and Variance  A more systematic way to see if you have high bias or high variance is to look at the performance of the algorithm on the Training, Test, and Dev Sets|training set and dev set  A characteristic of a High Bias (under-fit) model is that $Jtrain$ (cost of the Training, Test, and Dev Sets|training set) is high. A characteristic of a High Variance (overfit) model is that $J_{cv}$ is high but $J_{train}$ is low. #### Slide !Screenshot 2023-10-17 at 1.26.00 PM.png  ### Understanding Bias and Variance Take degree of polynomial $d$ As $d$ increases, that is to say we add degrees to the polynomial $J_{train}({\\vec{w},b})$  will decrease. $J_{cv}(\\vec{w},b)$ dips towards the middle, showing the ideal degree for the polynomial.  #### Slide !Screenshot 2023-10-17 at 1.29.21 PM.png  ### Technique for Diagnosing Bias and Variance How do you tell if your algorithm has a high bias or variance problem?  One key takeaway is that high bias means it is not even doing well on the training set and high variance means it does much worse on the cross validation set and the training set. #### High Bias (under-fit) In an under-fit model $J_{train}$ will be high and $J_{train} \\approx J_{cv}$  #### High Variance (overfit) In an over-fit model $J_{cv} \\gg J_{train}$ and $J_{train}$ will be low. #### High Bias and High Variance In a model with both high bias and high variance $J_{train}$ will be high and $J_{cv} \\gg J_{train}$  This is rare, and doesn't really happen for linear models with one $d$, but it does happen. This is when it overfits for some part of the input and under-fits for another part of the input. #### Slide !Screenshot 2023-10-17 at 1.38.51 PM.png \n",
      "üé¨ START 56 üîö END 86\n",
      "ü™Ö SUBSECTION: \n",
      " ## Regularization and bias/variance ### Linear Regression with Regularization Regularized Linear Regression model: Lambda $\\lambda$ is the regularization parameter (Big #merge and cleanup needed here in the regularization, Regularized Linear Regression, cost, Cost and Loss, loss, Loss Function, etc. Aliases are needed in many of these)  Take this model: $$f_{\\vec{w},b}(x)=w_1+w_2x^2 + w_4x^4+b$$ With this regularization|regularized Cost and Loss $$J(\\vec{w},b)=\\frac{1}{2m} \\sum_{i=1}^{m}(f_{\\vec{w},{b}}(\\vec{x}^{(i)}-y^{(i)})^2+\\frac{\\lambda}{2m} \\sum_{j=1}^nw_j^2$$  If lambda is large, say $\\lambda = 10,000$ than $w_1 \\approx 0, W_2 \\approx 0$ and $f_{\\vec{x},b}\\vec{x} \\approx b$, thus the model creates a flat line. This is underfit, $J_{train}()  On the other hand, if we set $\\lambda=0$, then we have a forth order polynomial with no regularization. We end up with a very overfit curve.  So, how do we find a good value for $\\lambda$? ##### Slide !Screenshot 2023-10-17 at 3.56.53 PM.png ### Choosing the Regularization parameter $\\lambda$   This will be similar to choosing $d$ with cross validation Try multiple values for $\\lambda$ and then choose the option with the lowest cost. ### Slide !Screenshot 2023-10-17 at 4.15.27 PM.png ### Bias and Variance as a Function of regularization parameter $\\lambda$  Cross Validation tries out many versions of $\\lambda$ and then chooses the one with the lowest cost. Cross validation will help find us a a good value of $d$ as well as $\\lambda$  #### Slide !Screenshot 2023-10-17 at 4.14.07 PM.png  \n",
      "üé¨ START 86 üîö END 116\n",
      "ü™Ö SUBSECTION: \n",
      " ## Establishing A Baseline Level of Performance ### Speech Recognition example Job is to take in audio and output the text of what a person is saying  Training error $J_{train}$ is percentage of audio clips that the program does not transcribe correctly in it's entirety. Lets say: \tHuman Level Performance: 10.6 \t$J_{train}$: 10.8% \t$J_{cv}$: 14.8%  Why is human level error so high? There is lots of noise in the audio. It seems unfair to expect a learning algorithm to do much better. It is thus is more useful to measure the training error against the human error. So, looking at these results, $J_{train}$ is only 0.2$ higher than the human level performance whereas $J_{cv}$ is a full 4.2% higher. We can thus conclude that this algorithm has more of a variance problem than a bias problem. ### Establishing a baseline level of performance What is the level of error you can reasonably hope to get to? - Human level performance - Competing algorithms performance - Guess based on experience ### Bias Variance Examples Gap between baseline and training error shows high bias. A gap between training error and cross validation error shows high variance.  If your goal is perfection, the baseline would be zero. But for a lot of real world examples, like audio recognition, there is a lot of noise in the data, so you need a higher baseline.  If there is a gap between all three is high you have both high bias and high variance. #### Slide !Screenshot 2023-10-17 at 4.42.49 PM.png  \n",
      "üé¨ START 116 üîö END 142\n",
      "ü™Ö SUBSECTION: \n",
      " ## Learning Curves  Noted in Learning Curves ### Overview Learning curves help understand how your learning algorithm is doing as a function of the amount of experience it has. Experience being the number of training examples it has.  The bigger the training set the harder it is to fit all examples perfectly. Thus; as the training set increases so does the training error $J_{train}(\\vec{w},b)$   Plotting a learning curve by training different models based on different subsets of training data is computationally expensive, so in practice it isn't done that often. But, it's a good mental visualization. #### Slides !Screenshot 2023-10-17 at 5.40.04 PM.png    ### High Bias Example If a learning algorithm suffers from high bias, getting more training data will not (by itself) help that much. #### Slide for High Bias !Screenshot 2023-10-17 at 5.44.29 PM.png  ### High Variance Example If a learning algorithm suffers from high variance, getting more training data is likely to help. #### Slide for High Variance !Screenshot 2023-10-17 at 5.49.45 PM.png   \n",
      "üé¨ START 142 üîö END 159\n",
      "ü™Ö SUBSECTION: \n",
      " ## Deciding what to try next revisited ### Examples when Debugging an Algorithm: Get more training examples: fixes high variance Try smaller set of features: fixes high variance Try getting additional features: fixes high bias \tExamples; an algorithm that lacks information wont even do well on the training set Adding polynomial features ($x_1^2,X_2^2, etc$): Fixes high bias Decreasing $\\lambda$ Fixes high bias Increasing $\\lambda$ Fixes high variance \tForces the algorithm to force a smoother function.  **Note!** Don't randomly throw away training examples just to fix a high bias problem. ### Takeaway #merge with Bias and Variance ? If your algorithm has high variance, try simplifying your model or getting more training data. Simplification can mean a smaller set of features or an increased regularization If your algorithm has high bias, that is to say its not even doing well on the training set, you mainly need to make your model more powerful and flexible to fit more complex functions. To do so you can give it additional features, add polynomial features, or decrease $\\lambda$  \n",
      "üé¨ START 159 üîö END 187\n",
      "ü™Ö SUBSECTION: \n",
      " ## Bias/Variance and Neural Networks ### The bias variance tradeoff Simple model = high bias Complex model = high variance Before neural networks, we had to worry about balancing this complexity between bias and variance. With neural networks we now are mostly worried about high variance.  ### Neural Networks and bias variance Large Neural Network|neural networks are low bias machines. If you make your neural network large enough you can almost always fit your training set well. #### Recipe for decreasing bias with a neural network 1. Train a neural network 2.  If the training set error $J_{train}(\\vec{w},b)$ is high relative to your baseline, increase the size of the neural network by adding hidden layers. 3. Once $J_{train}(\\vec{w},b)$ is low enough, see if it does well on the cross validation set 4.  If the cross validation set $J_{cv}(\\vec{w},b)$ is too high, add more data, then test again from step 2. 5. Repeat until $J_{cv}(\\vec{w},b)$  is low enough for your liking. ##### Slide Illustration !Screenshot 2023-10-17 at 6.38.46 PM.png ### Limitations and Notes Bigger networks are restricted by your computing power, data is restricted to the amount of data you have.  Sometimes you will pingpong back between high bias and high variance as you move through this recipe and develop a machine learning algorithm. Use these observations to shape what you do next in the process.  ### Neural Networks and Regularization A large neural network will usually do as well or better than a smaller one so long as regularization is chosen appropriately. Of course, larger neural networks are more computationally expensive. ### Neural Network Regularization #important  #function and TensorFlow implementation: !Screenshot 2023-10-17 at 6.46.28 PM.png*Note usually don't regularize B, it doesn't really affect anything* # Python Notes\n",
      "üé¨ START 187 üîö END 191\n",
      "ü™Ö SUBSECTION: \n",
      " ## Keywords From codecademy - Continue Keyword: used inside a loop to skip the remaining loop code block and begin the next loop iteration. - Break keyword escapes the loop, regardless of the iteration number. Once break executes the program will continue to execute after the loop.\n",
      "üé¨ START 191 üîö END 20\n",
      "ü™Ö SUBSECTION: \n",
      " \n",
      "üé¨ START 20 üîö END 32\n",
      "ü™Ö SUBSECTION: \n",
      " ### Diagnosing Bias/Variance See Bias and Variance  If you have more features, you can't visualize Bias and Variance  A more systematic way to see if you have high bias or high variance is to look at the performance of the algorithm on the Training, Test, and Dev Sets|training set and dev set  A characteristic of a High Bias (under-fit) model is that $Jtrain$ (cost of the Training, Test, and Dev Sets|training set) is high. A characteristic of a High Variance (overfit) model is that $J_{cv}$ is high but $J_{train}$ is low. #### Slide !Screenshot 2023-10-17 at 1.26.00 PM.png \n",
      "üé¨ START 32 üîö END 41\n",
      "ü™Ö SUBSECTION: \n",
      " ### Understanding Bias and Variance Take degree of polynomial $d$ As $d$ increases, that is to say we add degrees to the polynomial $J_{train}({\\vec{w},b})$  will decrease. $J_{cv}(\\vec{w},b)$ dips towards the middle, showing the ideal degree for the polynomial.  #### Slide !Screenshot 2023-10-17 at 1.29.21 PM.png \n",
      "üé¨ START 41 üîö END 57\n",
      "ü™Ö SUBSECTION: \n",
      " ### Technique for Diagnosing Bias and Variance How do you tell if your algorithm has a high bias or variance problem?  One key takeaway is that high bias means it is not even doing well on the training set and high variance means it does much worse on the cross validation set and the training set. #### High Bias (under-fit) In an under-fit model $J_{train}$ will be high and $J_{train} \\approx J_{cv}$  #### High Variance (overfit) In an over-fit model $J_{cv} \\gg J_{train}$ and $J_{train}$ will be low. #### High Bias and High Variance In a model with both high bias and high variance $J_{train}$ will be high and $J_{cv} \\gg J_{train}$  This is rare, and doesn't really happen for linear models with one $d$, but it does happen. This is when it overfits for some part of the input and under-fits for another part of the input. #### Slide !Screenshot 2023-10-17 at 1.38.51 PM.png  ## Regularization and bias/variance\n",
      "üé¨ START 57 üîö END 73\n",
      "ü™Ö SUBSECTION: \n",
      " ### Linear Regression with Regularization Regularized Linear Regression model: Lambda $\\lambda$ is the regularization parameter (Big #merge and cleanup needed here in the regularization, Regularized Linear Regression, cost, Cost and Loss, loss, Loss Function, etc. Aliases are needed in many of these)  Take this model: $$f_{\\vec{w},b}(x)=w_1+w_2x^2 + w_4x^4+b$$ With this regularization|regularized Cost and Loss $$J(\\vec{w},b)=\\frac{1}{2m} \\sum_{i=1}^{m}(f_{\\vec{w},{b}}(\\vec{x}^{(i)}-y^{(i)})^2+\\frac{\\lambda}{2m} \\sum_{j=1}^nw_j^2$$  If lambda is large, say $\\lambda = 10,000$ than $w_1 \\approx 0, W_2 \\approx 0$ and $f_{\\vec{x},b}\\vec{x} \\approx b$, thus the model creates a flat line. This is underfit, $J_{train}()  On the other hand, if we set $\\lambda=0$, then we have a forth order polynomial with no regularization. We end up with a very overfit curve.  So, how do we find a good value for $\\lambda$? ##### Slide !Screenshot 2023-10-17 at 3.56.53 PM.png\n",
      "üé¨ START 73 üîö END 77\n",
      "ü™Ö SUBSECTION: \n",
      " ### Choosing the Regularization parameter $\\lambda$   This will be similar to choosing $d$ with cross validation Try multiple values for $\\lambda$ and then choose the option with the lowest cost.\n",
      "üé¨ START 77 üîö END 79\n",
      "ü™Ö SUBSECTION: \n",
      " ### Slide !Screenshot 2023-10-17 at 4.15.27 PM.png\n",
      "üé¨ START 79 üîö END 87\n",
      "ü™Ö SUBSECTION: \n",
      " ### Bias and Variance as a Function of regularization parameter $\\lambda$  Cross Validation tries out many versions of $\\lambda$ and then chooses the one with the lowest cost. Cross validation will help find us a a good value of $d$ as well as $\\lambda$  #### Slide !Screenshot 2023-10-17 at 4.14.07 PM.png   ## Establishing A Baseline Level of Performance\n",
      "üé¨ START 87 üîö END 100\n",
      "ü™Ö SUBSECTION: \n",
      " ### Speech Recognition example Job is to take in audio and output the text of what a person is saying  Training error $J_{train}$ is percentage of audio clips that the program does not transcribe correctly in it's entirety. Lets say: \tHuman Level Performance: 10.6 \t$J_{train}$: 10.8% \t$J_{cv}$: 14.8%  Why is human level error so high? There is lots of noise in the audio. It seems unfair to expect a learning algorithm to do much better. It is thus is more useful to measure the training error against the human error. So, looking at these results, $J_{train}$ is only 0.2$ higher than the human level performance whereas $J_{cv}$ is a full 4.2% higher. We can thus conclude that this algorithm has more of a variance problem than a bias problem.\n",
      "üé¨ START 100 üîö END 105\n",
      "ü™Ö SUBSECTION: \n",
      " ### Establishing a baseline level of performance What is the level of error you can reasonably hope to get to? - Human level performance - Competing algorithms performance - Guess based on experience\n",
      "üé¨ START 105 üîö END 119\n",
      "ü™Ö SUBSECTION: \n",
      " ### Bias Variance Examples Gap between baseline and training error shows high bias. A gap between training error and cross validation error shows high variance.  If your goal is perfection, the baseline would be zero. But for a lot of real world examples, like audio recognition, there is a lot of noise in the data, so you need a higher baseline.  If there is a gap between all three is high you have both high bias and high variance. #### Slide !Screenshot 2023-10-17 at 4.42.49 PM.png   ## Learning Curves  Noted in Learning Curves\n",
      "üé¨ START 119 üîö END 130\n",
      "ü™Ö SUBSECTION: \n",
      " ### Overview Learning curves help understand how your learning algorithm is doing as a function of the amount of experience it has. Experience being the number of training examples it has.  The bigger the training set the harder it is to fit all examples perfectly. Thus; as the training set increases so does the training error $J_{train}(\\vec{w},b)$   Plotting a learning curve by training different models based on different subsets of training data is computationally expensive, so in practice it isn't done that often. But, it's a good mental visualization. #### Slides !Screenshot 2023-10-17 at 5.40.04 PM.png   \n",
      "üé¨ START 130 üîö END 135\n",
      "ü™Ö SUBSECTION: \n",
      " ### High Bias Example If a learning algorithm suffers from high bias, getting more training data will not (by itself) help that much. #### Slide for High Bias !Screenshot 2023-10-17 at 5.44.29 PM.png \n",
      "üé¨ START 135 üîö END 143\n",
      "ü™Ö SUBSECTION: \n",
      " ### High Variance Example If a learning algorithm suffers from high variance, getting more training data is likely to help. #### Slide for High Variance !Screenshot 2023-10-17 at 5.49.45 PM.png    ## Deciding what to try next revisited\n",
      "üé¨ START 143 üîö END 154\n",
      "ü™Ö SUBSECTION: \n",
      " ### Examples when Debugging an Algorithm: Get more training examples: fixes high variance Try smaller set of features: fixes high variance Try getting additional features: fixes high bias \tExamples; an algorithm that lacks information wont even do well on the training set Adding polynomial features ($x_1^2,X_2^2, etc$): Fixes high bias Decreasing $\\lambda$ Fixes high bias Increasing $\\lambda$ Fixes high variance \tForces the algorithm to force a smoother function.  **Note!** Don't randomly throw away training examples just to fix a high bias problem.\n",
      "üé¨ START 154 üîö END 160\n",
      "ü™Ö SUBSECTION: \n",
      " ### Takeaway #merge with Bias and Variance ? If your algorithm has high variance, try simplifying your model or getting more training data. Simplification can mean a smaller set of features or an increased regularization If your algorithm has high bias, that is to say its not even doing well on the training set, you mainly need to make your model more powerful and flexible to fit more complex functions. To do so you can give it additional features, add polynomial features, or decrease $\\lambda$   ## Bias/Variance and Neural Networks\n",
      "üé¨ START 160 üîö END 164\n",
      "ü™Ö SUBSECTION: \n",
      " ### The bias variance tradeoff Simple model = high bias Complex model = high variance Before neural networks, we had to worry about balancing this complexity between bias and variance. With neural networks we now are mostly worried about high variance. \n",
      "üé¨ START 164 üîö END 175\n",
      "ü™Ö SUBSECTION: \n",
      " ### Neural Networks and bias variance Large Neural Network|neural networks are low bias machines. If you make your neural network large enough you can almost always fit your training set well. #### Recipe for decreasing bias with a neural network 1. Train a neural network 2.  If the training set error $J_{train}(\\vec{w},b)$ is high relative to your baseline, increase the size of the neural network by adding hidden layers. 3. Once $J_{train}(\\vec{w},b)$ is low enough, see if it does well on the cross validation set 4.  If the cross validation set $J_{cv}(\\vec{w},b)$ is too high, add more data, then test again from step 2. 5. Repeat until $J_{cv}(\\vec{w},b)$  is low enough for your liking. ##### Slide Illustration !Screenshot 2023-10-17 at 6.38.46 PM.png\n",
      "üé¨ START 175 üîö END 180\n",
      "ü™Ö SUBSECTION: \n",
      " ### Limitations and Notes Bigger networks are restricted by your computing power, data is restricted to the amount of data you have.  Sometimes you will pingpong back between high bias and high variance as you move through this recipe and develop a machine learning algorithm. Use these observations to shape what you do next in the process. \n",
      "üé¨ START 180 üîö END 183\n",
      "ü™Ö SUBSECTION: \n",
      " ### Neural Networks and Regularization A large neural network will usually do as well or better than a smaller one so long as regularization is chosen appropriately. Of course, larger neural networks are more computationally expensive.\n",
      "ü™ô SECTION to tokenize:  # Afternoon To-Do #meta #todoList  ## Priority Completed: Complete Python module Completed: Complete Bias and Variance ## Bonus/tomorrow: Completed: Note cleanup with Aliases (see Career and Study To - Do) To Do: Linear Algebra Note Migration Completed: Integrate with this daily note \t\tTo Do: Training, Test, and Dev Sets \t\tTo Do: regularization (#merge and integrate) \t\tTo Do: Cost and Loss Completed: Big overhaul of Bias and Variance with regards to todays notes. \n",
      "üí∂ section has 122\n",
      "ü™ô SECTION to tokenize:  # Machine Learning Specialization Notes: These notes will be integrated into other notes but kept here in their entirety. ## Bias and Variance Machine Learning Specialization Advanced Machine Learning Algorithms, Week 3  Models almost never work the first time you try them out. Let's see how we can fix them.  ### Diagnosing Bias/Variance See Bias and Variance  If you have more features, you can't visualize Bias and Variance  A more systematic way to see if you have high bias or high variance is to look at the performance of the algorithm on the Training, Test, and Dev Sets|training set and dev set  A characteristic of a High Bias (under-fit) model is that $Jtrain$ (cost of the Training, Test, and Dev Sets|training set) is high. A characteristic of a High Variance (overfit) model is that $J_{cv}$ is high but $J_{train}$ is low. #### Slide !Screenshot 2023-10-17 at 1.26.00 PM.png  ### Understanding Bias and Variance Take degree of polynomial $d$ As $d$ increases, that is to say we add degrees to the polynomial $J_{train}({\\vec{w},b})$  will decrease. $J_{cv}(\\vec{w},b)$ dips towards the middle, showing the ideal degree for the polynomial.  #### Slide !Screenshot 2023-10-17 at 1.29.21 PM.png  ### Technique for Diagnosing Bias and Variance How do you tell if your algorithm has a high bias or variance problem?  One key takeaway is that high bias means it is not even doing well on the training set and high variance means it does much worse on the cross validation set and the training set. #### High Bias (under-fit) In an under-fit model $J_{train}$ will be high and $J_{train} \\approx J_{cv}$  #### High Variance (overfit) In an over-fit model $J_{cv} \\gg J_{train}$ and $J_{train}$ will be low. #### High Bias and High Variance In a model with both high bias and high variance $J_{train}$ will be high and $J_{cv} \\gg J_{train}$  This is rare, and doesn't really happen for linear models with one $d$, but it does happen. This is when it overfits for some part of the input and under-fits for another part of the input. #### Slide !Screenshot 2023-10-17 at 1.38.51 PM.png  ## Regularization and bias/variance ### Linear Regression with Regularization Regularized Linear Regression model: Lambda $\\lambda$ is the regularization parameter (Big #merge and cleanup needed here in the regularization, Regularized Linear Regression, cost, Cost and Loss, loss, Loss Function, etc. Aliases are needed in many of these)  Take this model: $$f_{\\vec{w},b}(x)=w_1+w_2x^2 + w_4x^4+b$$ With this regularization|regularized Cost and Loss $$J(\\vec{w},b)=\\frac{1}{2m} \\sum_{i=1}^{m}(f_{\\vec{w},{b}}(\\vec{x}^{(i)}-y^{(i)})^2+\\frac{\\lambda}{2m} \\sum_{j=1}^nw_j^2$$  If lambda is large, say $\\lambda = 10,000$ than $w_1 \\approx 0, W_2 \\approx 0$ and $f_{\\vec{x},b}\\vec{x} \\approx b$, thus the model creates a flat line. This is underfit, $J_{train}()  On the other hand, if we set $\\lambda=0$, then we have a forth order polynomial with no regularization. We end up with a very overfit curve.  So, how do we find a good value for $\\lambda$? ##### Slide !Screenshot 2023-10-17 at 3.56.53 PM.png ### Choosing the Regularization parameter $\\lambda$   This will be similar to choosing $d$ with cross validation Try multiple values for $\\lambda$ and then choose the option with the lowest cost. ### Slide !Screenshot 2023-10-17 at 4.15.27 PM.png ### Bias and Variance as a Function of regularization parameter $\\lambda$  Cross Validation tries out many versions of $\\lambda$ and then chooses the one with the lowest cost. Cross validation will help find us a a good value of $d$ as well as $\\lambda$  #### Slide !Screenshot 2023-10-17 at 4.14.07 PM.png   ## Establishing A Baseline Level of Performance ### Speech Recognition example Job is to take in audio and output the text of what a person is saying  Training error $J_{train}$ is percentage of audio clips that the program does not transcribe correctly in it's entirety. Lets say: \tHuman Level Performance: 10.6 \t$J_{train}$: 10.8% \t$J_{cv}$: 14.8%  Why is human level error so high? There is lots of noise in the audio. It seems unfair to expect a learning algorithm to do much better. It is thus is more useful to measure the training error against the human error. So, looking at these results, $J_{train}$ is only 0.2$ higher than the human level performance whereas $J_{cv}$ is a full 4.2% higher. We can thus conclude that this algorithm has more of a variance problem than a bias problem. ### Establishing a baseline level of performance What is the level of error you can reasonably hope to get to? - Human level performance - Competing algorithms performance - Guess based on experience ### Bias Variance Examples Gap between baseline and training error shows high bias. A gap between training error and cross validation error shows high variance.  If your goal is perfection, the baseline would be zero. But for a lot of real world examples, like audio recognition, there is a lot of noise in the data, so you need a higher baseline.  If there is a gap between all three is high you have both high bias and high variance. #### Slide !Screenshot 2023-10-17 at 4.42.49 PM.png   ## Learning Curves  Noted in Learning Curves ### Overview Learning curves help understand how your learning algorithm is doing as a function of the amount of experience it has. Experience being the number of training examples it has.  The bigger the training set the harder it is to fit all examples perfectly. Thus; as the training set increases so does the training error $J_{train}(\\vec{w},b)$   Plotting a learning curve by training different models based on different subsets of training data is computationally expensive, so in practice it isn't done that often. But, it's a good mental visualization. #### Slides !Screenshot 2023-10-17 at 5.40.04 PM.png    ### High Bias Example If a learning algorithm suffers from high bias, getting more training data will not (by itself) help that much. #### Slide for High Bias !Screenshot 2023-10-17 at 5.44.29 PM.png  ### High Variance Example If a learning algorithm suffers from high variance, getting more training data is likely to help. #### Slide for High Variance !Screenshot 2023-10-17 at 5.49.45 PM.png    ## Deciding what to try next revisited ### Examples when Debugging an Algorithm: Get more training examples: fixes high variance Try smaller set of features: fixes high variance Try getting additional features: fixes high bias \tExamples; an algorithm that lacks information wont even do well on the training set Adding polynomial features ($x_1^2,X_2^2, etc$): Fixes high bias Decreasing $\\lambda$ Fixes high bias Increasing $\\lambda$ Fixes high variance \tForces the algorithm to force a smoother function.  **Note!** Don't randomly throw away training examples just to fix a high bias problem. ### Takeaway #merge with Bias and Variance ? If your algorithm has high variance, try simplifying your model or getting more training data. Simplification can mean a smaller set of features or an increased regularization If your algorithm has high bias, that is to say its not even doing well on the training set, you mainly need to make your model more powerful and flexible to fit more complex functions. To do so you can give it additional features, add polynomial features, or decrease $\\lambda$   ## Bias/Variance and Neural Networks ### The bias variance tradeoff Simple model = high bias Complex model = high variance Before neural networks, we had to worry about balancing this complexity between bias and variance. With neural networks we now are mostly worried about high variance.  ### Neural Networks and bias variance Large Neural Network|neural networks are low bias machines. If you make your neural network large enough you can almost always fit your training set well. #### Recipe for decreasing bias with a neural network 1. Train a neural network 2.  If the training set error $J_{train}(\\vec{w},b)$ is high relative to your baseline, increase the size of the neural network by adding hidden layers. 3. Once $J_{train}(\\vec{w},b)$ is low enough, see if it does well on the cross validation set 4.  If the cross validation set $J_{cv}(\\vec{w},b)$ is too high, add more data, then test again from step 2. 5. Repeat until $J_{cv}(\\vec{w},b)$  is low enough for your liking. ##### Slide Illustration !Screenshot 2023-10-17 at 6.38.46 PM.png ### Limitations and Notes Bigger networks are restricted by your computing power, data is restricted to the amount of data you have.  Sometimes you will pingpong back between high bias and high variance as you move through this recipe and develop a machine learning algorithm. Use these observations to shape what you do next in the process.  ### Neural Networks and Regularization A large neural network will usually do as well or better than a smaller one so long as regularization is chosen appropriately. Of course, larger neural networks are more computationally expensive. ### Neural Network Regularization #important  #function and TensorFlow implementation: !Screenshot 2023-10-17 at 6.46.28 PM.png*Note usually don't regularize B, it doesn't really affect anything*\n",
      "üí∂ section has 2323\n",
      "ü™ô SECTION to tokenize:  \n",
      "üí∂ section has 2\n",
      "ü™ô SECTION to tokenize:  ## Priority Completed: Complete Python module Completed: Complete Bias and Variance\n",
      "üí∂ section has 17\n",
      "ü™ô SECTION to tokenize:  ## Bonus/tomorrow: Completed: Note cleanup with Aliases (see Career and Study To - Do) To Do: Linear Algebra Note Migration Completed: Integrate with this daily note \t\tTo Do: Training, Test, and Dev Sets \t\tTo Do: regularization (#merge and integrate) \t\tTo Do: Cost and Loss Completed: Big overhaul of Bias and Variance with regards to todays notes.  # Machine Learning Specialization Notes: These notes will be integrated into other notes but kept here in their entirety.\n",
      "üí∂ section has 116\n",
      "ü™ô SECTION to tokenize:  ## Bias and Variance Machine Learning Specialization Advanced Machine Learning Algorithms, Week 3  Models almost never work the first time you try them out. Let's see how we can fix them.  ### Diagnosing Bias/Variance See Bias and Variance  If you have more features, you can't visualize Bias and Variance  A more systematic way to see if you have high bias or high variance is to look at the performance of the algorithm on the Training, Test, and Dev Sets|training set and dev set  A characteristic of a High Bias (under-fit) model is that $Jtrain$ (cost of the Training, Test, and Dev Sets|training set) is high. A characteristic of a High Variance (overfit) model is that $J_{cv}$ is high but $J_{train}$ is low. #### Slide !Screenshot 2023-10-17 at 1.26.00 PM.png  ### Understanding Bias and Variance Take degree of polynomial $d$ As $d$ increases, that is to say we add degrees to the polynomial $J_{train}({\\vec{w},b})$  will decrease. $J_{cv}(\\vec{w},b)$ dips towards the middle, showing the ideal degree for the polynomial.  #### Slide !Screenshot 2023-10-17 at 1.29.21 PM.png  ### Technique for Diagnosing Bias and Variance How do you tell if your algorithm has a high bias or variance problem?  One key takeaway is that high bias means it is not even doing well on the training set and high variance means it does much worse on the cross validation set and the training set. #### High Bias (under-fit) In an under-fit model $J_{train}$ will be high and $J_{train} \\approx J_{cv}$  #### High Variance (overfit) In an over-fit model $J_{cv} \\gg J_{train}$ and $J_{train}$ will be low. #### High Bias and High Variance In a model with both high bias and high variance $J_{train}$ will be high and $J_{cv} \\gg J_{train}$  This is rare, and doesn't really happen for linear models with one $d$, but it does happen. This is when it overfits for some part of the input and under-fits for another part of the input. #### Slide !Screenshot 2023-10-17 at 1.38.51 PM.png \n",
      "üí∂ section has 554\n",
      "ü™ô SECTION to tokenize:  ## Regularization and bias/variance ### Linear Regression with Regularization Regularized Linear Regression model: Lambda $\\lambda$ is the regularization parameter (Big #merge and cleanup needed here in the regularization, Regularized Linear Regression, cost, Cost and Loss, loss, Loss Function, etc. Aliases are needed in many of these)  Take this model: $$f_{\\vec{w},b}(x)=w_1+w_2x^2 + w_4x^4+b$$ With this regularization|regularized Cost and Loss $$J(\\vec{w},b)=\\frac{1}{2m} \\sum_{i=1}^{m}(f_{\\vec{w},{b}}(\\vec{x}^{(i)}-y^{(i)})^2+\\frac{\\lambda}{2m} \\sum_{j=1}^nw_j^2$$  If lambda is large, say $\\lambda = 10,000$ than $w_1 \\approx 0, W_2 \\approx 0$ and $f_{\\vec{x},b}\\vec{x} \\approx b$, thus the model creates a flat line. This is underfit, $J_{train}()  On the other hand, if we set $\\lambda=0$, then we have a forth order polynomial with no regularization. We end up with a very overfit curve.  So, how do we find a good value for $\\lambda$? ##### Slide !Screenshot 2023-10-17 at 3.56.53 PM.png ### Choosing the Regularization parameter $\\lambda$   This will be similar to choosing $d$ with cross validation Try multiple values for $\\lambda$ and then choose the option with the lowest cost. ### Slide !Screenshot 2023-10-17 at 4.15.27 PM.png ### Bias and Variance as a Function of regularization parameter $\\lambda$  Cross Validation tries out many versions of $\\lambda$ and then chooses the one with the lowest cost. Cross validation will help find us a a good value of $d$ as well as $\\lambda$  #### Slide !Screenshot 2023-10-17 at 4.14.07 PM.png  \n",
      "üí∂ section has 487\n",
      "ü™ô SECTION to tokenize:  ## Establishing A Baseline Level of Performance ### Speech Recognition example Job is to take in audio and output the text of what a person is saying  Training error $J_{train}$ is percentage of audio clips that the program does not transcribe correctly in it's entirety. Lets say: \tHuman Level Performance: 10.6 \t$J_{train}$: 10.8% \t$J_{cv}$: 14.8%  Why is human level error so high? There is lots of noise in the audio. It seems unfair to expect a learning algorithm to do much better. It is thus is more useful to measure the training error against the human error. So, looking at these results, $J_{train}$ is only 0.2$ higher than the human level performance whereas $J_{cv}$ is a full 4.2% higher. We can thus conclude that this algorithm has more of a variance problem than a bias problem. ### Establishing a baseline level of performance What is the level of error you can reasonably hope to get to? - Human level performance - Competing algorithms performance - Guess based on experience ### Bias Variance Examples Gap between baseline and training error shows high bias. A gap between training error and cross validation error shows high variance.  If your goal is perfection, the baseline would be zero. But for a lot of real world examples, like audio recognition, there is a lot of noise in the data, so you need a higher baseline.  If there is a gap between all three is high you have both high bias and high variance. #### Slide !Screenshot 2023-10-17 at 4.42.49 PM.png  \n",
      "üí∂ section has 355\n",
      "ü™ô SECTION to tokenize:  ## Learning Curves  Noted in Learning Curves ### Overview Learning curves help understand how your learning algorithm is doing as a function of the amount of experience it has. Experience being the number of training examples it has.  The bigger the training set the harder it is to fit all examples perfectly. Thus; as the training set increases so does the training error $J_{train}(\\vec{w},b)$   Plotting a learning curve by training different models based on different subsets of training data is computationally expensive, so in practice it isn't done that often. But, it's a good mental visualization. #### Slides !Screenshot 2023-10-17 at 5.40.04 PM.png    ### High Bias Example If a learning algorithm suffers from high bias, getting more training data will not (by itself) help that much. #### Slide for High Bias !Screenshot 2023-10-17 at 5.44.29 PM.png  ### High Variance Example If a learning algorithm suffers from high variance, getting more training data is likely to help. #### Slide for High Variance !Screenshot 2023-10-17 at 5.49.45 PM.png   \n",
      "üí∂ section has 257\n",
      "ü™ô SECTION to tokenize:  ## Deciding what to try next revisited ### Examples when Debugging an Algorithm: Get more training examples: fixes high variance Try smaller set of features: fixes high variance Try getting additional features: fixes high bias \tExamples; an algorithm that lacks information wont even do well on the training set Adding polynomial features ($x_1^2,X_2^2, etc$): Fixes high bias Decreasing $\\lambda$ Fixes high bias Increasing $\\lambda$ Fixes high variance \tForces the algorithm to force a smoother function.  **Note!** Don't randomly throw away training examples just to fix a high bias problem. ### Takeaway #merge with Bias and Variance ? If your algorithm has high variance, try simplifying your model or getting more training data. Simplification can mean a smaller set of features or an increased regularization If your algorithm has high bias, that is to say its not even doing well on the training set, you mainly need to make your model more powerful and flexible to fit more complex functions. To do so you can give it additional features, add polynomial features, or decrease $\\lambda$  \n",
      "üí∂ section has 239\n",
      "ü™ô SECTION to tokenize:  ## Bias/Variance and Neural Networks ### The bias variance tradeoff Simple model = high bias Complex model = high variance Before neural networks, we had to worry about balancing this complexity between bias and variance. With neural networks we now are mostly worried about high variance.  ### Neural Networks and bias variance Large Neural Network|neural networks are low bias machines. If you make your neural network large enough you can almost always fit your training set well. #### Recipe for decreasing bias with a neural network 1. Train a neural network 2.  If the training set error $J_{train}(\\vec{w},b)$ is high relative to your baseline, increase the size of the neural network by adding hidden layers. 3. Once $J_{train}(\\vec{w},b)$ is low enough, see if it does well on the cross validation set 4.  If the cross validation set $J_{cv}(\\vec{w},b)$ is too high, add more data, then test again from step 2. 5. Repeat until $J_{cv}(\\vec{w},b)$  is low enough for your liking. ##### Slide Illustration !Screenshot 2023-10-17 at 6.38.46 PM.png ### Limitations and Notes Bigger networks are restricted by your computing power, data is restricted to the amount of data you have.  Sometimes you will pingpong back between high bias and high variance as you move through this recipe and develop a machine learning algorithm. Use these observations to shape what you do next in the process.  ### Neural Networks and Regularization A large neural network will usually do as well or better than a smaller one so long as regularization is chosen appropriately. Of course, larger neural networks are more computationally expensive. ### Neural Network Regularization #important  #function and TensorFlow implementation: !Screenshot 2023-10-17 at 6.46.28 PM.png*Note usually don't regularize B, it doesn't really affect anything* # Python Notes\n",
      "üí∂ section has 422\n",
      "ü™ô SECTION to tokenize:  ## Keywords From codecademy - Continue Keyword: used inside a loop to skip the remaining loop code block and begin the next loop iteration. - Break keyword escapes the loop, regardless of the iteration number. Once break executes the program will continue to execute after the loop.\n",
      "üí∂ section has 58\n",
      "ü™ô SECTION to tokenize:  \n",
      "üí∂ section has 2\n",
      "ü™ô SECTION to tokenize:  ### Diagnosing Bias/Variance See Bias and Variance  If you have more features, you can't visualize Bias and Variance  A more systematic way to see if you have high bias or high variance is to look at the performance of the algorithm on the Training, Test, and Dev Sets|training set and dev set  A characteristic of a High Bias (under-fit) model is that $Jtrain$ (cost of the Training, Test, and Dev Sets|training set) is high. A characteristic of a High Variance (overfit) model is that $J_{cv}$ is high but $J_{train}$ is low. #### Slide !Screenshot 2023-10-17 at 1.26.00 PM.png \n",
      "üí∂ section has 162\n",
      "ü™ô SECTION to tokenize:  ### Understanding Bias and Variance Take degree of polynomial $d$ As $d$ increases, that is to say we add degrees to the polynomial $J_{train}({\\vec{w},b})$  will decrease. $J_{cv}(\\vec{w},b)$ dips towards the middle, showing the ideal degree for the polynomial.  #### Slide !Screenshot 2023-10-17 at 1.29.21 PM.png \n",
      "üí∂ section has 104\n",
      "ü™ô SECTION to tokenize:  ### Technique for Diagnosing Bias and Variance How do you tell if your algorithm has a high bias or variance problem?  One key takeaway is that high bias means it is not even doing well on the training set and high variance means it does much worse on the cross validation set and the training set. #### High Bias (under-fit) In an under-fit model $J_{train}$ will be high and $J_{train} \\approx J_{cv}$  #### High Variance (overfit) In an over-fit model $J_{cv} \\gg J_{train}$ and $J_{train}$ will be low. #### High Bias and High Variance In a model with both high bias and high variance $J_{train}$ will be high and $J_{cv} \\gg J_{train}$  This is rare, and doesn't really happen for linear models with one $d$, but it does happen. This is when it overfits for some part of the input and under-fits for another part of the input. #### Slide !Screenshot 2023-10-17 at 1.38.51 PM.png  ## Regularization and bias/variance\n",
      "üí∂ section has 258\n",
      "ü™ô SECTION to tokenize:  ### Linear Regression with Regularization Regularized Linear Regression model: Lambda $\\lambda$ is the regularization parameter (Big #merge and cleanup needed here in the regularization, Regularized Linear Regression, cost, Cost and Loss, loss, Loss Function, etc. Aliases are needed in many of these)  Take this model: $$f_{\\vec{w},b}(x)=w_1+w_2x^2 + w_4x^4+b$$ With this regularization|regularized Cost and Loss $$J(\\vec{w},b)=\\frac{1}{2m} \\sum_{i=1}^{m}(f_{\\vec{w},{b}}(\\vec{x}^{(i)}-y^{(i)})^2+\\frac{\\lambda}{2m} \\sum_{j=1}^nw_j^2$$  If lambda is large, say $\\lambda = 10,000$ than $w_1 \\approx 0, W_2 \\approx 0$ and $f_{\\vec{x},b}\\vec{x} \\approx b$, thus the model creates a flat line. This is underfit, $J_{train}()  On the other hand, if we set $\\lambda=0$, then we have a forth order polynomial with no regularization. We end up with a very overfit curve.  So, how do we find a good value for $\\lambda$? ##### Slide !Screenshot 2023-10-17 at 3.56.53 PM.png\n",
      "üí∂ section has 338\n",
      "ü™ô SECTION to tokenize:  ### Choosing the Regularization parameter $\\lambda$   This will be similar to choosing $d$ with cross validation Try multiple values for $\\lambda$ and then choose the option with the lowest cost.\n",
      "üí∂ section has 43\n",
      "ü™ô SECTION to tokenize:  ### Slide !Screenshot 2023-10-17 at 4.15.27 PM.png\n",
      "üí∂ section has 21\n",
      "ü™ô SECTION to tokenize:  ### Bias and Variance as a Function of regularization parameter $\\lambda$  Cross Validation tries out many versions of $\\lambda$ and then chooses the one with the lowest cost. Cross validation will help find us a a good value of $d$ as well as $\\lambda$  #### Slide !Screenshot 2023-10-17 at 4.14.07 PM.png   ## Establishing A Baseline Level of Performance\n",
      "üí∂ section has 93\n",
      "ü™ô SECTION to tokenize:  ### Speech Recognition example Job is to take in audio and output the text of what a person is saying  Training error $J_{train}$ is percentage of audio clips that the program does not transcribe correctly in it's entirety. Lets say: \tHuman Level Performance: 10.6 \t$J_{train}$: 10.8% \t$J_{cv}$: 14.8%  Why is human level error so high? There is lots of noise in the audio. It seems unfair to expect a learning algorithm to do much better. It is thus is more useful to measure the training error against the human error. So, looking at these results, $J_{train}$ is only 0.2$ higher than the human level performance whereas $J_{cv}$ is a full 4.2% higher. We can thus conclude that this algorithm has more of a variance problem than a bias problem.\n",
      "üí∂ section has 194\n",
      "ü™ô SECTION to tokenize:  ### Establishing a baseline level of performance What is the level of error you can reasonably hope to get to? - Human level performance - Competing algorithms performance - Guess based on experience\n",
      "üí∂ section has 39\n",
      "ü™ô SECTION to tokenize:  ### Bias Variance Examples Gap between baseline and training error shows high bias. A gap between training error and cross validation error shows high variance.  If your goal is perfection, the baseline would be zero. But for a lot of real world examples, like audio recognition, there is a lot of noise in the data, so you need a higher baseline.  If there is a gap between all three is high you have both high bias and high variance. #### Slide !Screenshot 2023-10-17 at 4.42.49 PM.png   ## Learning Curves  Noted in Learning Curves\n",
      "üí∂ section has 127\n",
      "ü™ô SECTION to tokenize:  ### Overview Learning curves help understand how your learning algorithm is doing as a function of the amount of experience it has. Experience being the number of training examples it has.  The bigger the training set the harder it is to fit all examples perfectly. Thus; as the training set increases so does the training error $J_{train}(\\vec{w},b)$   Plotting a learning curve by training different models based on different subsets of training data is computationally expensive, so in practice it isn't done that often. But, it's a good mental visualization. #### Slides !Screenshot 2023-10-17 at 5.40.04 PM.png   \n",
      "üí∂ section has 143\n",
      "ü™ô SECTION to tokenize:  ### High Bias Example If a learning algorithm suffers from high bias, getting more training data will not (by itself) help that much. #### Slide for High Bias !Screenshot 2023-10-17 at 5.44.29 PM.png \n",
      "üí∂ section has 55\n",
      "ü™ô SECTION to tokenize:  ### High Variance Example If a learning algorithm suffers from high variance, getting more training data is likely to help. #### Slide for High Variance !Screenshot 2023-10-17 at 5.49.45 PM.png    ## Deciding what to try next revisited\n",
      "üí∂ section has 61\n",
      "ü™ô SECTION to tokenize:  ### Examples when Debugging an Algorithm: Get more training examples: fixes high variance Try smaller set of features: fixes high variance Try getting additional features: fixes high bias \tExamples; an algorithm that lacks information wont even do well on the training set Adding polynomial features ($x_1^2,X_2^2, etc$): Fixes high bias Decreasing $\\lambda$ Fixes high bias Increasing $\\lambda$ Fixes high variance \tForces the algorithm to force a smoother function.  **Note!** Don't randomly throw away training examples just to fix a high bias problem.\n",
      "üí∂ section has 124\n",
      "ü™ô SECTION to tokenize:  ### Takeaway #merge with Bias and Variance ? If your algorithm has high variance, try simplifying your model or getting more training data. Simplification can mean a smaller set of features or an increased regularization If your algorithm has high bias, that is to say its not even doing well on the training set, you mainly need to make your model more powerful and flexible to fit more complex functions. To do so you can give it additional features, add polynomial features, or decrease $\\lambda$   ## Bias/Variance and Neural Networks\n",
      "üí∂ section has 117\n",
      "ü™ô SECTION to tokenize:  ### The bias variance tradeoff Simple model = high bias Complex model = high variance Before neural networks, we had to worry about balancing this complexity between bias and variance. With neural networks we now are mostly worried about high variance. \n",
      "üí∂ section has 48\n",
      "ü™ô SECTION to tokenize:  ### Neural Networks and bias variance Large Neural Network|neural networks are low bias machines. If you make your neural network large enough you can almost always fit your training set well. #### Recipe for decreasing bias with a neural network 1. Train a neural network 2.  If the training set error $J_{train}(\\vec{w},b)$ is high relative to your baseline, increase the size of the neural network by adding hidden layers. 3. Once $J_{train}(\\vec{w},b)$ is low enough, see if it does well on the cross validation set 4.  If the cross validation set $J_{cv}(\\vec{w},b)$ is too high, add more data, then test again from step 2. 5. Repeat until $J_{cv}(\\vec{w},b)$  is low enough for your liking. ##### Slide Illustration !Screenshot 2023-10-17 at 6.38.46 PM.png\n",
      "üí∂ section has 206\n",
      "ü™ô SECTION to tokenize:  ### Limitations and Notes Bigger networks are restricted by your computing power, data is restricted to the amount of data you have.  Sometimes you will pingpong back between high bias and high variance as you move through this recipe and develop a machine learning algorithm. Use these observations to shape what you do next in the process. \n",
      "üí∂ section has 69\n",
      "ü™ô SECTION to tokenize:  ### Neural Networks and Regularization A large neural network will usually do as well or better than a smaller one so long as regularization is chosen appropriately. Of course, larger neural networks are more computationally expensive.\n",
      "üí∂ section has 44\n",
      "üêò Largest Token size is 2323, which is Too Big (max tokens are 1024)\n",
      "üóø header: üü©#### üü©\n",
      "üè≠ Building Subsections\n",
      "üëë Header positions:  [0, 13, 186, 2, 5, 15, 56, 86, 116, 142, 159, 187, 191, 20, 32, 41, 57, 73, 77, 79, 87, 100, 105, 119, 130, 135, 143, 154, 160, 164, 175, 180, 183, 29, 38, 45, 47, 49, 53, 82, 112, 125, 132, 137, 167]\n",
      "üé¨ START 0 üîö END 13\n",
      "ü™Ö SUBSECTION: \n",
      " # Afternoon To-Do #meta #todoList  ## Priority Completed: Complete Python module Completed: Complete Bias and Variance ## Bonus/tomorrow: Completed: Note cleanup with Aliases (see Career and Study To - Do) To Do: Linear Algebra Note Migration Completed: Integrate with this daily note \t\tTo Do: Training, Test, and Dev Sets \t\tTo Do: regularization (#merge and integrate) \t\tTo Do: Cost and Loss Completed: Big overhaul of Bias and Variance with regards to todays notes. \n",
      "üé¨ START 13 üîö END 186\n",
      "ü™Ö SUBSECTION: \n",
      " # Machine Learning Specialization Notes: These notes will be integrated into other notes but kept here in their entirety. ## Bias and Variance Machine Learning Specialization Advanced Machine Learning Algorithms, Week 3  Models almost never work the first time you try them out. Let's see how we can fix them.  ### Diagnosing Bias/Variance See Bias and Variance  If you have more features, you can't visualize Bias and Variance  A more systematic way to see if you have high bias or high variance is to look at the performance of the algorithm on the Training, Test, and Dev Sets|training set and dev set  A characteristic of a High Bias (under-fit) model is that $Jtrain$ (cost of the Training, Test, and Dev Sets|training set) is high. A characteristic of a High Variance (overfit) model is that $J_{cv}$ is high but $J_{train}$ is low. #### Slide !Screenshot 2023-10-17 at 1.26.00 PM.png  ### Understanding Bias and Variance Take degree of polynomial $d$ As $d$ increases, that is to say we add degrees to the polynomial $J_{train}({\\vec{w},b})$  will decrease. $J_{cv}(\\vec{w},b)$ dips towards the middle, showing the ideal degree for the polynomial.  #### Slide !Screenshot 2023-10-17 at 1.29.21 PM.png  ### Technique for Diagnosing Bias and Variance How do you tell if your algorithm has a high bias or variance problem?  One key takeaway is that high bias means it is not even doing well on the training set and high variance means it does much worse on the cross validation set and the training set. #### High Bias (under-fit) In an under-fit model $J_{train}$ will be high and $J_{train} \\approx J_{cv}$  #### High Variance (overfit) In an over-fit model $J_{cv} \\gg J_{train}$ and $J_{train}$ will be low. #### High Bias and High Variance In a model with both high bias and high variance $J_{train}$ will be high and $J_{cv} \\gg J_{train}$  This is rare, and doesn't really happen for linear models with one $d$, but it does happen. This is when it overfits for some part of the input and under-fits for another part of the input. #### Slide !Screenshot 2023-10-17 at 1.38.51 PM.png  ## Regularization and bias/variance ### Linear Regression with Regularization Regularized Linear Regression model: Lambda $\\lambda$ is the regularization parameter (Big #merge and cleanup needed here in the regularization, Regularized Linear Regression, cost, Cost and Loss, loss, Loss Function, etc. Aliases are needed in many of these)  Take this model: $$f_{\\vec{w},b}(x)=w_1+w_2x^2 + w_4x^4+b$$ With this regularization|regularized Cost and Loss $$J(\\vec{w},b)=\\frac{1}{2m} \\sum_{i=1}^{m}(f_{\\vec{w},{b}}(\\vec{x}^{(i)}-y^{(i)})^2+\\frac{\\lambda}{2m} \\sum_{j=1}^nw_j^2$$  If lambda is large, say $\\lambda = 10,000$ than $w_1 \\approx 0, W_2 \\approx 0$ and $f_{\\vec{x},b}\\vec{x} \\approx b$, thus the model creates a flat line. This is underfit, $J_{train}()  On the other hand, if we set $\\lambda=0$, then we have a forth order polynomial with no regularization. We end up with a very overfit curve.  So, how do we find a good value for $\\lambda$? ##### Slide !Screenshot 2023-10-17 at 3.56.53 PM.png ### Choosing the Regularization parameter $\\lambda$   This will be similar to choosing $d$ with cross validation Try multiple values for $\\lambda$ and then choose the option with the lowest cost. ### Slide !Screenshot 2023-10-17 at 4.15.27 PM.png ### Bias and Variance as a Function of regularization parameter $\\lambda$  Cross Validation tries out many versions of $\\lambda$ and then chooses the one with the lowest cost. Cross validation will help find us a a good value of $d$ as well as $\\lambda$  #### Slide !Screenshot 2023-10-17 at 4.14.07 PM.png   ## Establishing A Baseline Level of Performance ### Speech Recognition example Job is to take in audio and output the text of what a person is saying  Training error $J_{train}$ is percentage of audio clips that the program does not transcribe correctly in it's entirety. Lets say: \tHuman Level Performance: 10.6 \t$J_{train}$: 10.8% \t$J_{cv}$: 14.8%  Why is human level error so high? There is lots of noise in the audio. It seems unfair to expect a learning algorithm to do much better. It is thus is more useful to measure the training error against the human error. So, looking at these results, $J_{train}$ is only 0.2$ higher than the human level performance whereas $J_{cv}$ is a full 4.2% higher. We can thus conclude that this algorithm has more of a variance problem than a bias problem. ### Establishing a baseline level of performance What is the level of error you can reasonably hope to get to? - Human level performance - Competing algorithms performance - Guess based on experience ### Bias Variance Examples Gap between baseline and training error shows high bias. A gap between training error and cross validation error shows high variance.  If your goal is perfection, the baseline would be zero. But for a lot of real world examples, like audio recognition, there is a lot of noise in the data, so you need a higher baseline.  If there is a gap between all three is high you have both high bias and high variance. #### Slide !Screenshot 2023-10-17 at 4.42.49 PM.png   ## Learning Curves  Noted in Learning Curves ### Overview Learning curves help understand how your learning algorithm is doing as a function of the amount of experience it has. Experience being the number of training examples it has.  The bigger the training set the harder it is to fit all examples perfectly. Thus; as the training set increases so does the training error $J_{train}(\\vec{w},b)$   Plotting a learning curve by training different models based on different subsets of training data is computationally expensive, so in practice it isn't done that often. But, it's a good mental visualization. #### Slides !Screenshot 2023-10-17 at 5.40.04 PM.png    ### High Bias Example If a learning algorithm suffers from high bias, getting more training data will not (by itself) help that much. #### Slide for High Bias !Screenshot 2023-10-17 at 5.44.29 PM.png  ### High Variance Example If a learning algorithm suffers from high variance, getting more training data is likely to help. #### Slide for High Variance !Screenshot 2023-10-17 at 5.49.45 PM.png    ## Deciding what to try next revisited ### Examples when Debugging an Algorithm: Get more training examples: fixes high variance Try smaller set of features: fixes high variance Try getting additional features: fixes high bias \tExamples; an algorithm that lacks information wont even do well on the training set Adding polynomial features ($x_1^2,X_2^2, etc$): Fixes high bias Decreasing $\\lambda$ Fixes high bias Increasing $\\lambda$ Fixes high variance \tForces the algorithm to force a smoother function.  **Note!** Don't randomly throw away training examples just to fix a high bias problem. ### Takeaway #merge with Bias and Variance ? If your algorithm has high variance, try simplifying your model or getting more training data. Simplification can mean a smaller set of features or an increased regularization If your algorithm has high bias, that is to say its not even doing well on the training set, you mainly need to make your model more powerful and flexible to fit more complex functions. To do so you can give it additional features, add polynomial features, or decrease $\\lambda$   ## Bias/Variance and Neural Networks ### The bias variance tradeoff Simple model = high bias Complex model = high variance Before neural networks, we had to worry about balancing this complexity between bias and variance. With neural networks we now are mostly worried about high variance.  ### Neural Networks and bias variance Large Neural Network|neural networks are low bias machines. If you make your neural network large enough you can almost always fit your training set well. #### Recipe for decreasing bias with a neural network 1. Train a neural network 2.  If the training set error $J_{train}(\\vec{w},b)$ is high relative to your baseline, increase the size of the neural network by adding hidden layers. 3. Once $J_{train}(\\vec{w},b)$ is low enough, see if it does well on the cross validation set 4.  If the cross validation set $J_{cv}(\\vec{w},b)$ is too high, add more data, then test again from step 2. 5. Repeat until $J_{cv}(\\vec{w},b)$  is low enough for your liking. ##### Slide Illustration !Screenshot 2023-10-17 at 6.38.46 PM.png ### Limitations and Notes Bigger networks are restricted by your computing power, data is restricted to the amount of data you have.  Sometimes you will pingpong back between high bias and high variance as you move through this recipe and develop a machine learning algorithm. Use these observations to shape what you do next in the process.  ### Neural Networks and Regularization A large neural network will usually do as well or better than a smaller one so long as regularization is chosen appropriately. Of course, larger neural networks are more computationally expensive. ### Neural Network Regularization #important  #function and TensorFlow implementation: !Screenshot 2023-10-17 at 6.46.28 PM.png*Note usually don't regularize B, it doesn't really affect anything*\n",
      "üé¨ START 186 üîö END 2\n",
      "ü™Ö SUBSECTION: \n",
      " \n",
      "üé¨ START 2 üîö END 5\n",
      "ü™Ö SUBSECTION: \n",
      " ## Priority Completed: Complete Python module Completed: Complete Bias and Variance\n",
      "üé¨ START 5 üîö END 15\n",
      "ü™Ö SUBSECTION: \n",
      " ## Bonus/tomorrow: Completed: Note cleanup with Aliases (see Career and Study To - Do) To Do: Linear Algebra Note Migration Completed: Integrate with this daily note \t\tTo Do: Training, Test, and Dev Sets \t\tTo Do: regularization (#merge and integrate) \t\tTo Do: Cost and Loss Completed: Big overhaul of Bias and Variance with regards to todays notes.  # Machine Learning Specialization Notes: These notes will be integrated into other notes but kept here in their entirety.\n",
      "üé¨ START 15 üîö END 56\n",
      "ü™Ö SUBSECTION: \n",
      " ## Bias and Variance Machine Learning Specialization Advanced Machine Learning Algorithms, Week 3  Models almost never work the first time you try them out. Let's see how we can fix them.  ### Diagnosing Bias/Variance See Bias and Variance  If you have more features, you can't visualize Bias and Variance  A more systematic way to see if you have high bias or high variance is to look at the performance of the algorithm on the Training, Test, and Dev Sets|training set and dev set  A characteristic of a High Bias (under-fit) model is that $Jtrain$ (cost of the Training, Test, and Dev Sets|training set) is high. A characteristic of a High Variance (overfit) model is that $J_{cv}$ is high but $J_{train}$ is low. #### Slide !Screenshot 2023-10-17 at 1.26.00 PM.png  ### Understanding Bias and Variance Take degree of polynomial $d$ As $d$ increases, that is to say we add degrees to the polynomial $J_{train}({\\vec{w},b})$  will decrease. $J_{cv}(\\vec{w},b)$ dips towards the middle, showing the ideal degree for the polynomial.  #### Slide !Screenshot 2023-10-17 at 1.29.21 PM.png  ### Technique for Diagnosing Bias and Variance How do you tell if your algorithm has a high bias or variance problem?  One key takeaway is that high bias means it is not even doing well on the training set and high variance means it does much worse on the cross validation set and the training set. #### High Bias (under-fit) In an under-fit model $J_{train}$ will be high and $J_{train} \\approx J_{cv}$  #### High Variance (overfit) In an over-fit model $J_{cv} \\gg J_{train}$ and $J_{train}$ will be low. #### High Bias and High Variance In a model with both high bias and high variance $J_{train}$ will be high and $J_{cv} \\gg J_{train}$  This is rare, and doesn't really happen for linear models with one $d$, but it does happen. This is when it overfits for some part of the input and under-fits for another part of the input. #### Slide !Screenshot 2023-10-17 at 1.38.51 PM.png \n",
      "üé¨ START 56 üîö END 86\n",
      "ü™Ö SUBSECTION: \n",
      " ## Regularization and bias/variance ### Linear Regression with Regularization Regularized Linear Regression model: Lambda $\\lambda$ is the regularization parameter (Big #merge and cleanup needed here in the regularization, Regularized Linear Regression, cost, Cost and Loss, loss, Loss Function, etc. Aliases are needed in many of these)  Take this model: $$f_{\\vec{w},b}(x)=w_1+w_2x^2 + w_4x^4+b$$ With this regularization|regularized Cost and Loss $$J(\\vec{w},b)=\\frac{1}{2m} \\sum_{i=1}^{m}(f_{\\vec{w},{b}}(\\vec{x}^{(i)}-y^{(i)})^2+\\frac{\\lambda}{2m} \\sum_{j=1}^nw_j^2$$  If lambda is large, say $\\lambda = 10,000$ than $w_1 \\approx 0, W_2 \\approx 0$ and $f_{\\vec{x},b}\\vec{x} \\approx b$, thus the model creates a flat line. This is underfit, $J_{train}()  On the other hand, if we set $\\lambda=0$, then we have a forth order polynomial with no regularization. We end up with a very overfit curve.  So, how do we find a good value for $\\lambda$? ##### Slide !Screenshot 2023-10-17 at 3.56.53 PM.png ### Choosing the Regularization parameter $\\lambda$   This will be similar to choosing $d$ with cross validation Try multiple values for $\\lambda$ and then choose the option with the lowest cost. ### Slide !Screenshot 2023-10-17 at 4.15.27 PM.png ### Bias and Variance as a Function of regularization parameter $\\lambda$  Cross Validation tries out many versions of $\\lambda$ and then chooses the one with the lowest cost. Cross validation will help find us a a good value of $d$ as well as $\\lambda$  #### Slide !Screenshot 2023-10-17 at 4.14.07 PM.png  \n",
      "üé¨ START 86 üîö END 116\n",
      "ü™Ö SUBSECTION: \n",
      " ## Establishing A Baseline Level of Performance ### Speech Recognition example Job is to take in audio and output the text of what a person is saying  Training error $J_{train}$ is percentage of audio clips that the program does not transcribe correctly in it's entirety. Lets say: \tHuman Level Performance: 10.6 \t$J_{train}$: 10.8% \t$J_{cv}$: 14.8%  Why is human level error so high? There is lots of noise in the audio. It seems unfair to expect a learning algorithm to do much better. It is thus is more useful to measure the training error against the human error. So, looking at these results, $J_{train}$ is only 0.2$ higher than the human level performance whereas $J_{cv}$ is a full 4.2% higher. We can thus conclude that this algorithm has more of a variance problem than a bias problem. ### Establishing a baseline level of performance What is the level of error you can reasonably hope to get to? - Human level performance - Competing algorithms performance - Guess based on experience ### Bias Variance Examples Gap between baseline and training error shows high bias. A gap between training error and cross validation error shows high variance.  If your goal is perfection, the baseline would be zero. But for a lot of real world examples, like audio recognition, there is a lot of noise in the data, so you need a higher baseline.  If there is a gap between all three is high you have both high bias and high variance. #### Slide !Screenshot 2023-10-17 at 4.42.49 PM.png  \n",
      "üé¨ START 116 üîö END 142\n",
      "ü™Ö SUBSECTION: \n",
      " ## Learning Curves  Noted in Learning Curves ### Overview Learning curves help understand how your learning algorithm is doing as a function of the amount of experience it has. Experience being the number of training examples it has.  The bigger the training set the harder it is to fit all examples perfectly. Thus; as the training set increases so does the training error $J_{train}(\\vec{w},b)$   Plotting a learning curve by training different models based on different subsets of training data is computationally expensive, so in practice it isn't done that often. But, it's a good mental visualization. #### Slides !Screenshot 2023-10-17 at 5.40.04 PM.png    ### High Bias Example If a learning algorithm suffers from high bias, getting more training data will not (by itself) help that much. #### Slide for High Bias !Screenshot 2023-10-17 at 5.44.29 PM.png  ### High Variance Example If a learning algorithm suffers from high variance, getting more training data is likely to help. #### Slide for High Variance !Screenshot 2023-10-17 at 5.49.45 PM.png   \n",
      "üé¨ START 142 üîö END 159\n",
      "ü™Ö SUBSECTION: \n",
      " ## Deciding what to try next revisited ### Examples when Debugging an Algorithm: Get more training examples: fixes high variance Try smaller set of features: fixes high variance Try getting additional features: fixes high bias \tExamples; an algorithm that lacks information wont even do well on the training set Adding polynomial features ($x_1^2,X_2^2, etc$): Fixes high bias Decreasing $\\lambda$ Fixes high bias Increasing $\\lambda$ Fixes high variance \tForces the algorithm to force a smoother function.  **Note!** Don't randomly throw away training examples just to fix a high bias problem. ### Takeaway #merge with Bias and Variance ? If your algorithm has high variance, try simplifying your model or getting more training data. Simplification can mean a smaller set of features or an increased regularization If your algorithm has high bias, that is to say its not even doing well on the training set, you mainly need to make your model more powerful and flexible to fit more complex functions. To do so you can give it additional features, add polynomial features, or decrease $\\lambda$  \n",
      "üé¨ START 159 üîö END 187\n",
      "ü™Ö SUBSECTION: \n",
      " ## Bias/Variance and Neural Networks ### The bias variance tradeoff Simple model = high bias Complex model = high variance Before neural networks, we had to worry about balancing this complexity between bias and variance. With neural networks we now are mostly worried about high variance.  ### Neural Networks and bias variance Large Neural Network|neural networks are low bias machines. If you make your neural network large enough you can almost always fit your training set well. #### Recipe for decreasing bias with a neural network 1. Train a neural network 2.  If the training set error $J_{train}(\\vec{w},b)$ is high relative to your baseline, increase the size of the neural network by adding hidden layers. 3. Once $J_{train}(\\vec{w},b)$ is low enough, see if it does well on the cross validation set 4.  If the cross validation set $J_{cv}(\\vec{w},b)$ is too high, add more data, then test again from step 2. 5. Repeat until $J_{cv}(\\vec{w},b)$  is low enough for your liking. ##### Slide Illustration !Screenshot 2023-10-17 at 6.38.46 PM.png ### Limitations and Notes Bigger networks are restricted by your computing power, data is restricted to the amount of data you have.  Sometimes you will pingpong back between high bias and high variance as you move through this recipe and develop a machine learning algorithm. Use these observations to shape what you do next in the process.  ### Neural Networks and Regularization A large neural network will usually do as well or better than a smaller one so long as regularization is chosen appropriately. Of course, larger neural networks are more computationally expensive. ### Neural Network Regularization #important  #function and TensorFlow implementation: !Screenshot 2023-10-17 at 6.46.28 PM.png*Note usually don't regularize B, it doesn't really affect anything* # Python Notes\n",
      "üé¨ START 187 üîö END 191\n",
      "ü™Ö SUBSECTION: \n",
      " ## Keywords From codecademy - Continue Keyword: used inside a loop to skip the remaining loop code block and begin the next loop iteration. - Break keyword escapes the loop, regardless of the iteration number. Once break executes the program will continue to execute after the loop.\n",
      "üé¨ START 191 üîö END 20\n",
      "ü™Ö SUBSECTION: \n",
      " \n",
      "üé¨ START 20 üîö END 32\n",
      "ü™Ö SUBSECTION: \n",
      " ### Diagnosing Bias/Variance See Bias and Variance  If you have more features, you can't visualize Bias and Variance  A more systematic way to see if you have high bias or high variance is to look at the performance of the algorithm on the Training, Test, and Dev Sets|training set and dev set  A characteristic of a High Bias (under-fit) model is that $Jtrain$ (cost of the Training, Test, and Dev Sets|training set) is high. A characteristic of a High Variance (overfit) model is that $J_{cv}$ is high but $J_{train}$ is low. #### Slide !Screenshot 2023-10-17 at 1.26.00 PM.png \n",
      "üé¨ START 32 üîö END 41\n",
      "ü™Ö SUBSECTION: \n",
      " ### Understanding Bias and Variance Take degree of polynomial $d$ As $d$ increases, that is to say we add degrees to the polynomial $J_{train}({\\vec{w},b})$  will decrease. $J_{cv}(\\vec{w},b)$ dips towards the middle, showing the ideal degree for the polynomial.  #### Slide !Screenshot 2023-10-17 at 1.29.21 PM.png \n",
      "üé¨ START 41 üîö END 57\n",
      "ü™Ö SUBSECTION: \n",
      " ### Technique for Diagnosing Bias and Variance How do you tell if your algorithm has a high bias or variance problem?  One key takeaway is that high bias means it is not even doing well on the training set and high variance means it does much worse on the cross validation set and the training set. #### High Bias (under-fit) In an under-fit model $J_{train}$ will be high and $J_{train} \\approx J_{cv}$  #### High Variance (overfit) In an over-fit model $J_{cv} \\gg J_{train}$ and $J_{train}$ will be low. #### High Bias and High Variance In a model with both high bias and high variance $J_{train}$ will be high and $J_{cv} \\gg J_{train}$  This is rare, and doesn't really happen for linear models with one $d$, but it does happen. This is when it overfits for some part of the input and under-fits for another part of the input. #### Slide !Screenshot 2023-10-17 at 1.38.51 PM.png  ## Regularization and bias/variance\n",
      "üé¨ START 57 üîö END 73\n",
      "ü™Ö SUBSECTION: \n",
      " ### Linear Regression with Regularization Regularized Linear Regression model: Lambda $\\lambda$ is the regularization parameter (Big #merge and cleanup needed here in the regularization, Regularized Linear Regression, cost, Cost and Loss, loss, Loss Function, etc. Aliases are needed in many of these)  Take this model: $$f_{\\vec{w},b}(x)=w_1+w_2x^2 + w_4x^4+b$$ With this regularization|regularized Cost and Loss $$J(\\vec{w},b)=\\frac{1}{2m} \\sum_{i=1}^{m}(f_{\\vec{w},{b}}(\\vec{x}^{(i)}-y^{(i)})^2+\\frac{\\lambda}{2m} \\sum_{j=1}^nw_j^2$$  If lambda is large, say $\\lambda = 10,000$ than $w_1 \\approx 0, W_2 \\approx 0$ and $f_{\\vec{x},b}\\vec{x} \\approx b$, thus the model creates a flat line. This is underfit, $J_{train}()  On the other hand, if we set $\\lambda=0$, then we have a forth order polynomial with no regularization. We end up with a very overfit curve.  So, how do we find a good value for $\\lambda$? ##### Slide !Screenshot 2023-10-17 at 3.56.53 PM.png\n",
      "üé¨ START 73 üîö END 77\n",
      "ü™Ö SUBSECTION: \n",
      " ### Choosing the Regularization parameter $\\lambda$   This will be similar to choosing $d$ with cross validation Try multiple values for $\\lambda$ and then choose the option with the lowest cost.\n",
      "üé¨ START 77 üîö END 79\n",
      "ü™Ö SUBSECTION: \n",
      " ### Slide !Screenshot 2023-10-17 at 4.15.27 PM.png\n",
      "üé¨ START 79 üîö END 87\n",
      "ü™Ö SUBSECTION: \n",
      " ### Bias and Variance as a Function of regularization parameter $\\lambda$  Cross Validation tries out many versions of $\\lambda$ and then chooses the one with the lowest cost. Cross validation will help find us a a good value of $d$ as well as $\\lambda$  #### Slide !Screenshot 2023-10-17 at 4.14.07 PM.png   ## Establishing A Baseline Level of Performance\n",
      "üé¨ START 87 üîö END 100\n",
      "ü™Ö SUBSECTION: \n",
      " ### Speech Recognition example Job is to take in audio and output the text of what a person is saying  Training error $J_{train}$ is percentage of audio clips that the program does not transcribe correctly in it's entirety. Lets say: \tHuman Level Performance: 10.6 \t$J_{train}$: 10.8% \t$J_{cv}$: 14.8%  Why is human level error so high? There is lots of noise in the audio. It seems unfair to expect a learning algorithm to do much better. It is thus is more useful to measure the training error against the human error. So, looking at these results, $J_{train}$ is only 0.2$ higher than the human level performance whereas $J_{cv}$ is a full 4.2% higher. We can thus conclude that this algorithm has more of a variance problem than a bias problem.\n",
      "üé¨ START 100 üîö END 105\n",
      "ü™Ö SUBSECTION: \n",
      " ### Establishing a baseline level of performance What is the level of error you can reasonably hope to get to? - Human level performance - Competing algorithms performance - Guess based on experience\n",
      "üé¨ START 105 üîö END 119\n",
      "ü™Ö SUBSECTION: \n",
      " ### Bias Variance Examples Gap between baseline and training error shows high bias. A gap between training error and cross validation error shows high variance.  If your goal is perfection, the baseline would be zero. But for a lot of real world examples, like audio recognition, there is a lot of noise in the data, so you need a higher baseline.  If there is a gap between all three is high you have both high bias and high variance. #### Slide !Screenshot 2023-10-17 at 4.42.49 PM.png   ## Learning Curves  Noted in Learning Curves\n",
      "üé¨ START 119 üîö END 130\n",
      "ü™Ö SUBSECTION: \n",
      " ### Overview Learning curves help understand how your learning algorithm is doing as a function of the amount of experience it has. Experience being the number of training examples it has.  The bigger the training set the harder it is to fit all examples perfectly. Thus; as the training set increases so does the training error $J_{train}(\\vec{w},b)$   Plotting a learning curve by training different models based on different subsets of training data is computationally expensive, so in practice it isn't done that often. But, it's a good mental visualization. #### Slides !Screenshot 2023-10-17 at 5.40.04 PM.png   \n",
      "üé¨ START 130 üîö END 135\n",
      "ü™Ö SUBSECTION: \n",
      " ### High Bias Example If a learning algorithm suffers from high bias, getting more training data will not (by itself) help that much. #### Slide for High Bias !Screenshot 2023-10-17 at 5.44.29 PM.png \n",
      "üé¨ START 135 üîö END 143\n",
      "ü™Ö SUBSECTION: \n",
      " ### High Variance Example If a learning algorithm suffers from high variance, getting more training data is likely to help. #### Slide for High Variance !Screenshot 2023-10-17 at 5.49.45 PM.png    ## Deciding what to try next revisited\n",
      "üé¨ START 143 üîö END 154\n",
      "ü™Ö SUBSECTION: \n",
      " ### Examples when Debugging an Algorithm: Get more training examples: fixes high variance Try smaller set of features: fixes high variance Try getting additional features: fixes high bias \tExamples; an algorithm that lacks information wont even do well on the training set Adding polynomial features ($x_1^2,X_2^2, etc$): Fixes high bias Decreasing $\\lambda$ Fixes high bias Increasing $\\lambda$ Fixes high variance \tForces the algorithm to force a smoother function.  **Note!** Don't randomly throw away training examples just to fix a high bias problem.\n",
      "üé¨ START 154 üîö END 160\n",
      "ü™Ö SUBSECTION: \n",
      " ### Takeaway #merge with Bias and Variance ? If your algorithm has high variance, try simplifying your model or getting more training data. Simplification can mean a smaller set of features or an increased regularization If your algorithm has high bias, that is to say its not even doing well on the training set, you mainly need to make your model more powerful and flexible to fit more complex functions. To do so you can give it additional features, add polynomial features, or decrease $\\lambda$   ## Bias/Variance and Neural Networks\n",
      "üé¨ START 160 üîö END 164\n",
      "ü™Ö SUBSECTION: \n",
      " ### The bias variance tradeoff Simple model = high bias Complex model = high variance Before neural networks, we had to worry about balancing this complexity between bias and variance. With neural networks we now are mostly worried about high variance. \n",
      "üé¨ START 164 üîö END 175\n",
      "ü™Ö SUBSECTION: \n",
      " ### Neural Networks and bias variance Large Neural Network|neural networks are low bias machines. If you make your neural network large enough you can almost always fit your training set well. #### Recipe for decreasing bias with a neural network 1. Train a neural network 2.  If the training set error $J_{train}(\\vec{w},b)$ is high relative to your baseline, increase the size of the neural network by adding hidden layers. 3. Once $J_{train}(\\vec{w},b)$ is low enough, see if it does well on the cross validation set 4.  If the cross validation set $J_{cv}(\\vec{w},b)$ is too high, add more data, then test again from step 2. 5. Repeat until $J_{cv}(\\vec{w},b)$  is low enough for your liking. ##### Slide Illustration !Screenshot 2023-10-17 at 6.38.46 PM.png\n",
      "üé¨ START 175 üîö END 180\n",
      "ü™Ö SUBSECTION: \n",
      " ### Limitations and Notes Bigger networks are restricted by your computing power, data is restricted to the amount of data you have.  Sometimes you will pingpong back between high bias and high variance as you move through this recipe and develop a machine learning algorithm. Use these observations to shape what you do next in the process. \n",
      "üé¨ START 180 üîö END 183\n",
      "ü™Ö SUBSECTION: \n",
      " ### Neural Networks and Regularization A large neural network will usually do as well or better than a smaller one so long as regularization is chosen appropriately. Of course, larger neural networks are more computationally expensive.\n",
      "üé¨ START 183 üîö END 29\n",
      "ü™Ö SUBSECTION: \n",
      " \n",
      "üé¨ START 29 üîö END 38\n",
      "ü™Ö SUBSECTION: \n",
      " #### Slide !Screenshot 2023-10-17 at 1.26.00 PM.png  ### Understanding Bias and Variance Take degree of polynomial $d$ As $d$ increases, that is to say we add degrees to the polynomial $J_{train}({\\vec{w},b})$  will decrease. $J_{cv}(\\vec{w},b)$ dips towards the middle, showing the ideal degree for the polynomial. \n",
      "üé¨ START 38 üîö END 45\n",
      "ü™Ö SUBSECTION: \n",
      " #### Slide !Screenshot 2023-10-17 at 1.29.21 PM.png  ### Technique for Diagnosing Bias and Variance How do you tell if your algorithm has a high bias or variance problem?  One key takeaway is that high bias means it is not even doing well on the training set and high variance means it does much worse on the cross validation set and the training set.\n",
      "üé¨ START 45 üîö END 47\n",
      "ü™Ö SUBSECTION: \n",
      " #### High Bias (under-fit) In an under-fit model $J_{train}$ will be high and $J_{train} \\approx J_{cv}$ \n",
      "üé¨ START 47 üîö END 49\n",
      "ü™Ö SUBSECTION: \n",
      " #### High Variance (overfit) In an over-fit model $J_{cv} \\gg J_{train}$ and $J_{train}$ will be low.\n",
      "üé¨ START 49 üîö END 53\n",
      "ü™Ö SUBSECTION: \n",
      " #### High Bias and High Variance In a model with both high bias and high variance $J_{train}$ will be high and $J_{cv} \\gg J_{train}$  This is rare, and doesn't really happen for linear models with one $d$, but it does happen. This is when it overfits for some part of the input and under-fits for another part of the input.\n",
      "üé¨ START 53 üîö END 82\n",
      "ü™Ö SUBSECTION: \n",
      " #### Slide !Screenshot 2023-10-17 at 1.38.51 PM.png  ## Regularization and bias/variance ### Linear Regression with Regularization Regularized Linear Regression model: Lambda $\\lambda$ is the regularization parameter (Big #merge and cleanup needed here in the regularization, Regularized Linear Regression, cost, Cost and Loss, loss, Loss Function, etc. Aliases are needed in many of these)  Take this model: $$f_{\\vec{w},b}(x)=w_1+w_2x^2 + w_4x^4+b$$ With this regularization|regularized Cost and Loss $$J(\\vec{w},b)=\\frac{1}{2m} \\sum_{i=1}^{m}(f_{\\vec{w},{b}}(\\vec{x}^{(i)}-y^{(i)})^2+\\frac{\\lambda}{2m} \\sum_{j=1}^nw_j^2$$  If lambda is large, say $\\lambda = 10,000$ than $w_1 \\approx 0, W_2 \\approx 0$ and $f_{\\vec{x},b}\\vec{x} \\approx b$, thus the model creates a flat line. This is underfit, $J_{train}()  On the other hand, if we set $\\lambda=0$, then we have a forth order polynomial with no regularization. We end up with a very overfit curve.  So, how do we find a good value for $\\lambda$? ##### Slide !Screenshot 2023-10-17 at 3.56.53 PM.png ### Choosing the Regularization parameter $\\lambda$   This will be similar to choosing $d$ with cross validation Try multiple values for $\\lambda$ and then choose the option with the lowest cost. ### Slide !Screenshot 2023-10-17 at 4.15.27 PM.png ### Bias and Variance as a Function of regularization parameter $\\lambda$  Cross Validation tries out many versions of $\\lambda$ and then chooses the one with the lowest cost. Cross validation will help find us a a good value of $d$ as well as $\\lambda$ \n",
      "üé¨ START 82 üîö END 112\n",
      "ü™Ö SUBSECTION: \n",
      " #### Slide !Screenshot 2023-10-17 at 4.14.07 PM.png   ## Establishing A Baseline Level of Performance ### Speech Recognition example Job is to take in audio and output the text of what a person is saying  Training error $J_{train}$ is percentage of audio clips that the program does not transcribe correctly in it's entirety. Lets say: \tHuman Level Performance: 10.6 \t$J_{train}$: 10.8% \t$J_{cv}$: 14.8%  Why is human level error so high? There is lots of noise in the audio. It seems unfair to expect a learning algorithm to do much better. It is thus is more useful to measure the training error against the human error. So, looking at these results, $J_{train}$ is only 0.2$ higher than the human level performance whereas $J_{cv}$ is a full 4.2% higher. We can thus conclude that this algorithm has more of a variance problem than a bias problem. ### Establishing a baseline level of performance What is the level of error you can reasonably hope to get to? - Human level performance - Competing algorithms performance - Guess based on experience ### Bias Variance Examples Gap between baseline and training error shows high bias. A gap between training error and cross validation error shows high variance.  If your goal is perfection, the baseline would be zero. But for a lot of real world examples, like audio recognition, there is a lot of noise in the data, so you need a higher baseline.  If there is a gap between all three is high you have both high bias and high variance.\n",
      "üé¨ START 112 üîö END 125\n",
      "ü™Ö SUBSECTION: \n",
      " #### Slide !Screenshot 2023-10-17 at 4.42.49 PM.png   ## Learning Curves  Noted in Learning Curves ### Overview Learning curves help understand how your learning algorithm is doing as a function of the amount of experience it has. Experience being the number of training examples it has.  The bigger the training set the harder it is to fit all examples perfectly. Thus; as the training set increases so does the training error $J_{train}(\\vec{w},b)$   Plotting a learning curve by training different models based on different subsets of training data is computationally expensive, so in practice it isn't done that often. But, it's a good mental visualization.\n",
      "üé¨ START 125 üîö END 132\n",
      "ü™Ö SUBSECTION: \n",
      " #### Slides !Screenshot 2023-10-17 at 5.40.04 PM.png    ### High Bias Example If a learning algorithm suffers from high bias, getting more training data will not (by itself) help that much.\n",
      "üé¨ START 132 üîö END 137\n",
      "ü™Ö SUBSECTION: \n",
      " #### Slide for High Bias !Screenshot 2023-10-17 at 5.44.29 PM.png  ### High Variance Example If a learning algorithm suffers from high variance, getting more training data is likely to help.\n",
      "üé¨ START 137 üîö END 167\n",
      "ü™Ö SUBSECTION: \n",
      " #### Slide for High Variance !Screenshot 2023-10-17 at 5.49.45 PM.png    ## Deciding what to try next revisited ### Examples when Debugging an Algorithm: Get more training examples: fixes high variance Try smaller set of features: fixes high variance Try getting additional features: fixes high bias \tExamples; an algorithm that lacks information wont even do well on the training set Adding polynomial features ($x_1^2,X_2^2, etc$): Fixes high bias Decreasing $\\lambda$ Fixes high bias Increasing $\\lambda$ Fixes high variance \tForces the algorithm to force a smoother function.  **Note!** Don't randomly throw away training examples just to fix a high bias problem. ### Takeaway #merge with Bias and Variance ? If your algorithm has high variance, try simplifying your model or getting more training data. Simplification can mean a smaller set of features or an increased regularization If your algorithm has high bias, that is to say its not even doing well on the training set, you mainly need to make your model more powerful and flexible to fit more complex functions. To do so you can give it additional features, add polynomial features, or decrease $\\lambda$   ## Bias/Variance and Neural Networks ### The bias variance tradeoff Simple model = high bias Complex model = high variance Before neural networks, we had to worry about balancing this complexity between bias and variance. With neural networks we now are mostly worried about high variance.  ### Neural Networks and bias variance Large Neural Network|neural networks are low bias machines. If you make your neural network large enough you can almost always fit your training set well.\n",
      "ü™ô SECTION to tokenize:  # Afternoon To-Do #meta #todoList  ## Priority Completed: Complete Python module Completed: Complete Bias and Variance ## Bonus/tomorrow: Completed: Note cleanup with Aliases (see Career and Study To - Do) To Do: Linear Algebra Note Migration Completed: Integrate with this daily note \t\tTo Do: Training, Test, and Dev Sets \t\tTo Do: regularization (#merge and integrate) \t\tTo Do: Cost and Loss Completed: Big overhaul of Bias and Variance with regards to todays notes. \n",
      "üí∂ section has 122\n",
      "ü™ô SECTION to tokenize:  # Machine Learning Specialization Notes: These notes will be integrated into other notes but kept here in their entirety. ## Bias and Variance Machine Learning Specialization Advanced Machine Learning Algorithms, Week 3  Models almost never work the first time you try them out. Let's see how we can fix them.  ### Diagnosing Bias/Variance See Bias and Variance  If you have more features, you can't visualize Bias and Variance  A more systematic way to see if you have high bias or high variance is to look at the performance of the algorithm on the Training, Test, and Dev Sets|training set and dev set  A characteristic of a High Bias (under-fit) model is that $Jtrain$ (cost of the Training, Test, and Dev Sets|training set) is high. A characteristic of a High Variance (overfit) model is that $J_{cv}$ is high but $J_{train}$ is low. #### Slide !Screenshot 2023-10-17 at 1.26.00 PM.png  ### Understanding Bias and Variance Take degree of polynomial $d$ As $d$ increases, that is to say we add degrees to the polynomial $J_{train}({\\vec{w},b})$  will decrease. $J_{cv}(\\vec{w},b)$ dips towards the middle, showing the ideal degree for the polynomial.  #### Slide !Screenshot 2023-10-17 at 1.29.21 PM.png  ### Technique for Diagnosing Bias and Variance How do you tell if your algorithm has a high bias or variance problem?  One key takeaway is that high bias means it is not even doing well on the training set and high variance means it does much worse on the cross validation set and the training set. #### High Bias (under-fit) In an under-fit model $J_{train}$ will be high and $J_{train} \\approx J_{cv}$  #### High Variance (overfit) In an over-fit model $J_{cv} \\gg J_{train}$ and $J_{train}$ will be low. #### High Bias and High Variance In a model with both high bias and high variance $J_{train}$ will be high and $J_{cv} \\gg J_{train}$  This is rare, and doesn't really happen for linear models with one $d$, but it does happen. This is when it overfits for some part of the input and under-fits for another part of the input. #### Slide !Screenshot 2023-10-17 at 1.38.51 PM.png  ## Regularization and bias/variance ### Linear Regression with Regularization Regularized Linear Regression model: Lambda $\\lambda$ is the regularization parameter (Big #merge and cleanup needed here in the regularization, Regularized Linear Regression, cost, Cost and Loss, loss, Loss Function, etc. Aliases are needed in many of these)  Take this model: $$f_{\\vec{w},b}(x)=w_1+w_2x^2 + w_4x^4+b$$ With this regularization|regularized Cost and Loss $$J(\\vec{w},b)=\\frac{1}{2m} \\sum_{i=1}^{m}(f_{\\vec{w},{b}}(\\vec{x}^{(i)}-y^{(i)})^2+\\frac{\\lambda}{2m} \\sum_{j=1}^nw_j^2$$  If lambda is large, say $\\lambda = 10,000$ than $w_1 \\approx 0, W_2 \\approx 0$ and $f_{\\vec{x},b}\\vec{x} \\approx b$, thus the model creates a flat line. This is underfit, $J_{train}()  On the other hand, if we set $\\lambda=0$, then we have a forth order polynomial with no regularization. We end up with a very overfit curve.  So, how do we find a good value for $\\lambda$? ##### Slide !Screenshot 2023-10-17 at 3.56.53 PM.png ### Choosing the Regularization parameter $\\lambda$   This will be similar to choosing $d$ with cross validation Try multiple values for $\\lambda$ and then choose the option with the lowest cost. ### Slide !Screenshot 2023-10-17 at 4.15.27 PM.png ### Bias and Variance as a Function of regularization parameter $\\lambda$  Cross Validation tries out many versions of $\\lambda$ and then chooses the one with the lowest cost. Cross validation will help find us a a good value of $d$ as well as $\\lambda$  #### Slide !Screenshot 2023-10-17 at 4.14.07 PM.png   ## Establishing A Baseline Level of Performance ### Speech Recognition example Job is to take in audio and output the text of what a person is saying  Training error $J_{train}$ is percentage of audio clips that the program does not transcribe correctly in it's entirety. Lets say: \tHuman Level Performance: 10.6 \t$J_{train}$: 10.8% \t$J_{cv}$: 14.8%  Why is human level error so high? There is lots of noise in the audio. It seems unfair to expect a learning algorithm to do much better. It is thus is more useful to measure the training error against the human error. So, looking at these results, $J_{train}$ is only 0.2$ higher than the human level performance whereas $J_{cv}$ is a full 4.2% higher. We can thus conclude that this algorithm has more of a variance problem than a bias problem. ### Establishing a baseline level of performance What is the level of error you can reasonably hope to get to? - Human level performance - Competing algorithms performance - Guess based on experience ### Bias Variance Examples Gap between baseline and training error shows high bias. A gap between training error and cross validation error shows high variance.  If your goal is perfection, the baseline would be zero. But for a lot of real world examples, like audio recognition, there is a lot of noise in the data, so you need a higher baseline.  If there is a gap between all three is high you have both high bias and high variance. #### Slide !Screenshot 2023-10-17 at 4.42.49 PM.png   ## Learning Curves  Noted in Learning Curves ### Overview Learning curves help understand how your learning algorithm is doing as a function of the amount of experience it has. Experience being the number of training examples it has.  The bigger the training set the harder it is to fit all examples perfectly. Thus; as the training set increases so does the training error $J_{train}(\\vec{w},b)$   Plotting a learning curve by training different models based on different subsets of training data is computationally expensive, so in practice it isn't done that often. But, it's a good mental visualization. #### Slides !Screenshot 2023-10-17 at 5.40.04 PM.png    ### High Bias Example If a learning algorithm suffers from high bias, getting more training data will not (by itself) help that much. #### Slide for High Bias !Screenshot 2023-10-17 at 5.44.29 PM.png  ### High Variance Example If a learning algorithm suffers from high variance, getting more training data is likely to help. #### Slide for High Variance !Screenshot 2023-10-17 at 5.49.45 PM.png    ## Deciding what to try next revisited ### Examples when Debugging an Algorithm: Get more training examples: fixes high variance Try smaller set of features: fixes high variance Try getting additional features: fixes high bias \tExamples; an algorithm that lacks information wont even do well on the training set Adding polynomial features ($x_1^2,X_2^2, etc$): Fixes high bias Decreasing $\\lambda$ Fixes high bias Increasing $\\lambda$ Fixes high variance \tForces the algorithm to force a smoother function.  **Note!** Don't randomly throw away training examples just to fix a high bias problem. ### Takeaway #merge with Bias and Variance ? If your algorithm has high variance, try simplifying your model or getting more training data. Simplification can mean a smaller set of features or an increased regularization If your algorithm has high bias, that is to say its not even doing well on the training set, you mainly need to make your model more powerful and flexible to fit more complex functions. To do so you can give it additional features, add polynomial features, or decrease $\\lambda$   ## Bias/Variance and Neural Networks ### The bias variance tradeoff Simple model = high bias Complex model = high variance Before neural networks, we had to worry about balancing this complexity between bias and variance. With neural networks we now are mostly worried about high variance.  ### Neural Networks and bias variance Large Neural Network|neural networks are low bias machines. If you make your neural network large enough you can almost always fit your training set well. #### Recipe for decreasing bias with a neural network 1. Train a neural network 2.  If the training set error $J_{train}(\\vec{w},b)$ is high relative to your baseline, increase the size of the neural network by adding hidden layers. 3. Once $J_{train}(\\vec{w},b)$ is low enough, see if it does well on the cross validation set 4.  If the cross validation set $J_{cv}(\\vec{w},b)$ is too high, add more data, then test again from step 2. 5. Repeat until $J_{cv}(\\vec{w},b)$  is low enough for your liking. ##### Slide Illustration !Screenshot 2023-10-17 at 6.38.46 PM.png ### Limitations and Notes Bigger networks are restricted by your computing power, data is restricted to the amount of data you have.  Sometimes you will pingpong back between high bias and high variance as you move through this recipe and develop a machine learning algorithm. Use these observations to shape what you do next in the process.  ### Neural Networks and Regularization A large neural network will usually do as well or better than a smaller one so long as regularization is chosen appropriately. Of course, larger neural networks are more computationally expensive. ### Neural Network Regularization #important  #function and TensorFlow implementation: !Screenshot 2023-10-17 at 6.46.28 PM.png*Note usually don't regularize B, it doesn't really affect anything*\n",
      "üí∂ section has 2323\n",
      "ü™ô SECTION to tokenize:  \n",
      "üí∂ section has 2\n",
      "ü™ô SECTION to tokenize:  ## Priority Completed: Complete Python module Completed: Complete Bias and Variance\n",
      "üí∂ section has 17\n",
      "ü™ô SECTION to tokenize:  ## Bonus/tomorrow: Completed: Note cleanup with Aliases (see Career and Study To - Do) To Do: Linear Algebra Note Migration Completed: Integrate with this daily note \t\tTo Do: Training, Test, and Dev Sets \t\tTo Do: regularization (#merge and integrate) \t\tTo Do: Cost and Loss Completed: Big overhaul of Bias and Variance with regards to todays notes.  # Machine Learning Specialization Notes: These notes will be integrated into other notes but kept here in their entirety.\n",
      "üí∂ section has 116\n",
      "ü™ô SECTION to tokenize:  ## Bias and Variance Machine Learning Specialization Advanced Machine Learning Algorithms, Week 3  Models almost never work the first time you try them out. Let's see how we can fix them.  ### Diagnosing Bias/Variance See Bias and Variance  If you have more features, you can't visualize Bias and Variance  A more systematic way to see if you have high bias or high variance is to look at the performance of the algorithm on the Training, Test, and Dev Sets|training set and dev set  A characteristic of a High Bias (under-fit) model is that $Jtrain$ (cost of the Training, Test, and Dev Sets|training set) is high. A characteristic of a High Variance (overfit) model is that $J_{cv}$ is high but $J_{train}$ is low. #### Slide !Screenshot 2023-10-17 at 1.26.00 PM.png  ### Understanding Bias and Variance Take degree of polynomial $d$ As $d$ increases, that is to say we add degrees to the polynomial $J_{train}({\\vec{w},b})$  will decrease. $J_{cv}(\\vec{w},b)$ dips towards the middle, showing the ideal degree for the polynomial.  #### Slide !Screenshot 2023-10-17 at 1.29.21 PM.png  ### Technique for Diagnosing Bias and Variance How do you tell if your algorithm has a high bias or variance problem?  One key takeaway is that high bias means it is not even doing well on the training set and high variance means it does much worse on the cross validation set and the training set. #### High Bias (under-fit) In an under-fit model $J_{train}$ will be high and $J_{train} \\approx J_{cv}$  #### High Variance (overfit) In an over-fit model $J_{cv} \\gg J_{train}$ and $J_{train}$ will be low. #### High Bias and High Variance In a model with both high bias and high variance $J_{train}$ will be high and $J_{cv} \\gg J_{train}$  This is rare, and doesn't really happen for linear models with one $d$, but it does happen. This is when it overfits for some part of the input and under-fits for another part of the input. #### Slide !Screenshot 2023-10-17 at 1.38.51 PM.png \n",
      "üí∂ section has 554\n",
      "ü™ô SECTION to tokenize:  ## Regularization and bias/variance ### Linear Regression with Regularization Regularized Linear Regression model: Lambda $\\lambda$ is the regularization parameter (Big #merge and cleanup needed here in the regularization, Regularized Linear Regression, cost, Cost and Loss, loss, Loss Function, etc. Aliases are needed in many of these)  Take this model: $$f_{\\vec{w},b}(x)=w_1+w_2x^2 + w_4x^4+b$$ With this regularization|regularized Cost and Loss $$J(\\vec{w},b)=\\frac{1}{2m} \\sum_{i=1}^{m}(f_{\\vec{w},{b}}(\\vec{x}^{(i)}-y^{(i)})^2+\\frac{\\lambda}{2m} \\sum_{j=1}^nw_j^2$$  If lambda is large, say $\\lambda = 10,000$ than $w_1 \\approx 0, W_2 \\approx 0$ and $f_{\\vec{x},b}\\vec{x} \\approx b$, thus the model creates a flat line. This is underfit, $J_{train}()  On the other hand, if we set $\\lambda=0$, then we have a forth order polynomial with no regularization. We end up with a very overfit curve.  So, how do we find a good value for $\\lambda$? ##### Slide !Screenshot 2023-10-17 at 3.56.53 PM.png ### Choosing the Regularization parameter $\\lambda$   This will be similar to choosing $d$ with cross validation Try multiple values for $\\lambda$ and then choose the option with the lowest cost. ### Slide !Screenshot 2023-10-17 at 4.15.27 PM.png ### Bias and Variance as a Function of regularization parameter $\\lambda$  Cross Validation tries out many versions of $\\lambda$ and then chooses the one with the lowest cost. Cross validation will help find us a a good value of $d$ as well as $\\lambda$  #### Slide !Screenshot 2023-10-17 at 4.14.07 PM.png  \n",
      "üí∂ section has 487\n",
      "ü™ô SECTION to tokenize:  ## Establishing A Baseline Level of Performance ### Speech Recognition example Job is to take in audio and output the text of what a person is saying  Training error $J_{train}$ is percentage of audio clips that the program does not transcribe correctly in it's entirety. Lets say: \tHuman Level Performance: 10.6 \t$J_{train}$: 10.8% \t$J_{cv}$: 14.8%  Why is human level error so high? There is lots of noise in the audio. It seems unfair to expect a learning algorithm to do much better. It is thus is more useful to measure the training error against the human error. So, looking at these results, $J_{train}$ is only 0.2$ higher than the human level performance whereas $J_{cv}$ is a full 4.2% higher. We can thus conclude that this algorithm has more of a variance problem than a bias problem. ### Establishing a baseline level of performance What is the level of error you can reasonably hope to get to? - Human level performance - Competing algorithms performance - Guess based on experience ### Bias Variance Examples Gap between baseline and training error shows high bias. A gap between training error and cross validation error shows high variance.  If your goal is perfection, the baseline would be zero. But for a lot of real world examples, like audio recognition, there is a lot of noise in the data, so you need a higher baseline.  If there is a gap between all three is high you have both high bias and high variance. #### Slide !Screenshot 2023-10-17 at 4.42.49 PM.png  \n",
      "üí∂ section has 355\n",
      "ü™ô SECTION to tokenize:  ## Learning Curves  Noted in Learning Curves ### Overview Learning curves help understand how your learning algorithm is doing as a function of the amount of experience it has. Experience being the number of training examples it has.  The bigger the training set the harder it is to fit all examples perfectly. Thus; as the training set increases so does the training error $J_{train}(\\vec{w},b)$   Plotting a learning curve by training different models based on different subsets of training data is computationally expensive, so in practice it isn't done that often. But, it's a good mental visualization. #### Slides !Screenshot 2023-10-17 at 5.40.04 PM.png    ### High Bias Example If a learning algorithm suffers from high bias, getting more training data will not (by itself) help that much. #### Slide for High Bias !Screenshot 2023-10-17 at 5.44.29 PM.png  ### High Variance Example If a learning algorithm suffers from high variance, getting more training data is likely to help. #### Slide for High Variance !Screenshot 2023-10-17 at 5.49.45 PM.png   \n",
      "üí∂ section has 257\n",
      "ü™ô SECTION to tokenize:  ## Deciding what to try next revisited ### Examples when Debugging an Algorithm: Get more training examples: fixes high variance Try smaller set of features: fixes high variance Try getting additional features: fixes high bias \tExamples; an algorithm that lacks information wont even do well on the training set Adding polynomial features ($x_1^2,X_2^2, etc$): Fixes high bias Decreasing $\\lambda$ Fixes high bias Increasing $\\lambda$ Fixes high variance \tForces the algorithm to force a smoother function.  **Note!** Don't randomly throw away training examples just to fix a high bias problem. ### Takeaway #merge with Bias and Variance ? If your algorithm has high variance, try simplifying your model or getting more training data. Simplification can mean a smaller set of features or an increased regularization If your algorithm has high bias, that is to say its not even doing well on the training set, you mainly need to make your model more powerful and flexible to fit more complex functions. To do so you can give it additional features, add polynomial features, or decrease $\\lambda$  \n",
      "üí∂ section has 239\n",
      "ü™ô SECTION to tokenize:  ## Bias/Variance and Neural Networks ### The bias variance tradeoff Simple model = high bias Complex model = high variance Before neural networks, we had to worry about balancing this complexity between bias and variance. With neural networks we now are mostly worried about high variance.  ### Neural Networks and bias variance Large Neural Network|neural networks are low bias machines. If you make your neural network large enough you can almost always fit your training set well. #### Recipe for decreasing bias with a neural network 1. Train a neural network 2.  If the training set error $J_{train}(\\vec{w},b)$ is high relative to your baseline, increase the size of the neural network by adding hidden layers. 3. Once $J_{train}(\\vec{w},b)$ is low enough, see if it does well on the cross validation set 4.  If the cross validation set $J_{cv}(\\vec{w},b)$ is too high, add more data, then test again from step 2. 5. Repeat until $J_{cv}(\\vec{w},b)$  is low enough for your liking. ##### Slide Illustration !Screenshot 2023-10-17 at 6.38.46 PM.png ### Limitations and Notes Bigger networks are restricted by your computing power, data is restricted to the amount of data you have.  Sometimes you will pingpong back between high bias and high variance as you move through this recipe and develop a machine learning algorithm. Use these observations to shape what you do next in the process.  ### Neural Networks and Regularization A large neural network will usually do as well or better than a smaller one so long as regularization is chosen appropriately. Of course, larger neural networks are more computationally expensive. ### Neural Network Regularization #important  #function and TensorFlow implementation: !Screenshot 2023-10-17 at 6.46.28 PM.png*Note usually don't regularize B, it doesn't really affect anything* # Python Notes\n",
      "üí∂ section has 422\n",
      "ü™ô SECTION to tokenize:  ## Keywords From codecademy - Continue Keyword: used inside a loop to skip the remaining loop code block and begin the next loop iteration. - Break keyword escapes the loop, regardless of the iteration number. Once break executes the program will continue to execute after the loop.\n",
      "üí∂ section has 58\n",
      "ü™ô SECTION to tokenize:  \n",
      "üí∂ section has 2\n",
      "ü™ô SECTION to tokenize:  ### Diagnosing Bias/Variance See Bias and Variance  If you have more features, you can't visualize Bias and Variance  A more systematic way to see if you have high bias or high variance is to look at the performance of the algorithm on the Training, Test, and Dev Sets|training set and dev set  A characteristic of a High Bias (under-fit) model is that $Jtrain$ (cost of the Training, Test, and Dev Sets|training set) is high. A characteristic of a High Variance (overfit) model is that $J_{cv}$ is high but $J_{train}$ is low. #### Slide !Screenshot 2023-10-17 at 1.26.00 PM.png \n",
      "üí∂ section has 162\n",
      "ü™ô SECTION to tokenize:  ### Understanding Bias and Variance Take degree of polynomial $d$ As $d$ increases, that is to say we add degrees to the polynomial $J_{train}({\\vec{w},b})$  will decrease. $J_{cv}(\\vec{w},b)$ dips towards the middle, showing the ideal degree for the polynomial.  #### Slide !Screenshot 2023-10-17 at 1.29.21 PM.png \n",
      "üí∂ section has 104\n",
      "ü™ô SECTION to tokenize:  ### Technique for Diagnosing Bias and Variance How do you tell if your algorithm has a high bias or variance problem?  One key takeaway is that high bias means it is not even doing well on the training set and high variance means it does much worse on the cross validation set and the training set. #### High Bias (under-fit) In an under-fit model $J_{train}$ will be high and $J_{train} \\approx J_{cv}$  #### High Variance (overfit) In an over-fit model $J_{cv} \\gg J_{train}$ and $J_{train}$ will be low. #### High Bias and High Variance In a model with both high bias and high variance $J_{train}$ will be high and $J_{cv} \\gg J_{train}$  This is rare, and doesn't really happen for linear models with one $d$, but it does happen. This is when it overfits for some part of the input and under-fits for another part of the input. #### Slide !Screenshot 2023-10-17 at 1.38.51 PM.png  ## Regularization and bias/variance\n",
      "üí∂ section has 258\n",
      "ü™ô SECTION to tokenize:  ### Linear Regression with Regularization Regularized Linear Regression model: Lambda $\\lambda$ is the regularization parameter (Big #merge and cleanup needed here in the regularization, Regularized Linear Regression, cost, Cost and Loss, loss, Loss Function, etc. Aliases are needed in many of these)  Take this model: $$f_{\\vec{w},b}(x)=w_1+w_2x^2 + w_4x^4+b$$ With this regularization|regularized Cost and Loss $$J(\\vec{w},b)=\\frac{1}{2m} \\sum_{i=1}^{m}(f_{\\vec{w},{b}}(\\vec{x}^{(i)}-y^{(i)})^2+\\frac{\\lambda}{2m} \\sum_{j=1}^nw_j^2$$  If lambda is large, say $\\lambda = 10,000$ than $w_1 \\approx 0, W_2 \\approx 0$ and $f_{\\vec{x},b}\\vec{x} \\approx b$, thus the model creates a flat line. This is underfit, $J_{train}()  On the other hand, if we set $\\lambda=0$, then we have a forth order polynomial with no regularization. We end up with a very overfit curve.  So, how do we find a good value for $\\lambda$? ##### Slide !Screenshot 2023-10-17 at 3.56.53 PM.png\n",
      "üí∂ section has 338\n",
      "ü™ô SECTION to tokenize:  ### Choosing the Regularization parameter $\\lambda$   This will be similar to choosing $d$ with cross validation Try multiple values for $\\lambda$ and then choose the option with the lowest cost.\n",
      "üí∂ section has 43\n",
      "ü™ô SECTION to tokenize:  ### Slide !Screenshot 2023-10-17 at 4.15.27 PM.png\n",
      "üí∂ section has 21\n",
      "ü™ô SECTION to tokenize:  ### Bias and Variance as a Function of regularization parameter $\\lambda$  Cross Validation tries out many versions of $\\lambda$ and then chooses the one with the lowest cost. Cross validation will help find us a a good value of $d$ as well as $\\lambda$  #### Slide !Screenshot 2023-10-17 at 4.14.07 PM.png   ## Establishing A Baseline Level of Performance\n",
      "üí∂ section has 93\n",
      "ü™ô SECTION to tokenize:  ### Speech Recognition example Job is to take in audio and output the text of what a person is saying  Training error $J_{train}$ is percentage of audio clips that the program does not transcribe correctly in it's entirety. Lets say: \tHuman Level Performance: 10.6 \t$J_{train}$: 10.8% \t$J_{cv}$: 14.8%  Why is human level error so high? There is lots of noise in the audio. It seems unfair to expect a learning algorithm to do much better. It is thus is more useful to measure the training error against the human error. So, looking at these results, $J_{train}$ is only 0.2$ higher than the human level performance whereas $J_{cv}$ is a full 4.2% higher. We can thus conclude that this algorithm has more of a variance problem than a bias problem.\n",
      "üí∂ section has 194\n",
      "ü™ô SECTION to tokenize:  ### Establishing a baseline level of performance What is the level of error you can reasonably hope to get to? - Human level performance - Competing algorithms performance - Guess based on experience\n",
      "üí∂ section has 39\n",
      "ü™ô SECTION to tokenize:  ### Bias Variance Examples Gap between baseline and training error shows high bias. A gap between training error and cross validation error shows high variance.  If your goal is perfection, the baseline would be zero. But for a lot of real world examples, like audio recognition, there is a lot of noise in the data, so you need a higher baseline.  If there is a gap between all three is high you have both high bias and high variance. #### Slide !Screenshot 2023-10-17 at 4.42.49 PM.png   ## Learning Curves  Noted in Learning Curves\n",
      "üí∂ section has 127\n",
      "ü™ô SECTION to tokenize:  ### Overview Learning curves help understand how your learning algorithm is doing as a function of the amount of experience it has. Experience being the number of training examples it has.  The bigger the training set the harder it is to fit all examples perfectly. Thus; as the training set increases so does the training error $J_{train}(\\vec{w},b)$   Plotting a learning curve by training different models based on different subsets of training data is computationally expensive, so in practice it isn't done that often. But, it's a good mental visualization. #### Slides !Screenshot 2023-10-17 at 5.40.04 PM.png   \n",
      "üí∂ section has 143\n",
      "ü™ô SECTION to tokenize:  ### High Bias Example If a learning algorithm suffers from high bias, getting more training data will not (by itself) help that much. #### Slide for High Bias !Screenshot 2023-10-17 at 5.44.29 PM.png \n",
      "üí∂ section has 55\n",
      "ü™ô SECTION to tokenize:  ### High Variance Example If a learning algorithm suffers from high variance, getting more training data is likely to help. #### Slide for High Variance !Screenshot 2023-10-17 at 5.49.45 PM.png    ## Deciding what to try next revisited\n",
      "üí∂ section has 61\n",
      "ü™ô SECTION to tokenize:  ### Examples when Debugging an Algorithm: Get more training examples: fixes high variance Try smaller set of features: fixes high variance Try getting additional features: fixes high bias \tExamples; an algorithm that lacks information wont even do well on the training set Adding polynomial features ($x_1^2,X_2^2, etc$): Fixes high bias Decreasing $\\lambda$ Fixes high bias Increasing $\\lambda$ Fixes high variance \tForces the algorithm to force a smoother function.  **Note!** Don't randomly throw away training examples just to fix a high bias problem.\n",
      "üí∂ section has 124\n",
      "ü™ô SECTION to tokenize:  ### Takeaway #merge with Bias and Variance ? If your algorithm has high variance, try simplifying your model or getting more training data. Simplification can mean a smaller set of features or an increased regularization If your algorithm has high bias, that is to say its not even doing well on the training set, you mainly need to make your model more powerful and flexible to fit more complex functions. To do so you can give it additional features, add polynomial features, or decrease $\\lambda$   ## Bias/Variance and Neural Networks\n",
      "üí∂ section has 117\n",
      "ü™ô SECTION to tokenize:  ### The bias variance tradeoff Simple model = high bias Complex model = high variance Before neural networks, we had to worry about balancing this complexity between bias and variance. With neural networks we now are mostly worried about high variance. \n",
      "üí∂ section has 48\n",
      "ü™ô SECTION to tokenize:  ### Neural Networks and bias variance Large Neural Network|neural networks are low bias machines. If you make your neural network large enough you can almost always fit your training set well. #### Recipe for decreasing bias with a neural network 1. Train a neural network 2.  If the training set error $J_{train}(\\vec{w},b)$ is high relative to your baseline, increase the size of the neural network by adding hidden layers. 3. Once $J_{train}(\\vec{w},b)$ is low enough, see if it does well on the cross validation set 4.  If the cross validation set $J_{cv}(\\vec{w},b)$ is too high, add more data, then test again from step 2. 5. Repeat until $J_{cv}(\\vec{w},b)$  is low enough for your liking. ##### Slide Illustration !Screenshot 2023-10-17 at 6.38.46 PM.png\n",
      "üí∂ section has 206\n",
      "ü™ô SECTION to tokenize:  ### Limitations and Notes Bigger networks are restricted by your computing power, data is restricted to the amount of data you have.  Sometimes you will pingpong back between high bias and high variance as you move through this recipe and develop a machine learning algorithm. Use these observations to shape what you do next in the process. \n",
      "üí∂ section has 69\n",
      "ü™ô SECTION to tokenize:  ### Neural Networks and Regularization A large neural network will usually do as well or better than a smaller one so long as regularization is chosen appropriately. Of course, larger neural networks are more computationally expensive.\n",
      "üí∂ section has 44\n",
      "ü™ô SECTION to tokenize:  \n",
      "üí∂ section has 2\n",
      "ü™ô SECTION to tokenize:  #### Slide !Screenshot 2023-10-17 at 1.26.00 PM.png  ### Understanding Bias and Variance Take degree of polynomial $d$ As $d$ increases, that is to say we add degrees to the polynomial $J_{train}({\\vec{w},b})$  will decrease. $J_{cv}(\\vec{w},b)$ dips towards the middle, showing the ideal degree for the polynomial. \n",
      "üí∂ section has 103\n",
      "ü™ô SECTION to tokenize:  #### Slide !Screenshot 2023-10-17 at 1.29.21 PM.png  ### Technique for Diagnosing Bias and Variance How do you tell if your algorithm has a high bias or variance problem?  One key takeaway is that high bias means it is not even doing well on the training set and high variance means it does much worse on the cross validation set and the training set.\n",
      "üí∂ section has 84\n",
      "ü™ô SECTION to tokenize:  #### High Bias (under-fit) In an under-fit model $J_{train}$ will be high and $J_{train} \\approx J_{cv}$ \n",
      "üí∂ section has 41\n",
      "ü™ô SECTION to tokenize:  #### High Variance (overfit) In an over-fit model $J_{cv} \\gg J_{train}$ and $J_{train}$ will be low.\n",
      "üí∂ section has 39\n",
      "ü™ô SECTION to tokenize:  #### High Bias and High Variance In a model with both high bias and high variance $J_{train}$ will be high and $J_{cv} \\gg J_{train}$  This is rare, and doesn't really happen for linear models with one $d$, but it does happen. This is when it overfits for some part of the input and under-fits for another part of the input.\n",
      "üí∂ section has 88\n",
      "ü™ô SECTION to tokenize:  #### Slide !Screenshot 2023-10-17 at 1.38.51 PM.png  ## Regularization and bias/variance ### Linear Regression with Regularization Regularized Linear Regression model: Lambda $\\lambda$ is the regularization parameter (Big #merge and cleanup needed here in the regularization, Regularized Linear Regression, cost, Cost and Loss, loss, Loss Function, etc. Aliases are needed in many of these)  Take this model: $$f_{\\vec{w},b}(x)=w_1+w_2x^2 + w_4x^4+b$$ With this regularization|regularized Cost and Loss $$J(\\vec{w},b)=\\frac{1}{2m} \\sum_{i=1}^{m}(f_{\\vec{w},{b}}(\\vec{x}^{(i)}-y^{(i)})^2+\\frac{\\lambda}{2m} \\sum_{j=1}^nw_j^2$$  If lambda is large, say $\\lambda = 10,000$ than $w_1 \\approx 0, W_2 \\approx 0$ and $f_{\\vec{x},b}\\vec{x} \\approx b$, thus the model creates a flat line. This is underfit, $J_{train}()  On the other hand, if we set $\\lambda=0$, then we have a forth order polynomial with no regularization. We end up with a very overfit curve.  So, how do we find a good value for $\\lambda$? ##### Slide !Screenshot 2023-10-17 at 3.56.53 PM.png ### Choosing the Regularization parameter $\\lambda$   This will be similar to choosing $d$ with cross validation Try multiple values for $\\lambda$ and then choose the option with the lowest cost. ### Slide !Screenshot 2023-10-17 at 4.15.27 PM.png ### Bias and Variance as a Function of regularization parameter $\\lambda$  Cross Validation tries out many versions of $\\lambda$ and then chooses the one with the lowest cost. Cross validation will help find us a a good value of $d$ as well as $\\lambda$ \n",
      "üí∂ section has 485\n",
      "ü™ô SECTION to tokenize:  #### Slide !Screenshot 2023-10-17 at 4.14.07 PM.png   ## Establishing A Baseline Level of Performance ### Speech Recognition example Job is to take in audio and output the text of what a person is saying  Training error $J_{train}$ is percentage of audio clips that the program does not transcribe correctly in it's entirety. Lets say: \tHuman Level Performance: 10.6 \t$J_{train}$: 10.8% \t$J_{cv}$: 14.8%  Why is human level error so high? There is lots of noise in the audio. It seems unfair to expect a learning algorithm to do much better. It is thus is more useful to measure the training error against the human error. So, looking at these results, $J_{train}$ is only 0.2$ higher than the human level performance whereas $J_{cv}$ is a full 4.2% higher. We can thus conclude that this algorithm has more of a variance problem than a bias problem. ### Establishing a baseline level of performance What is the level of error you can reasonably hope to get to? - Human level performance - Competing algorithms performance - Guess based on experience ### Bias Variance Examples Gap between baseline and training error shows high bias. A gap between training error and cross validation error shows high variance.  If your goal is perfection, the baseline would be zero. But for a lot of real world examples, like audio recognition, there is a lot of noise in the data, so you need a higher baseline.  If there is a gap between all three is high you have both high bias and high variance.\n",
      "üí∂ section has 354\n",
      "ü™ô SECTION to tokenize:  #### Slide !Screenshot 2023-10-17 at 4.42.49 PM.png   ## Learning Curves  Noted in Learning Curves ### Overview Learning curves help understand how your learning algorithm is doing as a function of the amount of experience it has. Experience being the number of training examples it has.  The bigger the training set the harder it is to fit all examples perfectly. Thus; as the training set increases so does the training error $J_{train}(\\vec{w},b)$   Plotting a learning curve by training different models based on different subsets of training data is computationally expensive, so in practice it isn't done that often. But, it's a good mental visualization.\n",
      "üí∂ section has 151\n",
      "ü™ô SECTION to tokenize:  #### Slides !Screenshot 2023-10-17 at 5.40.04 PM.png    ### High Bias Example If a learning algorithm suffers from high bias, getting more training data will not (by itself) help that much.\n",
      "üí∂ section has 53\n",
      "ü™ô SECTION to tokenize:  #### Slide for High Bias !Screenshot 2023-10-17 at 5.44.29 PM.png  ### High Variance Example If a learning algorithm suffers from high variance, getting more training data is likely to help.\n",
      "üí∂ section has 49\n",
      "ü™ô SECTION to tokenize:  #### Slide for High Variance !Screenshot 2023-10-17 at 5.49.45 PM.png    ## Deciding what to try next revisited ### Examples when Debugging an Algorithm: Get more training examples: fixes high variance Try smaller set of features: fixes high variance Try getting additional features: fixes high bias \tExamples; an algorithm that lacks information wont even do well on the training set Adding polynomial features ($x_1^2,X_2^2, etc$): Fixes high bias Decreasing $\\lambda$ Fixes high bias Increasing $\\lambda$ Fixes high variance \tForces the algorithm to force a smoother function.  **Note!** Don't randomly throw away training examples just to fix a high bias problem. ### Takeaway #merge with Bias and Variance ? If your algorithm has high variance, try simplifying your model or getting more training data. Simplification can mean a smaller set of features or an increased regularization If your algorithm has high bias, that is to say its not even doing well on the training set, you mainly need to make your model more powerful and flexible to fit more complex functions. To do so you can give it additional features, add polynomial features, or decrease $\\lambda$   ## Bias/Variance and Neural Networks ### The bias variance tradeoff Simple model = high bias Complex model = high variance Before neural networks, we had to worry about balancing this complexity between bias and variance. With neural networks we now are mostly worried about high variance.  ### Neural Networks and bias variance Large Neural Network|neural networks are low bias machines. If you make your neural network large enough you can almost always fit your training set well.\n",
      "üí∂ section has 356\n",
      "üêò Largest Token size is 2323, which is Too Big (max tokens are 1024)\n",
      "üóø header: üü©##### üü©\n",
      "üè≠ Building Subsections\n",
      "üëë Header positions:  [0, 13, 186, 2, 5, 15, 56, 86, 116, 142, 159, 187, 191, 20, 32, 41, 57, 73, 77, 79, 87, 100, 105, 119, 130, 135, 143, 154, 160, 164, 175, 180, 183, 29, 38, 45, 47, 49, 53, 82, 112, 125, 132, 137, 167, 71, 173]\n",
      "üé¨ START 0 üîö END 13\n",
      "ü™Ö SUBSECTION: \n",
      " # Afternoon To-Do #meta #todoList  ## Priority Completed: Complete Python module Completed: Complete Bias and Variance ## Bonus/tomorrow: Completed: Note cleanup with Aliases (see Career and Study To - Do) To Do: Linear Algebra Note Migration Completed: Integrate with this daily note \t\tTo Do: Training, Test, and Dev Sets \t\tTo Do: regularization (#merge and integrate) \t\tTo Do: Cost and Loss Completed: Big overhaul of Bias and Variance with regards to todays notes. \n",
      "üé¨ START 13 üîö END 186\n",
      "ü™Ö SUBSECTION: \n",
      " # Machine Learning Specialization Notes: These notes will be integrated into other notes but kept here in their entirety. ## Bias and Variance Machine Learning Specialization Advanced Machine Learning Algorithms, Week 3  Models almost never work the first time you try them out. Let's see how we can fix them.  ### Diagnosing Bias/Variance See Bias and Variance  If you have more features, you can't visualize Bias and Variance  A more systematic way to see if you have high bias or high variance is to look at the performance of the algorithm on the Training, Test, and Dev Sets|training set and dev set  A characteristic of a High Bias (under-fit) model is that $Jtrain$ (cost of the Training, Test, and Dev Sets|training set) is high. A characteristic of a High Variance (overfit) model is that $J_{cv}$ is high but $J_{train}$ is low. #### Slide !Screenshot 2023-10-17 at 1.26.00 PM.png  ### Understanding Bias and Variance Take degree of polynomial $d$ As $d$ increases, that is to say we add degrees to the polynomial $J_{train}({\\vec{w},b})$  will decrease. $J_{cv}(\\vec{w},b)$ dips towards the middle, showing the ideal degree for the polynomial.  #### Slide !Screenshot 2023-10-17 at 1.29.21 PM.png  ### Technique for Diagnosing Bias and Variance How do you tell if your algorithm has a high bias or variance problem?  One key takeaway is that high bias means it is not even doing well on the training set and high variance means it does much worse on the cross validation set and the training set. #### High Bias (under-fit) In an under-fit model $J_{train}$ will be high and $J_{train} \\approx J_{cv}$  #### High Variance (overfit) In an over-fit model $J_{cv} \\gg J_{train}$ and $J_{train}$ will be low. #### High Bias and High Variance In a model with both high bias and high variance $J_{train}$ will be high and $J_{cv} \\gg J_{train}$  This is rare, and doesn't really happen for linear models with one $d$, but it does happen. This is when it overfits for some part of the input and under-fits for another part of the input. #### Slide !Screenshot 2023-10-17 at 1.38.51 PM.png  ## Regularization and bias/variance ### Linear Regression with Regularization Regularized Linear Regression model: Lambda $\\lambda$ is the regularization parameter (Big #merge and cleanup needed here in the regularization, Regularized Linear Regression, cost, Cost and Loss, loss, Loss Function, etc. Aliases are needed in many of these)  Take this model: $$f_{\\vec{w},b}(x)=w_1+w_2x^2 + w_4x^4+b$$ With this regularization|regularized Cost and Loss $$J(\\vec{w},b)=\\frac{1}{2m} \\sum_{i=1}^{m}(f_{\\vec{w},{b}}(\\vec{x}^{(i)}-y^{(i)})^2+\\frac{\\lambda}{2m} \\sum_{j=1}^nw_j^2$$  If lambda is large, say $\\lambda = 10,000$ than $w_1 \\approx 0, W_2 \\approx 0$ and $f_{\\vec{x},b}\\vec{x} \\approx b$, thus the model creates a flat line. This is underfit, $J_{train}()  On the other hand, if we set $\\lambda=0$, then we have a forth order polynomial with no regularization. We end up with a very overfit curve.  So, how do we find a good value for $\\lambda$? ##### Slide !Screenshot 2023-10-17 at 3.56.53 PM.png ### Choosing the Regularization parameter $\\lambda$   This will be similar to choosing $d$ with cross validation Try multiple values for $\\lambda$ and then choose the option with the lowest cost. ### Slide !Screenshot 2023-10-17 at 4.15.27 PM.png ### Bias and Variance as a Function of regularization parameter $\\lambda$  Cross Validation tries out many versions of $\\lambda$ and then chooses the one with the lowest cost. Cross validation will help find us a a good value of $d$ as well as $\\lambda$  #### Slide !Screenshot 2023-10-17 at 4.14.07 PM.png   ## Establishing A Baseline Level of Performance ### Speech Recognition example Job is to take in audio and output the text of what a person is saying  Training error $J_{train}$ is percentage of audio clips that the program does not transcribe correctly in it's entirety. Lets say: \tHuman Level Performance: 10.6 \t$J_{train}$: 10.8% \t$J_{cv}$: 14.8%  Why is human level error so high? There is lots of noise in the audio. It seems unfair to expect a learning algorithm to do much better. It is thus is more useful to measure the training error against the human error. So, looking at these results, $J_{train}$ is only 0.2$ higher than the human level performance whereas $J_{cv}$ is a full 4.2% higher. We can thus conclude that this algorithm has more of a variance problem than a bias problem. ### Establishing a baseline level of performance What is the level of error you can reasonably hope to get to? - Human level performance - Competing algorithms performance - Guess based on experience ### Bias Variance Examples Gap between baseline and training error shows high bias. A gap between training error and cross validation error shows high variance.  If your goal is perfection, the baseline would be zero. But for a lot of real world examples, like audio recognition, there is a lot of noise in the data, so you need a higher baseline.  If there is a gap between all three is high you have both high bias and high variance. #### Slide !Screenshot 2023-10-17 at 4.42.49 PM.png   ## Learning Curves  Noted in Learning Curves ### Overview Learning curves help understand how your learning algorithm is doing as a function of the amount of experience it has. Experience being the number of training examples it has.  The bigger the training set the harder it is to fit all examples perfectly. Thus; as the training set increases so does the training error $J_{train}(\\vec{w},b)$   Plotting a learning curve by training different models based on different subsets of training data is computationally expensive, so in practice it isn't done that often. But, it's a good mental visualization. #### Slides !Screenshot 2023-10-17 at 5.40.04 PM.png    ### High Bias Example If a learning algorithm suffers from high bias, getting more training data will not (by itself) help that much. #### Slide for High Bias !Screenshot 2023-10-17 at 5.44.29 PM.png  ### High Variance Example If a learning algorithm suffers from high variance, getting more training data is likely to help. #### Slide for High Variance !Screenshot 2023-10-17 at 5.49.45 PM.png    ## Deciding what to try next revisited ### Examples when Debugging an Algorithm: Get more training examples: fixes high variance Try smaller set of features: fixes high variance Try getting additional features: fixes high bias \tExamples; an algorithm that lacks information wont even do well on the training set Adding polynomial features ($x_1^2,X_2^2, etc$): Fixes high bias Decreasing $\\lambda$ Fixes high bias Increasing $\\lambda$ Fixes high variance \tForces the algorithm to force a smoother function.  **Note!** Don't randomly throw away training examples just to fix a high bias problem. ### Takeaway #merge with Bias and Variance ? If your algorithm has high variance, try simplifying your model or getting more training data. Simplification can mean a smaller set of features or an increased regularization If your algorithm has high bias, that is to say its not even doing well on the training set, you mainly need to make your model more powerful and flexible to fit more complex functions. To do so you can give it additional features, add polynomial features, or decrease $\\lambda$   ## Bias/Variance and Neural Networks ### The bias variance tradeoff Simple model = high bias Complex model = high variance Before neural networks, we had to worry about balancing this complexity between bias and variance. With neural networks we now are mostly worried about high variance.  ### Neural Networks and bias variance Large Neural Network|neural networks are low bias machines. If you make your neural network large enough you can almost always fit your training set well. #### Recipe for decreasing bias with a neural network 1. Train a neural network 2.  If the training set error $J_{train}(\\vec{w},b)$ is high relative to your baseline, increase the size of the neural network by adding hidden layers. 3. Once $J_{train}(\\vec{w},b)$ is low enough, see if it does well on the cross validation set 4.  If the cross validation set $J_{cv}(\\vec{w},b)$ is too high, add more data, then test again from step 2. 5. Repeat until $J_{cv}(\\vec{w},b)$  is low enough for your liking. ##### Slide Illustration !Screenshot 2023-10-17 at 6.38.46 PM.png ### Limitations and Notes Bigger networks are restricted by your computing power, data is restricted to the amount of data you have.  Sometimes you will pingpong back between high bias and high variance as you move through this recipe and develop a machine learning algorithm. Use these observations to shape what you do next in the process.  ### Neural Networks and Regularization A large neural network will usually do as well or better than a smaller one so long as regularization is chosen appropriately. Of course, larger neural networks are more computationally expensive. ### Neural Network Regularization #important  #function and TensorFlow implementation: !Screenshot 2023-10-17 at 6.46.28 PM.png*Note usually don't regularize B, it doesn't really affect anything*\n",
      "üé¨ START 186 üîö END 2\n",
      "ü™Ö SUBSECTION: \n",
      " \n",
      "üé¨ START 2 üîö END 5\n",
      "ü™Ö SUBSECTION: \n",
      " ## Priority Completed: Complete Python module Completed: Complete Bias and Variance\n",
      "üé¨ START 5 üîö END 15\n",
      "ü™Ö SUBSECTION: \n",
      " ## Bonus/tomorrow: Completed: Note cleanup with Aliases (see Career and Study To - Do) To Do: Linear Algebra Note Migration Completed: Integrate with this daily note \t\tTo Do: Training, Test, and Dev Sets \t\tTo Do: regularization (#merge and integrate) \t\tTo Do: Cost and Loss Completed: Big overhaul of Bias and Variance with regards to todays notes.  # Machine Learning Specialization Notes: These notes will be integrated into other notes but kept here in their entirety.\n",
      "üé¨ START 15 üîö END 56\n",
      "ü™Ö SUBSECTION: \n",
      " ## Bias and Variance Machine Learning Specialization Advanced Machine Learning Algorithms, Week 3  Models almost never work the first time you try them out. Let's see how we can fix them.  ### Diagnosing Bias/Variance See Bias and Variance  If you have more features, you can't visualize Bias and Variance  A more systematic way to see if you have high bias or high variance is to look at the performance of the algorithm on the Training, Test, and Dev Sets|training set and dev set  A characteristic of a High Bias (under-fit) model is that $Jtrain$ (cost of the Training, Test, and Dev Sets|training set) is high. A characteristic of a High Variance (overfit) model is that $J_{cv}$ is high but $J_{train}$ is low. #### Slide !Screenshot 2023-10-17 at 1.26.00 PM.png  ### Understanding Bias and Variance Take degree of polynomial $d$ As $d$ increases, that is to say we add degrees to the polynomial $J_{train}({\\vec{w},b})$  will decrease. $J_{cv}(\\vec{w},b)$ dips towards the middle, showing the ideal degree for the polynomial.  #### Slide !Screenshot 2023-10-17 at 1.29.21 PM.png  ### Technique for Diagnosing Bias and Variance How do you tell if your algorithm has a high bias or variance problem?  One key takeaway is that high bias means it is not even doing well on the training set and high variance means it does much worse on the cross validation set and the training set. #### High Bias (under-fit) In an under-fit model $J_{train}$ will be high and $J_{train} \\approx J_{cv}$  #### High Variance (overfit) In an over-fit model $J_{cv} \\gg J_{train}$ and $J_{train}$ will be low. #### High Bias and High Variance In a model with both high bias and high variance $J_{train}$ will be high and $J_{cv} \\gg J_{train}$  This is rare, and doesn't really happen for linear models with one $d$, but it does happen. This is when it overfits for some part of the input and under-fits for another part of the input. #### Slide !Screenshot 2023-10-17 at 1.38.51 PM.png \n",
      "üé¨ START 56 üîö END 86\n",
      "ü™Ö SUBSECTION: \n",
      " ## Regularization and bias/variance ### Linear Regression with Regularization Regularized Linear Regression model: Lambda $\\lambda$ is the regularization parameter (Big #merge and cleanup needed here in the regularization, Regularized Linear Regression, cost, Cost and Loss, loss, Loss Function, etc. Aliases are needed in many of these)  Take this model: $$f_{\\vec{w},b}(x)=w_1+w_2x^2 + w_4x^4+b$$ With this regularization|regularized Cost and Loss $$J(\\vec{w},b)=\\frac{1}{2m} \\sum_{i=1}^{m}(f_{\\vec{w},{b}}(\\vec{x}^{(i)}-y^{(i)})^2+\\frac{\\lambda}{2m} \\sum_{j=1}^nw_j^2$$  If lambda is large, say $\\lambda = 10,000$ than $w_1 \\approx 0, W_2 \\approx 0$ and $f_{\\vec{x},b}\\vec{x} \\approx b$, thus the model creates a flat line. This is underfit, $J_{train}()  On the other hand, if we set $\\lambda=0$, then we have a forth order polynomial with no regularization. We end up with a very overfit curve.  So, how do we find a good value for $\\lambda$? ##### Slide !Screenshot 2023-10-17 at 3.56.53 PM.png ### Choosing the Regularization parameter $\\lambda$   This will be similar to choosing $d$ with cross validation Try multiple values for $\\lambda$ and then choose the option with the lowest cost. ### Slide !Screenshot 2023-10-17 at 4.15.27 PM.png ### Bias and Variance as a Function of regularization parameter $\\lambda$  Cross Validation tries out many versions of $\\lambda$ and then chooses the one with the lowest cost. Cross validation will help find us a a good value of $d$ as well as $\\lambda$  #### Slide !Screenshot 2023-10-17 at 4.14.07 PM.png  \n",
      "üé¨ START 86 üîö END 116\n",
      "ü™Ö SUBSECTION: \n",
      " ## Establishing A Baseline Level of Performance ### Speech Recognition example Job is to take in audio and output the text of what a person is saying  Training error $J_{train}$ is percentage of audio clips that the program does not transcribe correctly in it's entirety. Lets say: \tHuman Level Performance: 10.6 \t$J_{train}$: 10.8% \t$J_{cv}$: 14.8%  Why is human level error so high? There is lots of noise in the audio. It seems unfair to expect a learning algorithm to do much better. It is thus is more useful to measure the training error against the human error. So, looking at these results, $J_{train}$ is only 0.2$ higher than the human level performance whereas $J_{cv}$ is a full 4.2% higher. We can thus conclude that this algorithm has more of a variance problem than a bias problem. ### Establishing a baseline level of performance What is the level of error you can reasonably hope to get to? - Human level performance - Competing algorithms performance - Guess based on experience ### Bias Variance Examples Gap between baseline and training error shows high bias. A gap between training error and cross validation error shows high variance.  If your goal is perfection, the baseline would be zero. But for a lot of real world examples, like audio recognition, there is a lot of noise in the data, so you need a higher baseline.  If there is a gap between all three is high you have both high bias and high variance. #### Slide !Screenshot 2023-10-17 at 4.42.49 PM.png  \n",
      "üé¨ START 116 üîö END 142\n",
      "ü™Ö SUBSECTION: \n",
      " ## Learning Curves  Noted in Learning Curves ### Overview Learning curves help understand how your learning algorithm is doing as a function of the amount of experience it has. Experience being the number of training examples it has.  The bigger the training set the harder it is to fit all examples perfectly. Thus; as the training set increases so does the training error $J_{train}(\\vec{w},b)$   Plotting a learning curve by training different models based on different subsets of training data is computationally expensive, so in practice it isn't done that often. But, it's a good mental visualization. #### Slides !Screenshot 2023-10-17 at 5.40.04 PM.png    ### High Bias Example If a learning algorithm suffers from high bias, getting more training data will not (by itself) help that much. #### Slide for High Bias !Screenshot 2023-10-17 at 5.44.29 PM.png  ### High Variance Example If a learning algorithm suffers from high variance, getting more training data is likely to help. #### Slide for High Variance !Screenshot 2023-10-17 at 5.49.45 PM.png   \n",
      "üé¨ START 142 üîö END 159\n",
      "ü™Ö SUBSECTION: \n",
      " ## Deciding what to try next revisited ### Examples when Debugging an Algorithm: Get more training examples: fixes high variance Try smaller set of features: fixes high variance Try getting additional features: fixes high bias \tExamples; an algorithm that lacks information wont even do well on the training set Adding polynomial features ($x_1^2,X_2^2, etc$): Fixes high bias Decreasing $\\lambda$ Fixes high bias Increasing $\\lambda$ Fixes high variance \tForces the algorithm to force a smoother function.  **Note!** Don't randomly throw away training examples just to fix a high bias problem. ### Takeaway #merge with Bias and Variance ? If your algorithm has high variance, try simplifying your model or getting more training data. Simplification can mean a smaller set of features or an increased regularization If your algorithm has high bias, that is to say its not even doing well on the training set, you mainly need to make your model more powerful and flexible to fit more complex functions. To do so you can give it additional features, add polynomial features, or decrease $\\lambda$  \n",
      "üé¨ START 159 üîö END 187\n",
      "ü™Ö SUBSECTION: \n",
      " ## Bias/Variance and Neural Networks ### The bias variance tradeoff Simple model = high bias Complex model = high variance Before neural networks, we had to worry about balancing this complexity between bias and variance. With neural networks we now are mostly worried about high variance.  ### Neural Networks and bias variance Large Neural Network|neural networks are low bias machines. If you make your neural network large enough you can almost always fit your training set well. #### Recipe for decreasing bias with a neural network 1. Train a neural network 2.  If the training set error $J_{train}(\\vec{w},b)$ is high relative to your baseline, increase the size of the neural network by adding hidden layers. 3. Once $J_{train}(\\vec{w},b)$ is low enough, see if it does well on the cross validation set 4.  If the cross validation set $J_{cv}(\\vec{w},b)$ is too high, add more data, then test again from step 2. 5. Repeat until $J_{cv}(\\vec{w},b)$  is low enough for your liking. ##### Slide Illustration !Screenshot 2023-10-17 at 6.38.46 PM.png ### Limitations and Notes Bigger networks are restricted by your computing power, data is restricted to the amount of data you have.  Sometimes you will pingpong back between high bias and high variance as you move through this recipe and develop a machine learning algorithm. Use these observations to shape what you do next in the process.  ### Neural Networks and Regularization A large neural network will usually do as well or better than a smaller one so long as regularization is chosen appropriately. Of course, larger neural networks are more computationally expensive. ### Neural Network Regularization #important  #function and TensorFlow implementation: !Screenshot 2023-10-17 at 6.46.28 PM.png*Note usually don't regularize B, it doesn't really affect anything* # Python Notes\n",
      "üé¨ START 187 üîö END 191\n",
      "ü™Ö SUBSECTION: \n",
      " ## Keywords From codecademy - Continue Keyword: used inside a loop to skip the remaining loop code block and begin the next loop iteration. - Break keyword escapes the loop, regardless of the iteration number. Once break executes the program will continue to execute after the loop.\n",
      "üé¨ START 191 üîö END 20\n",
      "ü™Ö SUBSECTION: \n",
      " \n",
      "üé¨ START 20 üîö END 32\n",
      "ü™Ö SUBSECTION: \n",
      " ### Diagnosing Bias/Variance See Bias and Variance  If you have more features, you can't visualize Bias and Variance  A more systematic way to see if you have high bias or high variance is to look at the performance of the algorithm on the Training, Test, and Dev Sets|training set and dev set  A characteristic of a High Bias (under-fit) model is that $Jtrain$ (cost of the Training, Test, and Dev Sets|training set) is high. A characteristic of a High Variance (overfit) model is that $J_{cv}$ is high but $J_{train}$ is low. #### Slide !Screenshot 2023-10-17 at 1.26.00 PM.png \n",
      "üé¨ START 32 üîö END 41\n",
      "ü™Ö SUBSECTION: \n",
      " ### Understanding Bias and Variance Take degree of polynomial $d$ As $d$ increases, that is to say we add degrees to the polynomial $J_{train}({\\vec{w},b})$  will decrease. $J_{cv}(\\vec{w},b)$ dips towards the middle, showing the ideal degree for the polynomial.  #### Slide !Screenshot 2023-10-17 at 1.29.21 PM.png \n",
      "üé¨ START 41 üîö END 57\n",
      "ü™Ö SUBSECTION: \n",
      " ### Technique for Diagnosing Bias and Variance How do you tell if your algorithm has a high bias or variance problem?  One key takeaway is that high bias means it is not even doing well on the training set and high variance means it does much worse on the cross validation set and the training set. #### High Bias (under-fit) In an under-fit model $J_{train}$ will be high and $J_{train} \\approx J_{cv}$  #### High Variance (overfit) In an over-fit model $J_{cv} \\gg J_{train}$ and $J_{train}$ will be low. #### High Bias and High Variance In a model with both high bias and high variance $J_{train}$ will be high and $J_{cv} \\gg J_{train}$  This is rare, and doesn't really happen for linear models with one $d$, but it does happen. This is when it overfits for some part of the input and under-fits for another part of the input. #### Slide !Screenshot 2023-10-17 at 1.38.51 PM.png  ## Regularization and bias/variance\n",
      "üé¨ START 57 üîö END 73\n",
      "ü™Ö SUBSECTION: \n",
      " ### Linear Regression with Regularization Regularized Linear Regression model: Lambda $\\lambda$ is the regularization parameter (Big #merge and cleanup needed here in the regularization, Regularized Linear Regression, cost, Cost and Loss, loss, Loss Function, etc. Aliases are needed in many of these)  Take this model: $$f_{\\vec{w},b}(x)=w_1+w_2x^2 + w_4x^4+b$$ With this regularization|regularized Cost and Loss $$J(\\vec{w},b)=\\frac{1}{2m} \\sum_{i=1}^{m}(f_{\\vec{w},{b}}(\\vec{x}^{(i)}-y^{(i)})^2+\\frac{\\lambda}{2m} \\sum_{j=1}^nw_j^2$$  If lambda is large, say $\\lambda = 10,000$ than $w_1 \\approx 0, W_2 \\approx 0$ and $f_{\\vec{x},b}\\vec{x} \\approx b$, thus the model creates a flat line. This is underfit, $J_{train}()  On the other hand, if we set $\\lambda=0$, then we have a forth order polynomial with no regularization. We end up with a very overfit curve.  So, how do we find a good value for $\\lambda$? ##### Slide !Screenshot 2023-10-17 at 3.56.53 PM.png\n",
      "üé¨ START 73 üîö END 77\n",
      "ü™Ö SUBSECTION: \n",
      " ### Choosing the Regularization parameter $\\lambda$   This will be similar to choosing $d$ with cross validation Try multiple values for $\\lambda$ and then choose the option with the lowest cost.\n",
      "üé¨ START 77 üîö END 79\n",
      "ü™Ö SUBSECTION: \n",
      " ### Slide !Screenshot 2023-10-17 at 4.15.27 PM.png\n",
      "üé¨ START 79 üîö END 87\n",
      "ü™Ö SUBSECTION: \n",
      " ### Bias and Variance as a Function of regularization parameter $\\lambda$  Cross Validation tries out many versions of $\\lambda$ and then chooses the one with the lowest cost. Cross validation will help find us a a good value of $d$ as well as $\\lambda$  #### Slide !Screenshot 2023-10-17 at 4.14.07 PM.png   ## Establishing A Baseline Level of Performance\n",
      "üé¨ START 87 üîö END 100\n",
      "ü™Ö SUBSECTION: \n",
      " ### Speech Recognition example Job is to take in audio and output the text of what a person is saying  Training error $J_{train}$ is percentage of audio clips that the program does not transcribe correctly in it's entirety. Lets say: \tHuman Level Performance: 10.6 \t$J_{train}$: 10.8% \t$J_{cv}$: 14.8%  Why is human level error so high? There is lots of noise in the audio. It seems unfair to expect a learning algorithm to do much better. It is thus is more useful to measure the training error against the human error. So, looking at these results, $J_{train}$ is only 0.2$ higher than the human level performance whereas $J_{cv}$ is a full 4.2% higher. We can thus conclude that this algorithm has more of a variance problem than a bias problem.\n",
      "üé¨ START 100 üîö END 105\n",
      "ü™Ö SUBSECTION: \n",
      " ### Establishing a baseline level of performance What is the level of error you can reasonably hope to get to? - Human level performance - Competing algorithms performance - Guess based on experience\n",
      "üé¨ START 105 üîö END 119\n",
      "ü™Ö SUBSECTION: \n",
      " ### Bias Variance Examples Gap between baseline and training error shows high bias. A gap between training error and cross validation error shows high variance.  If your goal is perfection, the baseline would be zero. But for a lot of real world examples, like audio recognition, there is a lot of noise in the data, so you need a higher baseline.  If there is a gap between all three is high you have both high bias and high variance. #### Slide !Screenshot 2023-10-17 at 4.42.49 PM.png   ## Learning Curves  Noted in Learning Curves\n",
      "üé¨ START 119 üîö END 130\n",
      "ü™Ö SUBSECTION: \n",
      " ### Overview Learning curves help understand how your learning algorithm is doing as a function of the amount of experience it has. Experience being the number of training examples it has.  The bigger the training set the harder it is to fit all examples perfectly. Thus; as the training set increases so does the training error $J_{train}(\\vec{w},b)$   Plotting a learning curve by training different models based on different subsets of training data is computationally expensive, so in practice it isn't done that often. But, it's a good mental visualization. #### Slides !Screenshot 2023-10-17 at 5.40.04 PM.png   \n",
      "üé¨ START 130 üîö END 135\n",
      "ü™Ö SUBSECTION: \n",
      " ### High Bias Example If a learning algorithm suffers from high bias, getting more training data will not (by itself) help that much. #### Slide for High Bias !Screenshot 2023-10-17 at 5.44.29 PM.png \n",
      "üé¨ START 135 üîö END 143\n",
      "ü™Ö SUBSECTION: \n",
      " ### High Variance Example If a learning algorithm suffers from high variance, getting more training data is likely to help. #### Slide for High Variance !Screenshot 2023-10-17 at 5.49.45 PM.png    ## Deciding what to try next revisited\n",
      "üé¨ START 143 üîö END 154\n",
      "ü™Ö SUBSECTION: \n",
      " ### Examples when Debugging an Algorithm: Get more training examples: fixes high variance Try smaller set of features: fixes high variance Try getting additional features: fixes high bias \tExamples; an algorithm that lacks information wont even do well on the training set Adding polynomial features ($x_1^2,X_2^2, etc$): Fixes high bias Decreasing $\\lambda$ Fixes high bias Increasing $\\lambda$ Fixes high variance \tForces the algorithm to force a smoother function.  **Note!** Don't randomly throw away training examples just to fix a high bias problem.\n",
      "üé¨ START 154 üîö END 160\n",
      "ü™Ö SUBSECTION: \n",
      " ### Takeaway #merge with Bias and Variance ? If your algorithm has high variance, try simplifying your model or getting more training data. Simplification can mean a smaller set of features or an increased regularization If your algorithm has high bias, that is to say its not even doing well on the training set, you mainly need to make your model more powerful and flexible to fit more complex functions. To do so you can give it additional features, add polynomial features, or decrease $\\lambda$   ## Bias/Variance and Neural Networks\n",
      "üé¨ START 160 üîö END 164\n",
      "ü™Ö SUBSECTION: \n",
      " ### The bias variance tradeoff Simple model = high bias Complex model = high variance Before neural networks, we had to worry about balancing this complexity between bias and variance. With neural networks we now are mostly worried about high variance. \n",
      "üé¨ START 164 üîö END 175\n",
      "ü™Ö SUBSECTION: \n",
      " ### Neural Networks and bias variance Large Neural Network|neural networks are low bias machines. If you make your neural network large enough you can almost always fit your training set well. #### Recipe for decreasing bias with a neural network 1. Train a neural network 2.  If the training set error $J_{train}(\\vec{w},b)$ is high relative to your baseline, increase the size of the neural network by adding hidden layers. 3. Once $J_{train}(\\vec{w},b)$ is low enough, see if it does well on the cross validation set 4.  If the cross validation set $J_{cv}(\\vec{w},b)$ is too high, add more data, then test again from step 2. 5. Repeat until $J_{cv}(\\vec{w},b)$  is low enough for your liking. ##### Slide Illustration !Screenshot 2023-10-17 at 6.38.46 PM.png\n",
      "üé¨ START 175 üîö END 180\n",
      "ü™Ö SUBSECTION: \n",
      " ### Limitations and Notes Bigger networks are restricted by your computing power, data is restricted to the amount of data you have.  Sometimes you will pingpong back between high bias and high variance as you move through this recipe and develop a machine learning algorithm. Use these observations to shape what you do next in the process. \n",
      "üé¨ START 180 üîö END 183\n",
      "ü™Ö SUBSECTION: \n",
      " ### Neural Networks and Regularization A large neural network will usually do as well or better than a smaller one so long as regularization is chosen appropriately. Of course, larger neural networks are more computationally expensive.\n",
      "üé¨ START 183 üîö END 29\n",
      "ü™Ö SUBSECTION: \n",
      " \n",
      "üé¨ START 29 üîö END 38\n",
      "ü™Ö SUBSECTION: \n",
      " #### Slide !Screenshot 2023-10-17 at 1.26.00 PM.png  ### Understanding Bias and Variance Take degree of polynomial $d$ As $d$ increases, that is to say we add degrees to the polynomial $J_{train}({\\vec{w},b})$  will decrease. $J_{cv}(\\vec{w},b)$ dips towards the middle, showing the ideal degree for the polynomial. \n",
      "üé¨ START 38 üîö END 45\n",
      "ü™Ö SUBSECTION: \n",
      " #### Slide !Screenshot 2023-10-17 at 1.29.21 PM.png  ### Technique for Diagnosing Bias and Variance How do you tell if your algorithm has a high bias or variance problem?  One key takeaway is that high bias means it is not even doing well on the training set and high variance means it does much worse on the cross validation set and the training set.\n",
      "üé¨ START 45 üîö END 47\n",
      "ü™Ö SUBSECTION: \n",
      " #### High Bias (under-fit) In an under-fit model $J_{train}$ will be high and $J_{train} \\approx J_{cv}$ \n",
      "üé¨ START 47 üîö END 49\n",
      "ü™Ö SUBSECTION: \n",
      " #### High Variance (overfit) In an over-fit model $J_{cv} \\gg J_{train}$ and $J_{train}$ will be low.\n",
      "üé¨ START 49 üîö END 53\n",
      "ü™Ö SUBSECTION: \n",
      " #### High Bias and High Variance In a model with both high bias and high variance $J_{train}$ will be high and $J_{cv} \\gg J_{train}$  This is rare, and doesn't really happen for linear models with one $d$, but it does happen. This is when it overfits for some part of the input and under-fits for another part of the input.\n",
      "üé¨ START 53 üîö END 82\n",
      "ü™Ö SUBSECTION: \n",
      " #### Slide !Screenshot 2023-10-17 at 1.38.51 PM.png  ## Regularization and bias/variance ### Linear Regression with Regularization Regularized Linear Regression model: Lambda $\\lambda$ is the regularization parameter (Big #merge and cleanup needed here in the regularization, Regularized Linear Regression, cost, Cost and Loss, loss, Loss Function, etc. Aliases are needed in many of these)  Take this model: $$f_{\\vec{w},b}(x)=w_1+w_2x^2 + w_4x^4+b$$ With this regularization|regularized Cost and Loss $$J(\\vec{w},b)=\\frac{1}{2m} \\sum_{i=1}^{m}(f_{\\vec{w},{b}}(\\vec{x}^{(i)}-y^{(i)})^2+\\frac{\\lambda}{2m} \\sum_{j=1}^nw_j^2$$  If lambda is large, say $\\lambda = 10,000$ than $w_1 \\approx 0, W_2 \\approx 0$ and $f_{\\vec{x},b}\\vec{x} \\approx b$, thus the model creates a flat line. This is underfit, $J_{train}()  On the other hand, if we set $\\lambda=0$, then we have a forth order polynomial with no regularization. We end up with a very overfit curve.  So, how do we find a good value for $\\lambda$? ##### Slide !Screenshot 2023-10-17 at 3.56.53 PM.png ### Choosing the Regularization parameter $\\lambda$   This will be similar to choosing $d$ with cross validation Try multiple values for $\\lambda$ and then choose the option with the lowest cost. ### Slide !Screenshot 2023-10-17 at 4.15.27 PM.png ### Bias and Variance as a Function of regularization parameter $\\lambda$  Cross Validation tries out many versions of $\\lambda$ and then chooses the one with the lowest cost. Cross validation will help find us a a good value of $d$ as well as $\\lambda$ \n",
      "üé¨ START 82 üîö END 112\n",
      "ü™Ö SUBSECTION: \n",
      " #### Slide !Screenshot 2023-10-17 at 4.14.07 PM.png   ## Establishing A Baseline Level of Performance ### Speech Recognition example Job is to take in audio and output the text of what a person is saying  Training error $J_{train}$ is percentage of audio clips that the program does not transcribe correctly in it's entirety. Lets say: \tHuman Level Performance: 10.6 \t$J_{train}$: 10.8% \t$J_{cv}$: 14.8%  Why is human level error so high? There is lots of noise in the audio. It seems unfair to expect a learning algorithm to do much better. It is thus is more useful to measure the training error against the human error. So, looking at these results, $J_{train}$ is only 0.2$ higher than the human level performance whereas $J_{cv}$ is a full 4.2% higher. We can thus conclude that this algorithm has more of a variance problem than a bias problem. ### Establishing a baseline level of performance What is the level of error you can reasonably hope to get to? - Human level performance - Competing algorithms performance - Guess based on experience ### Bias Variance Examples Gap between baseline and training error shows high bias. A gap between training error and cross validation error shows high variance.  If your goal is perfection, the baseline would be zero. But for a lot of real world examples, like audio recognition, there is a lot of noise in the data, so you need a higher baseline.  If there is a gap between all three is high you have both high bias and high variance.\n",
      "üé¨ START 112 üîö END 125\n",
      "ü™Ö SUBSECTION: \n",
      " #### Slide !Screenshot 2023-10-17 at 4.42.49 PM.png   ## Learning Curves  Noted in Learning Curves ### Overview Learning curves help understand how your learning algorithm is doing as a function of the amount of experience it has. Experience being the number of training examples it has.  The bigger the training set the harder it is to fit all examples perfectly. Thus; as the training set increases so does the training error $J_{train}(\\vec{w},b)$   Plotting a learning curve by training different models based on different subsets of training data is computationally expensive, so in practice it isn't done that often. But, it's a good mental visualization.\n",
      "üé¨ START 125 üîö END 132\n",
      "ü™Ö SUBSECTION: \n",
      " #### Slides !Screenshot 2023-10-17 at 5.40.04 PM.png    ### High Bias Example If a learning algorithm suffers from high bias, getting more training data will not (by itself) help that much.\n",
      "üé¨ START 132 üîö END 137\n",
      "ü™Ö SUBSECTION: \n",
      " #### Slide for High Bias !Screenshot 2023-10-17 at 5.44.29 PM.png  ### High Variance Example If a learning algorithm suffers from high variance, getting more training data is likely to help.\n",
      "üé¨ START 137 üîö END 167\n",
      "ü™Ö SUBSECTION: \n",
      " #### Slide for High Variance !Screenshot 2023-10-17 at 5.49.45 PM.png    ## Deciding what to try next revisited ### Examples when Debugging an Algorithm: Get more training examples: fixes high variance Try smaller set of features: fixes high variance Try getting additional features: fixes high bias \tExamples; an algorithm that lacks information wont even do well on the training set Adding polynomial features ($x_1^2,X_2^2, etc$): Fixes high bias Decreasing $\\lambda$ Fixes high bias Increasing $\\lambda$ Fixes high variance \tForces the algorithm to force a smoother function.  **Note!** Don't randomly throw away training examples just to fix a high bias problem. ### Takeaway #merge with Bias and Variance ? If your algorithm has high variance, try simplifying your model or getting more training data. Simplification can mean a smaller set of features or an increased regularization If your algorithm has high bias, that is to say its not even doing well on the training set, you mainly need to make your model more powerful and flexible to fit more complex functions. To do so you can give it additional features, add polynomial features, or decrease $\\lambda$   ## Bias/Variance and Neural Networks ### The bias variance tradeoff Simple model = high bias Complex model = high variance Before neural networks, we had to worry about balancing this complexity between bias and variance. With neural networks we now are mostly worried about high variance.  ### Neural Networks and bias variance Large Neural Network|neural networks are low bias machines. If you make your neural network large enough you can almost always fit your training set well.\n",
      "üé¨ START 167 üîö END 71\n",
      "ü™Ö SUBSECTION: \n",
      " \n",
      "üé¨ START 71 üîö END 173\n",
      "ü™Ö SUBSECTION: \n",
      " ##### Slide !Screenshot 2023-10-17 at 3.56.53 PM.png ### Choosing the Regularization parameter $\\lambda$   This will be similar to choosing $d$ with cross validation Try multiple values for $\\lambda$ and then choose the option with the lowest cost. ### Slide !Screenshot 2023-10-17 at 4.15.27 PM.png ### Bias and Variance as a Function of regularization parameter $\\lambda$  Cross Validation tries out many versions of $\\lambda$ and then chooses the one with the lowest cost. Cross validation will help find us a a good value of $d$ as well as $\\lambda$  #### Slide !Screenshot 2023-10-17 at 4.14.07 PM.png   ## Establishing A Baseline Level of Performance ### Speech Recognition example Job is to take in audio and output the text of what a person is saying  Training error $J_{train}$ is percentage of audio clips that the program does not transcribe correctly in it's entirety. Lets say: \tHuman Level Performance: 10.6 \t$J_{train}$: 10.8% \t$J_{cv}$: 14.8%  Why is human level error so high? There is lots of noise in the audio. It seems unfair to expect a learning algorithm to do much better. It is thus is more useful to measure the training error against the human error. So, looking at these results, $J_{train}$ is only 0.2$ higher than the human level performance whereas $J_{cv}$ is a full 4.2% higher. We can thus conclude that this algorithm has more of a variance problem than a bias problem. ### Establishing a baseline level of performance What is the level of error you can reasonably hope to get to? - Human level performance - Competing algorithms performance - Guess based on experience ### Bias Variance Examples Gap between baseline and training error shows high bias. A gap between training error and cross validation error shows high variance.  If your goal is perfection, the baseline would be zero. But for a lot of real world examples, like audio recognition, there is a lot of noise in the data, so you need a higher baseline.  If there is a gap between all three is high you have both high bias and high variance. #### Slide !Screenshot 2023-10-17 at 4.42.49 PM.png   ## Learning Curves  Noted in Learning Curves ### Overview Learning curves help understand how your learning algorithm is doing as a function of the amount of experience it has. Experience being the number of training examples it has.  The bigger the training set the harder it is to fit all examples perfectly. Thus; as the training set increases so does the training error $J_{train}(\\vec{w},b)$   Plotting a learning curve by training different models based on different subsets of training data is computationally expensive, so in practice it isn't done that often. But, it's a good mental visualization. #### Slides !Screenshot 2023-10-17 at 5.40.04 PM.png    ### High Bias Example If a learning algorithm suffers from high bias, getting more training data will not (by itself) help that much. #### Slide for High Bias !Screenshot 2023-10-17 at 5.44.29 PM.png  ### High Variance Example If a learning algorithm suffers from high variance, getting more training data is likely to help. #### Slide for High Variance !Screenshot 2023-10-17 at 5.49.45 PM.png    ## Deciding what to try next revisited ### Examples when Debugging an Algorithm: Get more training examples: fixes high variance Try smaller set of features: fixes high variance Try getting additional features: fixes high bias \tExamples; an algorithm that lacks information wont even do well on the training set Adding polynomial features ($x_1^2,X_2^2, etc$): Fixes high bias Decreasing $\\lambda$ Fixes high bias Increasing $\\lambda$ Fixes high variance \tForces the algorithm to force a smoother function.  **Note!** Don't randomly throw away training examples just to fix a high bias problem. ### Takeaway #merge with Bias and Variance ? If your algorithm has high variance, try simplifying your model or getting more training data. Simplification can mean a smaller set of features or an increased regularization If your algorithm has high bias, that is to say its not even doing well on the training set, you mainly need to make your model more powerful and flexible to fit more complex functions. To do so you can give it additional features, add polynomial features, or decrease $\\lambda$   ## Bias/Variance and Neural Networks ### The bias variance tradeoff Simple model = high bias Complex model = high variance Before neural networks, we had to worry about balancing this complexity between bias and variance. With neural networks we now are mostly worried about high variance.  ### Neural Networks and bias variance Large Neural Network|neural networks are low bias machines. If you make your neural network large enough you can almost always fit your training set well. #### Recipe for decreasing bias with a neural network 1. Train a neural network 2.  If the training set error $J_{train}(\\vec{w},b)$ is high relative to your baseline, increase the size of the neural network by adding hidden layers. 3. Once $J_{train}(\\vec{w},b)$ is low enough, see if it does well on the cross validation set 4.  If the cross validation set $J_{cv}(\\vec{w},b)$ is too high, add more data, then test again from step 2. 5. Repeat until $J_{cv}(\\vec{w},b)$  is low enough for your liking.\n",
      "ü™ô SECTION to tokenize:  # Afternoon To-Do #meta #todoList  ## Priority Completed: Complete Python module Completed: Complete Bias and Variance ## Bonus/tomorrow: Completed: Note cleanup with Aliases (see Career and Study To - Do) To Do: Linear Algebra Note Migration Completed: Integrate with this daily note \t\tTo Do: Training, Test, and Dev Sets \t\tTo Do: regularization (#merge and integrate) \t\tTo Do: Cost and Loss Completed: Big overhaul of Bias and Variance with regards to todays notes. \n",
      "üí∂ section has 122\n",
      "ü™ô SECTION to tokenize:  # Machine Learning Specialization Notes: These notes will be integrated into other notes but kept here in their entirety. ## Bias and Variance Machine Learning Specialization Advanced Machine Learning Algorithms, Week 3  Models almost never work the first time you try them out. Let's see how we can fix them.  ### Diagnosing Bias/Variance See Bias and Variance  If you have more features, you can't visualize Bias and Variance  A more systematic way to see if you have high bias or high variance is to look at the performance of the algorithm on the Training, Test, and Dev Sets|training set and dev set  A characteristic of a High Bias (under-fit) model is that $Jtrain$ (cost of the Training, Test, and Dev Sets|training set) is high. A characteristic of a High Variance (overfit) model is that $J_{cv}$ is high but $J_{train}$ is low. #### Slide !Screenshot 2023-10-17 at 1.26.00 PM.png  ### Understanding Bias and Variance Take degree of polynomial $d$ As $d$ increases, that is to say we add degrees to the polynomial $J_{train}({\\vec{w},b})$  will decrease. $J_{cv}(\\vec{w},b)$ dips towards the middle, showing the ideal degree for the polynomial.  #### Slide !Screenshot 2023-10-17 at 1.29.21 PM.png  ### Technique for Diagnosing Bias and Variance How do you tell if your algorithm has a high bias or variance problem?  One key takeaway is that high bias means it is not even doing well on the training set and high variance means it does much worse on the cross validation set and the training set. #### High Bias (under-fit) In an under-fit model $J_{train}$ will be high and $J_{train} \\approx J_{cv}$  #### High Variance (overfit) In an over-fit model $J_{cv} \\gg J_{train}$ and $J_{train}$ will be low. #### High Bias and High Variance In a model with both high bias and high variance $J_{train}$ will be high and $J_{cv} \\gg J_{train}$  This is rare, and doesn't really happen for linear models with one $d$, but it does happen. This is when it overfits for some part of the input and under-fits for another part of the input. #### Slide !Screenshot 2023-10-17 at 1.38.51 PM.png  ## Regularization and bias/variance ### Linear Regression with Regularization Regularized Linear Regression model: Lambda $\\lambda$ is the regularization parameter (Big #merge and cleanup needed here in the regularization, Regularized Linear Regression, cost, Cost and Loss, loss, Loss Function, etc. Aliases are needed in many of these)  Take this model: $$f_{\\vec{w},b}(x)=w_1+w_2x^2 + w_4x^4+b$$ With this regularization|regularized Cost and Loss $$J(\\vec{w},b)=\\frac{1}{2m} \\sum_{i=1}^{m}(f_{\\vec{w},{b}}(\\vec{x}^{(i)}-y^{(i)})^2+\\frac{\\lambda}{2m} \\sum_{j=1}^nw_j^2$$  If lambda is large, say $\\lambda = 10,000$ than $w_1 \\approx 0, W_2 \\approx 0$ and $f_{\\vec{x},b}\\vec{x} \\approx b$, thus the model creates a flat line. This is underfit, $J_{train}()  On the other hand, if we set $\\lambda=0$, then we have a forth order polynomial with no regularization. We end up with a very overfit curve.  So, how do we find a good value for $\\lambda$? ##### Slide !Screenshot 2023-10-17 at 3.56.53 PM.png ### Choosing the Regularization parameter $\\lambda$   This will be similar to choosing $d$ with cross validation Try multiple values for $\\lambda$ and then choose the option with the lowest cost. ### Slide !Screenshot 2023-10-17 at 4.15.27 PM.png ### Bias and Variance as a Function of regularization parameter $\\lambda$  Cross Validation tries out many versions of $\\lambda$ and then chooses the one with the lowest cost. Cross validation will help find us a a good value of $d$ as well as $\\lambda$  #### Slide !Screenshot 2023-10-17 at 4.14.07 PM.png   ## Establishing A Baseline Level of Performance ### Speech Recognition example Job is to take in audio and output the text of what a person is saying  Training error $J_{train}$ is percentage of audio clips that the program does not transcribe correctly in it's entirety. Lets say: \tHuman Level Performance: 10.6 \t$J_{train}$: 10.8% \t$J_{cv}$: 14.8%  Why is human level error so high? There is lots of noise in the audio. It seems unfair to expect a learning algorithm to do much better. It is thus is more useful to measure the training error against the human error. So, looking at these results, $J_{train}$ is only 0.2$ higher than the human level performance whereas $J_{cv}$ is a full 4.2% higher. We can thus conclude that this algorithm has more of a variance problem than a bias problem. ### Establishing a baseline level of performance What is the level of error you can reasonably hope to get to? - Human level performance - Competing algorithms performance - Guess based on experience ### Bias Variance Examples Gap between baseline and training error shows high bias. A gap between training error and cross validation error shows high variance.  If your goal is perfection, the baseline would be zero. But for a lot of real world examples, like audio recognition, there is a lot of noise in the data, so you need a higher baseline.  If there is a gap between all three is high you have both high bias and high variance. #### Slide !Screenshot 2023-10-17 at 4.42.49 PM.png   ## Learning Curves  Noted in Learning Curves ### Overview Learning curves help understand how your learning algorithm is doing as a function of the amount of experience it has. Experience being the number of training examples it has.  The bigger the training set the harder it is to fit all examples perfectly. Thus; as the training set increases so does the training error $J_{train}(\\vec{w},b)$   Plotting a learning curve by training different models based on different subsets of training data is computationally expensive, so in practice it isn't done that often. But, it's a good mental visualization. #### Slides !Screenshot 2023-10-17 at 5.40.04 PM.png    ### High Bias Example If a learning algorithm suffers from high bias, getting more training data will not (by itself) help that much. #### Slide for High Bias !Screenshot 2023-10-17 at 5.44.29 PM.png  ### High Variance Example If a learning algorithm suffers from high variance, getting more training data is likely to help. #### Slide for High Variance !Screenshot 2023-10-17 at 5.49.45 PM.png    ## Deciding what to try next revisited ### Examples when Debugging an Algorithm: Get more training examples: fixes high variance Try smaller set of features: fixes high variance Try getting additional features: fixes high bias \tExamples; an algorithm that lacks information wont even do well on the training set Adding polynomial features ($x_1^2,X_2^2, etc$): Fixes high bias Decreasing $\\lambda$ Fixes high bias Increasing $\\lambda$ Fixes high variance \tForces the algorithm to force a smoother function.  **Note!** Don't randomly throw away training examples just to fix a high bias problem. ### Takeaway #merge with Bias and Variance ? If your algorithm has high variance, try simplifying your model or getting more training data. Simplification can mean a smaller set of features or an increased regularization If your algorithm has high bias, that is to say its not even doing well on the training set, you mainly need to make your model more powerful and flexible to fit more complex functions. To do so you can give it additional features, add polynomial features, or decrease $\\lambda$   ## Bias/Variance and Neural Networks ### The bias variance tradeoff Simple model = high bias Complex model = high variance Before neural networks, we had to worry about balancing this complexity between bias and variance. With neural networks we now are mostly worried about high variance.  ### Neural Networks and bias variance Large Neural Network|neural networks are low bias machines. If you make your neural network large enough you can almost always fit your training set well. #### Recipe for decreasing bias with a neural network 1. Train a neural network 2.  If the training set error $J_{train}(\\vec{w},b)$ is high relative to your baseline, increase the size of the neural network by adding hidden layers. 3. Once $J_{train}(\\vec{w},b)$ is low enough, see if it does well on the cross validation set 4.  If the cross validation set $J_{cv}(\\vec{w},b)$ is too high, add more data, then test again from step 2. 5. Repeat until $J_{cv}(\\vec{w},b)$  is low enough for your liking. ##### Slide Illustration !Screenshot 2023-10-17 at 6.38.46 PM.png ### Limitations and Notes Bigger networks are restricted by your computing power, data is restricted to the amount of data you have.  Sometimes you will pingpong back between high bias and high variance as you move through this recipe and develop a machine learning algorithm. Use these observations to shape what you do next in the process.  ### Neural Networks and Regularization A large neural network will usually do as well or better than a smaller one so long as regularization is chosen appropriately. Of course, larger neural networks are more computationally expensive. ### Neural Network Regularization #important  #function and TensorFlow implementation: !Screenshot 2023-10-17 at 6.46.28 PM.png*Note usually don't regularize B, it doesn't really affect anything*\n",
      "üí∂ section has 2323\n",
      "ü™ô SECTION to tokenize:  \n",
      "üí∂ section has 2\n",
      "ü™ô SECTION to tokenize:  ## Priority Completed: Complete Python module Completed: Complete Bias and Variance\n",
      "üí∂ section has 17\n",
      "ü™ô SECTION to tokenize:  ## Bonus/tomorrow: Completed: Note cleanup with Aliases (see Career and Study To - Do) To Do: Linear Algebra Note Migration Completed: Integrate with this daily note \t\tTo Do: Training, Test, and Dev Sets \t\tTo Do: regularization (#merge and integrate) \t\tTo Do: Cost and Loss Completed: Big overhaul of Bias and Variance with regards to todays notes.  # Machine Learning Specialization Notes: These notes will be integrated into other notes but kept here in their entirety.\n",
      "üí∂ section has 116\n",
      "ü™ô SECTION to tokenize:  ## Bias and Variance Machine Learning Specialization Advanced Machine Learning Algorithms, Week 3  Models almost never work the first time you try them out. Let's see how we can fix them.  ### Diagnosing Bias/Variance See Bias and Variance  If you have more features, you can't visualize Bias and Variance  A more systematic way to see if you have high bias or high variance is to look at the performance of the algorithm on the Training, Test, and Dev Sets|training set and dev set  A characteristic of a High Bias (under-fit) model is that $Jtrain$ (cost of the Training, Test, and Dev Sets|training set) is high. A characteristic of a High Variance (overfit) model is that $J_{cv}$ is high but $J_{train}$ is low. #### Slide !Screenshot 2023-10-17 at 1.26.00 PM.png  ### Understanding Bias and Variance Take degree of polynomial $d$ As $d$ increases, that is to say we add degrees to the polynomial $J_{train}({\\vec{w},b})$  will decrease. $J_{cv}(\\vec{w},b)$ dips towards the middle, showing the ideal degree for the polynomial.  #### Slide !Screenshot 2023-10-17 at 1.29.21 PM.png  ### Technique for Diagnosing Bias and Variance How do you tell if your algorithm has a high bias or variance problem?  One key takeaway is that high bias means it is not even doing well on the training set and high variance means it does much worse on the cross validation set and the training set. #### High Bias (under-fit) In an under-fit model $J_{train}$ will be high and $J_{train} \\approx J_{cv}$  #### High Variance (overfit) In an over-fit model $J_{cv} \\gg J_{train}$ and $J_{train}$ will be low. #### High Bias and High Variance In a model with both high bias and high variance $J_{train}$ will be high and $J_{cv} \\gg J_{train}$  This is rare, and doesn't really happen for linear models with one $d$, but it does happen. This is when it overfits for some part of the input and under-fits for another part of the input. #### Slide !Screenshot 2023-10-17 at 1.38.51 PM.png \n",
      "üí∂ section has 554\n",
      "ü™ô SECTION to tokenize:  ## Regularization and bias/variance ### Linear Regression with Regularization Regularized Linear Regression model: Lambda $\\lambda$ is the regularization parameter (Big #merge and cleanup needed here in the regularization, Regularized Linear Regression, cost, Cost and Loss, loss, Loss Function, etc. Aliases are needed in many of these)  Take this model: $$f_{\\vec{w},b}(x)=w_1+w_2x^2 + w_4x^4+b$$ With this regularization|regularized Cost and Loss $$J(\\vec{w},b)=\\frac{1}{2m} \\sum_{i=1}^{m}(f_{\\vec{w},{b}}(\\vec{x}^{(i)}-y^{(i)})^2+\\frac{\\lambda}{2m} \\sum_{j=1}^nw_j^2$$  If lambda is large, say $\\lambda = 10,000$ than $w_1 \\approx 0, W_2 \\approx 0$ and $f_{\\vec{x},b}\\vec{x} \\approx b$, thus the model creates a flat line. This is underfit, $J_{train}()  On the other hand, if we set $\\lambda=0$, then we have a forth order polynomial with no regularization. We end up with a very overfit curve.  So, how do we find a good value for $\\lambda$? ##### Slide !Screenshot 2023-10-17 at 3.56.53 PM.png ### Choosing the Regularization parameter $\\lambda$   This will be similar to choosing $d$ with cross validation Try multiple values for $\\lambda$ and then choose the option with the lowest cost. ### Slide !Screenshot 2023-10-17 at 4.15.27 PM.png ### Bias and Variance as a Function of regularization parameter $\\lambda$  Cross Validation tries out many versions of $\\lambda$ and then chooses the one with the lowest cost. Cross validation will help find us a a good value of $d$ as well as $\\lambda$  #### Slide !Screenshot 2023-10-17 at 4.14.07 PM.png  \n",
      "üí∂ section has 487\n",
      "ü™ô SECTION to tokenize:  ## Establishing A Baseline Level of Performance ### Speech Recognition example Job is to take in audio and output the text of what a person is saying  Training error $J_{train}$ is percentage of audio clips that the program does not transcribe correctly in it's entirety. Lets say: \tHuman Level Performance: 10.6 \t$J_{train}$: 10.8% \t$J_{cv}$: 14.8%  Why is human level error so high? There is lots of noise in the audio. It seems unfair to expect a learning algorithm to do much better. It is thus is more useful to measure the training error against the human error. So, looking at these results, $J_{train}$ is only 0.2$ higher than the human level performance whereas $J_{cv}$ is a full 4.2% higher. We can thus conclude that this algorithm has more of a variance problem than a bias problem. ### Establishing a baseline level of performance What is the level of error you can reasonably hope to get to? - Human level performance - Competing algorithms performance - Guess based on experience ### Bias Variance Examples Gap between baseline and training error shows high bias. A gap between training error and cross validation error shows high variance.  If your goal is perfection, the baseline would be zero. But for a lot of real world examples, like audio recognition, there is a lot of noise in the data, so you need a higher baseline.  If there is a gap between all three is high you have both high bias and high variance. #### Slide !Screenshot 2023-10-17 at 4.42.49 PM.png  \n",
      "üí∂ section has 355\n",
      "ü™ô SECTION to tokenize:  ## Learning Curves  Noted in Learning Curves ### Overview Learning curves help understand how your learning algorithm is doing as a function of the amount of experience it has. Experience being the number of training examples it has.  The bigger the training set the harder it is to fit all examples perfectly. Thus; as the training set increases so does the training error $J_{train}(\\vec{w},b)$   Plotting a learning curve by training different models based on different subsets of training data is computationally expensive, so in practice it isn't done that often. But, it's a good mental visualization. #### Slides !Screenshot 2023-10-17 at 5.40.04 PM.png    ### High Bias Example If a learning algorithm suffers from high bias, getting more training data will not (by itself) help that much. #### Slide for High Bias !Screenshot 2023-10-17 at 5.44.29 PM.png  ### High Variance Example If a learning algorithm suffers from high variance, getting more training data is likely to help. #### Slide for High Variance !Screenshot 2023-10-17 at 5.49.45 PM.png   \n",
      "üí∂ section has 257\n",
      "ü™ô SECTION to tokenize:  ## Deciding what to try next revisited ### Examples when Debugging an Algorithm: Get more training examples: fixes high variance Try smaller set of features: fixes high variance Try getting additional features: fixes high bias \tExamples; an algorithm that lacks information wont even do well on the training set Adding polynomial features ($x_1^2,X_2^2, etc$): Fixes high bias Decreasing $\\lambda$ Fixes high bias Increasing $\\lambda$ Fixes high variance \tForces the algorithm to force a smoother function.  **Note!** Don't randomly throw away training examples just to fix a high bias problem. ### Takeaway #merge with Bias and Variance ? If your algorithm has high variance, try simplifying your model or getting more training data. Simplification can mean a smaller set of features or an increased regularization If your algorithm has high bias, that is to say its not even doing well on the training set, you mainly need to make your model more powerful and flexible to fit more complex functions. To do so you can give it additional features, add polynomial features, or decrease $\\lambda$  \n",
      "üí∂ section has 239\n",
      "ü™ô SECTION to tokenize:  ## Bias/Variance and Neural Networks ### The bias variance tradeoff Simple model = high bias Complex model = high variance Before neural networks, we had to worry about balancing this complexity between bias and variance. With neural networks we now are mostly worried about high variance.  ### Neural Networks and bias variance Large Neural Network|neural networks are low bias machines. If you make your neural network large enough you can almost always fit your training set well. #### Recipe for decreasing bias with a neural network 1. Train a neural network 2.  If the training set error $J_{train}(\\vec{w},b)$ is high relative to your baseline, increase the size of the neural network by adding hidden layers. 3. Once $J_{train}(\\vec{w},b)$ is low enough, see if it does well on the cross validation set 4.  If the cross validation set $J_{cv}(\\vec{w},b)$ is too high, add more data, then test again from step 2. 5. Repeat until $J_{cv}(\\vec{w},b)$  is low enough for your liking. ##### Slide Illustration !Screenshot 2023-10-17 at 6.38.46 PM.png ### Limitations and Notes Bigger networks are restricted by your computing power, data is restricted to the amount of data you have.  Sometimes you will pingpong back between high bias and high variance as you move through this recipe and develop a machine learning algorithm. Use these observations to shape what you do next in the process.  ### Neural Networks and Regularization A large neural network will usually do as well or better than a smaller one so long as regularization is chosen appropriately. Of course, larger neural networks are more computationally expensive. ### Neural Network Regularization #important  #function and TensorFlow implementation: !Screenshot 2023-10-17 at 6.46.28 PM.png*Note usually don't regularize B, it doesn't really affect anything* # Python Notes\n",
      "üí∂ section has 422\n",
      "ü™ô SECTION to tokenize:  ## Keywords From codecademy - Continue Keyword: used inside a loop to skip the remaining loop code block and begin the next loop iteration. - Break keyword escapes the loop, regardless of the iteration number. Once break executes the program will continue to execute after the loop.\n",
      "üí∂ section has 58\n",
      "ü™ô SECTION to tokenize:  \n",
      "üí∂ section has 2\n",
      "ü™ô SECTION to tokenize:  ### Diagnosing Bias/Variance See Bias and Variance  If you have more features, you can't visualize Bias and Variance  A more systematic way to see if you have high bias or high variance is to look at the performance of the algorithm on the Training, Test, and Dev Sets|training set and dev set  A characteristic of a High Bias (under-fit) model is that $Jtrain$ (cost of the Training, Test, and Dev Sets|training set) is high. A characteristic of a High Variance (overfit) model is that $J_{cv}$ is high but $J_{train}$ is low. #### Slide !Screenshot 2023-10-17 at 1.26.00 PM.png \n",
      "üí∂ section has 162\n",
      "ü™ô SECTION to tokenize:  ### Understanding Bias and Variance Take degree of polynomial $d$ As $d$ increases, that is to say we add degrees to the polynomial $J_{train}({\\vec{w},b})$  will decrease. $J_{cv}(\\vec{w},b)$ dips towards the middle, showing the ideal degree for the polynomial.  #### Slide !Screenshot 2023-10-17 at 1.29.21 PM.png \n",
      "üí∂ section has 104\n",
      "ü™ô SECTION to tokenize:  ### Technique for Diagnosing Bias and Variance How do you tell if your algorithm has a high bias or variance problem?  One key takeaway is that high bias means it is not even doing well on the training set and high variance means it does much worse on the cross validation set and the training set. #### High Bias (under-fit) In an under-fit model $J_{train}$ will be high and $J_{train} \\approx J_{cv}$  #### High Variance (overfit) In an over-fit model $J_{cv} \\gg J_{train}$ and $J_{train}$ will be low. #### High Bias and High Variance In a model with both high bias and high variance $J_{train}$ will be high and $J_{cv} \\gg J_{train}$  This is rare, and doesn't really happen for linear models with one $d$, but it does happen. This is when it overfits for some part of the input and under-fits for another part of the input. #### Slide !Screenshot 2023-10-17 at 1.38.51 PM.png  ## Regularization and bias/variance\n",
      "üí∂ section has 258\n",
      "ü™ô SECTION to tokenize:  ### Linear Regression with Regularization Regularized Linear Regression model: Lambda $\\lambda$ is the regularization parameter (Big #merge and cleanup needed here in the regularization, Regularized Linear Regression, cost, Cost and Loss, loss, Loss Function, etc. Aliases are needed in many of these)  Take this model: $$f_{\\vec{w},b}(x)=w_1+w_2x^2 + w_4x^4+b$$ With this regularization|regularized Cost and Loss $$J(\\vec{w},b)=\\frac{1}{2m} \\sum_{i=1}^{m}(f_{\\vec{w},{b}}(\\vec{x}^{(i)}-y^{(i)})^2+\\frac{\\lambda}{2m} \\sum_{j=1}^nw_j^2$$  If lambda is large, say $\\lambda = 10,000$ than $w_1 \\approx 0, W_2 \\approx 0$ and $f_{\\vec{x},b}\\vec{x} \\approx b$, thus the model creates a flat line. This is underfit, $J_{train}()  On the other hand, if we set $\\lambda=0$, then we have a forth order polynomial with no regularization. We end up with a very overfit curve.  So, how do we find a good value for $\\lambda$? ##### Slide !Screenshot 2023-10-17 at 3.56.53 PM.png\n",
      "üí∂ section has 338\n",
      "ü™ô SECTION to tokenize:  ### Choosing the Regularization parameter $\\lambda$   This will be similar to choosing $d$ with cross validation Try multiple values for $\\lambda$ and then choose the option with the lowest cost.\n",
      "üí∂ section has 43\n",
      "ü™ô SECTION to tokenize:  ### Slide !Screenshot 2023-10-17 at 4.15.27 PM.png\n",
      "üí∂ section has 21\n",
      "ü™ô SECTION to tokenize:  ### Bias and Variance as a Function of regularization parameter $\\lambda$  Cross Validation tries out many versions of $\\lambda$ and then chooses the one with the lowest cost. Cross validation will help find us a a good value of $d$ as well as $\\lambda$  #### Slide !Screenshot 2023-10-17 at 4.14.07 PM.png   ## Establishing A Baseline Level of Performance\n",
      "üí∂ section has 93\n",
      "ü™ô SECTION to tokenize:  ### Speech Recognition example Job is to take in audio and output the text of what a person is saying  Training error $J_{train}$ is percentage of audio clips that the program does not transcribe correctly in it's entirety. Lets say: \tHuman Level Performance: 10.6 \t$J_{train}$: 10.8% \t$J_{cv}$: 14.8%  Why is human level error so high? There is lots of noise in the audio. It seems unfair to expect a learning algorithm to do much better. It is thus is more useful to measure the training error against the human error. So, looking at these results, $J_{train}$ is only 0.2$ higher than the human level performance whereas $J_{cv}$ is a full 4.2% higher. We can thus conclude that this algorithm has more of a variance problem than a bias problem.\n",
      "üí∂ section has 194\n",
      "ü™ô SECTION to tokenize:  ### Establishing a baseline level of performance What is the level of error you can reasonably hope to get to? - Human level performance - Competing algorithms performance - Guess based on experience\n",
      "üí∂ section has 39\n",
      "ü™ô SECTION to tokenize:  ### Bias Variance Examples Gap between baseline and training error shows high bias. A gap between training error and cross validation error shows high variance.  If your goal is perfection, the baseline would be zero. But for a lot of real world examples, like audio recognition, there is a lot of noise in the data, so you need a higher baseline.  If there is a gap between all three is high you have both high bias and high variance. #### Slide !Screenshot 2023-10-17 at 4.42.49 PM.png   ## Learning Curves  Noted in Learning Curves\n",
      "üí∂ section has 127\n",
      "ü™ô SECTION to tokenize:  ### Overview Learning curves help understand how your learning algorithm is doing as a function of the amount of experience it has. Experience being the number of training examples it has.  The bigger the training set the harder it is to fit all examples perfectly. Thus; as the training set increases so does the training error $J_{train}(\\vec{w},b)$   Plotting a learning curve by training different models based on different subsets of training data is computationally expensive, so in practice it isn't done that often. But, it's a good mental visualization. #### Slides !Screenshot 2023-10-17 at 5.40.04 PM.png   \n",
      "üí∂ section has 143\n",
      "ü™ô SECTION to tokenize:  ### High Bias Example If a learning algorithm suffers from high bias, getting more training data will not (by itself) help that much. #### Slide for High Bias !Screenshot 2023-10-17 at 5.44.29 PM.png \n",
      "üí∂ section has 55\n",
      "ü™ô SECTION to tokenize:  ### High Variance Example If a learning algorithm suffers from high variance, getting more training data is likely to help. #### Slide for High Variance !Screenshot 2023-10-17 at 5.49.45 PM.png    ## Deciding what to try next revisited\n",
      "üí∂ section has 61\n",
      "ü™ô SECTION to tokenize:  ### Examples when Debugging an Algorithm: Get more training examples: fixes high variance Try smaller set of features: fixes high variance Try getting additional features: fixes high bias \tExamples; an algorithm that lacks information wont even do well on the training set Adding polynomial features ($x_1^2,X_2^2, etc$): Fixes high bias Decreasing $\\lambda$ Fixes high bias Increasing $\\lambda$ Fixes high variance \tForces the algorithm to force a smoother function.  **Note!** Don't randomly throw away training examples just to fix a high bias problem.\n",
      "üí∂ section has 124\n",
      "ü™ô SECTION to tokenize:  ### Takeaway #merge with Bias and Variance ? If your algorithm has high variance, try simplifying your model or getting more training data. Simplification can mean a smaller set of features or an increased regularization If your algorithm has high bias, that is to say its not even doing well on the training set, you mainly need to make your model more powerful and flexible to fit more complex functions. To do so you can give it additional features, add polynomial features, or decrease $\\lambda$   ## Bias/Variance and Neural Networks\n",
      "üí∂ section has 117\n",
      "ü™ô SECTION to tokenize:  ### The bias variance tradeoff Simple model = high bias Complex model = high variance Before neural networks, we had to worry about balancing this complexity between bias and variance. With neural networks we now are mostly worried about high variance. \n",
      "üí∂ section has 48\n",
      "ü™ô SECTION to tokenize:  ### Neural Networks and bias variance Large Neural Network|neural networks are low bias machines. If you make your neural network large enough you can almost always fit your training set well. #### Recipe for decreasing bias with a neural network 1. Train a neural network 2.  If the training set error $J_{train}(\\vec{w},b)$ is high relative to your baseline, increase the size of the neural network by adding hidden layers. 3. Once $J_{train}(\\vec{w},b)$ is low enough, see if it does well on the cross validation set 4.  If the cross validation set $J_{cv}(\\vec{w},b)$ is too high, add more data, then test again from step 2. 5. Repeat until $J_{cv}(\\vec{w},b)$  is low enough for your liking. ##### Slide Illustration !Screenshot 2023-10-17 at 6.38.46 PM.png\n",
      "üí∂ section has 206\n",
      "ü™ô SECTION to tokenize:  ### Limitations and Notes Bigger networks are restricted by your computing power, data is restricted to the amount of data you have.  Sometimes you will pingpong back between high bias and high variance as you move through this recipe and develop a machine learning algorithm. Use these observations to shape what you do next in the process. \n",
      "üí∂ section has 69\n",
      "ü™ô SECTION to tokenize:  ### Neural Networks and Regularization A large neural network will usually do as well or better than a smaller one so long as regularization is chosen appropriately. Of course, larger neural networks are more computationally expensive.\n",
      "üí∂ section has 44\n",
      "ü™ô SECTION to tokenize:  \n",
      "üí∂ section has 2\n",
      "ü™ô SECTION to tokenize:  #### Slide !Screenshot 2023-10-17 at 1.26.00 PM.png  ### Understanding Bias and Variance Take degree of polynomial $d$ As $d$ increases, that is to say we add degrees to the polynomial $J_{train}({\\vec{w},b})$  will decrease. $J_{cv}(\\vec{w},b)$ dips towards the middle, showing the ideal degree for the polynomial. \n",
      "üí∂ section has 103\n",
      "ü™ô SECTION to tokenize:  #### Slide !Screenshot 2023-10-17 at 1.29.21 PM.png  ### Technique for Diagnosing Bias and Variance How do you tell if your algorithm has a high bias or variance problem?  One key takeaway is that high bias means it is not even doing well on the training set and high variance means it does much worse on the cross validation set and the training set.\n",
      "üí∂ section has 84\n",
      "ü™ô SECTION to tokenize:  #### High Bias (under-fit) In an under-fit model $J_{train}$ will be high and $J_{train} \\approx J_{cv}$ \n",
      "üí∂ section has 41\n",
      "ü™ô SECTION to tokenize:  #### High Variance (overfit) In an over-fit model $J_{cv} \\gg J_{train}$ and $J_{train}$ will be low.\n",
      "üí∂ section has 39\n",
      "ü™ô SECTION to tokenize:  #### High Bias and High Variance In a model with both high bias and high variance $J_{train}$ will be high and $J_{cv} \\gg J_{train}$  This is rare, and doesn't really happen for linear models with one $d$, but it does happen. This is when it overfits for some part of the input and under-fits for another part of the input.\n",
      "üí∂ section has 88\n",
      "ü™ô SECTION to tokenize:  #### Slide !Screenshot 2023-10-17 at 1.38.51 PM.png  ## Regularization and bias/variance ### Linear Regression with Regularization Regularized Linear Regression model: Lambda $\\lambda$ is the regularization parameter (Big #merge and cleanup needed here in the regularization, Regularized Linear Regression, cost, Cost and Loss, loss, Loss Function, etc. Aliases are needed in many of these)  Take this model: $$f_{\\vec{w},b}(x)=w_1+w_2x^2 + w_4x^4+b$$ With this regularization|regularized Cost and Loss $$J(\\vec{w},b)=\\frac{1}{2m} \\sum_{i=1}^{m}(f_{\\vec{w},{b}}(\\vec{x}^{(i)}-y^{(i)})^2+\\frac{\\lambda}{2m} \\sum_{j=1}^nw_j^2$$  If lambda is large, say $\\lambda = 10,000$ than $w_1 \\approx 0, W_2 \\approx 0$ and $f_{\\vec{x},b}\\vec{x} \\approx b$, thus the model creates a flat line. This is underfit, $J_{train}()  On the other hand, if we set $\\lambda=0$, then we have a forth order polynomial with no regularization. We end up with a very overfit curve.  So, how do we find a good value for $\\lambda$? ##### Slide !Screenshot 2023-10-17 at 3.56.53 PM.png ### Choosing the Regularization parameter $\\lambda$   This will be similar to choosing $d$ with cross validation Try multiple values for $\\lambda$ and then choose the option with the lowest cost. ### Slide !Screenshot 2023-10-17 at 4.15.27 PM.png ### Bias and Variance as a Function of regularization parameter $\\lambda$  Cross Validation tries out many versions of $\\lambda$ and then chooses the one with the lowest cost. Cross validation will help find us a a good value of $d$ as well as $\\lambda$ \n",
      "üí∂ section has 485\n",
      "ü™ô SECTION to tokenize:  #### Slide !Screenshot 2023-10-17 at 4.14.07 PM.png   ## Establishing A Baseline Level of Performance ### Speech Recognition example Job is to take in audio and output the text of what a person is saying  Training error $J_{train}$ is percentage of audio clips that the program does not transcribe correctly in it's entirety. Lets say: \tHuman Level Performance: 10.6 \t$J_{train}$: 10.8% \t$J_{cv}$: 14.8%  Why is human level error so high? There is lots of noise in the audio. It seems unfair to expect a learning algorithm to do much better. It is thus is more useful to measure the training error against the human error. So, looking at these results, $J_{train}$ is only 0.2$ higher than the human level performance whereas $J_{cv}$ is a full 4.2% higher. We can thus conclude that this algorithm has more of a variance problem than a bias problem. ### Establishing a baseline level of performance What is the level of error you can reasonably hope to get to? - Human level performance - Competing algorithms performance - Guess based on experience ### Bias Variance Examples Gap between baseline and training error shows high bias. A gap between training error and cross validation error shows high variance.  If your goal is perfection, the baseline would be zero. But for a lot of real world examples, like audio recognition, there is a lot of noise in the data, so you need a higher baseline.  If there is a gap between all three is high you have both high bias and high variance.\n",
      "üí∂ section has 354\n",
      "ü™ô SECTION to tokenize:  #### Slide !Screenshot 2023-10-17 at 4.42.49 PM.png   ## Learning Curves  Noted in Learning Curves ### Overview Learning curves help understand how your learning algorithm is doing as a function of the amount of experience it has. Experience being the number of training examples it has.  The bigger the training set the harder it is to fit all examples perfectly. Thus; as the training set increases so does the training error $J_{train}(\\vec{w},b)$   Plotting a learning curve by training different models based on different subsets of training data is computationally expensive, so in practice it isn't done that often. But, it's a good mental visualization.\n",
      "üí∂ section has 151\n",
      "ü™ô SECTION to tokenize:  #### Slides !Screenshot 2023-10-17 at 5.40.04 PM.png    ### High Bias Example If a learning algorithm suffers from high bias, getting more training data will not (by itself) help that much.\n",
      "üí∂ section has 53\n",
      "ü™ô SECTION to tokenize:  #### Slide for High Bias !Screenshot 2023-10-17 at 5.44.29 PM.png  ### High Variance Example If a learning algorithm suffers from high variance, getting more training data is likely to help.\n",
      "üí∂ section has 49\n",
      "ü™ô SECTION to tokenize:  #### Slide for High Variance !Screenshot 2023-10-17 at 5.49.45 PM.png    ## Deciding what to try next revisited ### Examples when Debugging an Algorithm: Get more training examples: fixes high variance Try smaller set of features: fixes high variance Try getting additional features: fixes high bias \tExamples; an algorithm that lacks information wont even do well on the training set Adding polynomial features ($x_1^2,X_2^2, etc$): Fixes high bias Decreasing $\\lambda$ Fixes high bias Increasing $\\lambda$ Fixes high variance \tForces the algorithm to force a smoother function.  **Note!** Don't randomly throw away training examples just to fix a high bias problem. ### Takeaway #merge with Bias and Variance ? If your algorithm has high variance, try simplifying your model or getting more training data. Simplification can mean a smaller set of features or an increased regularization If your algorithm has high bias, that is to say its not even doing well on the training set, you mainly need to make your model more powerful and flexible to fit more complex functions. To do so you can give it additional features, add polynomial features, or decrease $\\lambda$   ## Bias/Variance and Neural Networks ### The bias variance tradeoff Simple model = high bias Complex model = high variance Before neural networks, we had to worry about balancing this complexity between bias and variance. With neural networks we now are mostly worried about high variance.  ### Neural Networks and bias variance Large Neural Network|neural networks are low bias machines. If you make your neural network large enough you can almost always fit your training set well.\n",
      "üí∂ section has 356\n",
      "ü™ô SECTION to tokenize:  \n",
      "üí∂ section has 2\n",
      "ü™ô SECTION to tokenize:  ##### Slide !Screenshot 2023-10-17 at 3.56.53 PM.png ### Choosing the Regularization parameter $\\lambda$   This will be similar to choosing $d$ with cross validation Try multiple values for $\\lambda$ and then choose the option with the lowest cost. ### Slide !Screenshot 2023-10-17 at 4.15.27 PM.png ### Bias and Variance as a Function of regularization parameter $\\lambda$  Cross Validation tries out many versions of $\\lambda$ and then chooses the one with the lowest cost. Cross validation will help find us a a good value of $d$ as well as $\\lambda$  #### Slide !Screenshot 2023-10-17 at 4.14.07 PM.png   ## Establishing A Baseline Level of Performance ### Speech Recognition example Job is to take in audio and output the text of what a person is saying  Training error $J_{train}$ is percentage of audio clips that the program does not transcribe correctly in it's entirety. Lets say: \tHuman Level Performance: 10.6 \t$J_{train}$: 10.8% \t$J_{cv}$: 14.8%  Why is human level error so high? There is lots of noise in the audio. It seems unfair to expect a learning algorithm to do much better. It is thus is more useful to measure the training error against the human error. So, looking at these results, $J_{train}$ is only 0.2$ higher than the human level performance whereas $J_{cv}$ is a full 4.2% higher. We can thus conclude that this algorithm has more of a variance problem than a bias problem. ### Establishing a baseline level of performance What is the level of error you can reasonably hope to get to? - Human level performance - Competing algorithms performance - Guess based on experience ### Bias Variance Examples Gap between baseline and training error shows high bias. A gap between training error and cross validation error shows high variance.  If your goal is perfection, the baseline would be zero. But for a lot of real world examples, like audio recognition, there is a lot of noise in the data, so you need a higher baseline.  If there is a gap between all three is high you have both high bias and high variance. #### Slide !Screenshot 2023-10-17 at 4.42.49 PM.png   ## Learning Curves  Noted in Learning Curves ### Overview Learning curves help understand how your learning algorithm is doing as a function of the amount of experience it has. Experience being the number of training examples it has.  The bigger the training set the harder it is to fit all examples perfectly. Thus; as the training set increases so does the training error $J_{train}(\\vec{w},b)$   Plotting a learning curve by training different models based on different subsets of training data is computationally expensive, so in practice it isn't done that often. But, it's a good mental visualization. #### Slides !Screenshot 2023-10-17 at 5.40.04 PM.png    ### High Bias Example If a learning algorithm suffers from high bias, getting more training data will not (by itself) help that much. #### Slide for High Bias !Screenshot 2023-10-17 at 5.44.29 PM.png  ### High Variance Example If a learning algorithm suffers from high variance, getting more training data is likely to help. #### Slide for High Variance !Screenshot 2023-10-17 at 5.49.45 PM.png    ## Deciding what to try next revisited ### Examples when Debugging an Algorithm: Get more training examples: fixes high variance Try smaller set of features: fixes high variance Try getting additional features: fixes high bias \tExamples; an algorithm that lacks information wont even do well on the training set Adding polynomial features ($x_1^2,X_2^2, etc$): Fixes high bias Decreasing $\\lambda$ Fixes high bias Increasing $\\lambda$ Fixes high variance \tForces the algorithm to force a smoother function.  **Note!** Don't randomly throw away training examples just to fix a high bias problem. ### Takeaway #merge with Bias and Variance ? If your algorithm has high variance, try simplifying your model or getting more training data. Simplification can mean a smaller set of features or an increased regularization If your algorithm has high bias, that is to say its not even doing well on the training set, you mainly need to make your model more powerful and flexible to fit more complex functions. To do so you can give it additional features, add polynomial features, or decrease $\\lambda$   ## Bias/Variance and Neural Networks ### The bias variance tradeoff Simple model = high bias Complex model = high variance Before neural networks, we had to worry about balancing this complexity between bias and variance. With neural networks we now are mostly worried about high variance.  ### Neural Networks and bias variance Large Neural Network|neural networks are low bias machines. If you make your neural network large enough you can almost always fit your training set well. #### Recipe for decreasing bias with a neural network 1. Train a neural network 2.  If the training set error $J_{train}(\\vec{w},b)$ is high relative to your baseline, increase the size of the neural network by adding hidden layers. 3. Once $J_{train}(\\vec{w},b)$ is low enough, see if it does well on the cross validation set 4.  If the cross validation set $J_{cv}(\\vec{w},b)$ is too high, add more data, then test again from step 2. 5. Repeat until $J_{cv}(\\vec{w},b)$  is low enough for your liking.\n",
      "üí∂ section has 1246\n",
      "üêò Largest Token size is 2323, which is Too Big (max tokens are 1024)\n",
      "ü´Ä there are 46 new sections after splitting the body\n",
      "SECTIONS:\n",
      "['#dailyNote   spam for note testing  more preamble  ', '#', ' ', 'A', 'f', 't', 'e', 'r', 'n', 'o', 'o', 'n', ' ', 'T', 'o', '-', 'D', 'o', ' ', '#', 'm', 'e', 't', 'a', ' ', '#', 't', 'o', 'd', 'o', 'L', 'i', 's', 't', ' ', ' ', '#', '#', ' ', 'P', 'r', 'i', 'o', 'r', 'i', 't', 'y', ' ', 'C', 'o', 'm', 'p', 'l', 'e', 't', 'e', 'd', ':', ' ', 'C', 'o', 'm', 'p', 'l', 'e', 't', 'e', ' ', 'P', 'y', 't', 'h', 'o', 'n', ' ', 'm', 'o', 'd', 'u', 'l', 'e', ' ', 'C', 'o', 'm', 'p', 'l', 'e', 't', 'e', 'd', ':', ' ', 'C', 'o', 'm', 'p', 'l', 'e', 't', 'e', ' ', 'B', 'i', 'a', 's', ' ', 'a', 'n', 'd', ' ', 'V', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', '#', '#', ' ', 'B', 'o', 'n', 'u', 's', '/', 't', 'o', 'm', 'o', 'r', 'r', 'o', 'w', ':', ' ', 'C', 'o', 'm', 'p', 'l', 'e', 't', 'e', 'd', ':', ' ', 'N', 'o', 't', 'e', ' ', 'c', 'l', 'e', 'a', 'n', 'u', 'p', ' ', 'w', 'i', 't', 'h', ' ', 'A', 'l', 'i', 'a', 's', 'e', 's', ' ', '(', 's', 'e', 'e', ' ', 'C', 'a', 'r', 'e', 'e', 'r', ' ', 'a', 'n', 'd', ' ', 'S', 't', 'u', 'd', 'y', ' ', 'T', 'o', ' ', '-', ' ', 'D', 'o', ')', ' ', 'T', 'o', ' ', 'D', 'o', ':', ' ', 'L', 'i', 'n', 'e', 'a', 'r', ' ', 'A', 'l', 'g', 'e', 'b', 'r', 'a', ' ', 'N', 'o', 't', 'e', ' ', 'M', 'i', 'g', 'r', 'a', 't', 'i', 'o', 'n', ' ', 'C', 'o', 'm', 'p', 'l', 'e', 't', 'e', 'd', ':', ' ', 'I', 'n', 't', 'e', 'g', 'r', 'a', 't', 'e', ' ', 'w', 'i', 't', 'h', ' ', 't', 'h', 'i', 's', ' ', 'd', 'a', 'i', 'l', 'y', ' ', 'n', 'o', 't', 'e', ' ', '\\t', '\\t', 'T', 'o', ' ', 'D', 'o', ':', ' ', 'T', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ',', ' ', 'T', 'e', 's', 't', ',', ' ', 'a', 'n', 'd', ' ', 'D', 'e', 'v', ' ', 'S', 'e', 't', 's', ' ', '\\t', '\\t', 'T', 'o', ' ', 'D', 'o', ':', ' ', 'r', 'e', 'g', 'u', 'l', 'a', 'r', 'i', 'z', 'a', 't', 'i', 'o', 'n', ' ', '(', '#', 'm', 'e', 'r', 'g', 'e', ' ', 'a', 'n', 'd', ' ', 'i', 'n', 't', 'e', 'g', 'r', 'a', 't', 'e', ')', ' ', '\\t', '\\t', 'T', 'o', ' ', 'D', 'o', ':', ' ', 'C', 'o', 's', 't', ' ', 'a', 'n', 'd', ' ', 'L', 'o', 's', 's', ' ', 'C', 'o', 'm', 'p', 'l', 'e', 't', 'e', 'd', ':', ' ', 'B', 'i', 'g', ' ', 'o', 'v', 'e', 'r', 'h', 'a', 'u', 'l', ' ', 'o', 'f', ' ', 'B', 'i', 'a', 's', ' ', 'a', 'n', 'd', ' ', 'V', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', 'w', 'i', 't', 'h', ' ', 'r', 'e', 'g', 'a', 'r', 'd', 's', ' ', 't', 'o', ' ', 't', 'o', 'd', 'a', 'y', 's', ' ', 'n', 'o', 't', 'e', 's', '.', ' ', '#', ' ', 'M', 'a', 'c', 'h', 'i', 'n', 'e', ' ', 'L', 'e', 'a', 'r', 'n', 'i', 'n', 'g', ' ', 'S', 'p', 'e', 'c', 'i', 'a', 'l', 'i', 'z', 'a', 't', 'i', 'o', 'n', ' ', 'N', 'o', 't', 'e', 's', ':', ' ', 'T', 'h', 'e', 's', 'e', ' ', 'n', 'o', 't', 'e', 's', ' ', 'w', 'i', 'l', 'l', ' ', 'b', 'e', ' ', 'i', 'n', 't', 'e', 'g', 'r', 'a', 't', 'e', 'd', ' ', 'i', 'n', 't', 'o', ' ', 'o', 't', 'h', 'e', 'r', ' ', 'n', 'o', 't', 'e', 's', ' ', 'b', 'u', 't', ' ', 'k', 'e', 'p', 't', ' ', 'h', 'e', 'r', 'e', ' ', 'i', 'n', ' ', 't', 'h', 'e', 'i', 'r', ' ', 'e', 'n', 't', 'i', 'r', 'e', 't', 'y', '.', ' ', '#', '#', ' ', 'B', 'i', 'a', 's', ' ', 'a', 'n', 'd', ' ', 'V', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', 'M', 'a', 'c', 'h', 'i', 'n', 'e', ' ', 'L', 'e', 'a', 'r', 'n', 'i', 'n', 'g', ' ', 'S', 'p', 'e', 'c', 'i', 'a', 'l', 'i', 'z', 'a', 't', 'i', 'o', 'n', ' ', 'A', 'd', 'v', 'a', 'n', 'c', 'e', 'd', ' ', 'M', 'a', 'c', 'h', 'i', 'n', 'e', ' ', 'L', 'e', 'a', 'r', 'n', 'i', 'n', 'g', ' ', 'A', 'l', 'g', 'o', 'r', 'i', 't', 'h', 'm', 's', ',', ' ', 'W', 'e', 'e', 'k', ' ', '3', ' ', ' ', 'M', 'o', 'd', 'e', 'l', 's', ' ', 'a', 'l', 'm', 'o', 's', 't', ' ', 'n', 'e', 'v', 'e', 'r', ' ', 'w', 'o', 'r', 'k', ' ', 't', 'h', 'e', ' ', 'f', 'i', 'r', 's', 't', ' ', 't', 'i', 'm', 'e', ' ', 'y', 'o', 'u', ' ', 't', 'r', 'y', ' ', 't', 'h', 'e', 'm', ' ', 'o', 'u', 't', '.', ' ', 'L', 'e', 't', \"'\", 's', ' ', 's', 'e', 'e', ' ', 'h', 'o', 'w', ' ', 'w', 'e', ' ', 'c', 'a', 'n', ' ', 'f', 'i', 'x', ' ', 't', 'h', 'e', 'm', '.', ' ', ' ', '#', '#', '#', ' ', 'D', 'i', 'a', 'g', 'n', 'o', 's', 'i', 'n', 'g', ' ', 'B', 'i', 'a', 's', '/', 'V', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', 'S', 'e', 'e', ' ', 'B', 'i', 'a', 's', ' ', 'a', 'n', 'd', ' ', 'V', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', ' ', 'I', 'f', ' ', 'y', 'o', 'u', ' ', 'h', 'a', 'v', 'e', ' ', 'm', 'o', 'r', 'e', ' ', 'f', 'e', 'a', 't', 'u', 'r', 'e', 's', ',', ' ', 'y', 'o', 'u', ' ', 'c', 'a', 'n', \"'\", 't', ' ', 'v', 'i', 's', 'u', 'a', 'l', 'i', 'z', 'e', ' ', 'B', 'i', 'a', 's', ' ', 'a', 'n', 'd', ' ', 'V', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', ' ', 'A', ' ', 'm', 'o', 'r', 'e', ' ', 's', 'y', 's', 't', 'e', 'm', 'a', 't', 'i', 'c', ' ', 'w', 'a', 'y', ' ', 't', 'o', ' ', 's', 'e', 'e', ' ', 'i', 'f', ' ', 'y', 'o', 'u', ' ', 'h', 'a', 'v', 'e', ' ', 'h', 'i', 'g', 'h', ' ', 'b', 'i', 'a', 's', ' ', 'o', 'r', ' ', 'h', 'i', 'g', 'h', ' ', 'v', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', 'i', 's', ' ', 't', 'o', ' ', 'l', 'o', 'o', 'k', ' ', 'a', 't', ' ', 't', 'h', 'e', ' ', 'p', 'e', 'r', 'f', 'o', 'r', 'm', 'a', 'n', 'c', 'e', ' ', 'o', 'f', ' ', 't', 'h', 'e', ' ', 'a', 'l', 'g', 'o', 'r', 'i', 't', 'h', 'm', ' ', 'o', 'n', ' ', 't', 'h', 'e', ' ', 'T', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ',', ' ', 'T', 'e', 's', 't', ',', ' ', 'a', 'n', 'd', ' ', 'D', 'e', 'v', ' ', 'S', 'e', 't', 's', '|', 't', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ' ', 's', 'e', 't', ' ', 'a', 'n', 'd', ' ', 'd', 'e', 'v', ' ', 's', 'e', 't', ' ', ' ', 'A', ' ', 'c', 'h', 'a', 'r', 'a', 'c', 't', 'e', 'r', 'i', 's', 't', 'i', 'c', ' ', 'o', 'f', ' ', 'a', ' ', 'H', 'i', 'g', 'h', ' ', 'B', 'i', 'a', 's', ' ', '(', 'u', 'n', 'd', 'e', 'r', '-', 'f', 'i', 't', ')', ' ', 'm', 'o', 'd', 'e', 'l', ' ', 'i', 's', ' ', 't', 'h', 'a', 't', ' ', '$', 'J', 't', 'r', 'a', 'i', 'n', '$', ' ', '(', 'c', 'o', 's', 't', ' ', 'o', 'f', ' ', 't', 'h', 'e', ' ', 'T', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ',', ' ', 'T', 'e', 's', 't', ',', ' ', 'a', 'n', 'd', ' ', 'D', 'e', 'v', ' ', 'S', 'e', 't', 's', '|', 't', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ' ', 's', 'e', 't', ')', ' ', 'i', 's', ' ', 'h', 'i', 'g', 'h', '.', ' ', 'A', ' ', 'c', 'h', 'a', 'r', 'a', 'c', 't', 'e', 'r', 'i', 's', 't', 'i', 'c', ' ', 'o', 'f', ' ', 'a', ' ', 'H', 'i', 'g', 'h', ' ', 'V', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', '(', 'o', 'v', 'e', 'r', 'f', 'i', 't', ')', ' ', 'm', 'o', 'd', 'e', 'l', ' ', 'i', 's', ' ', 't', 'h', 'a', 't', ' ', '$', 'J', '_', '{', 'c', 'v', '}', '$', ' ', 'i', 's', ' ', 'h', 'i', 'g', 'h', ' ', 'b', 'u', 't', ' ', '$', 'J', '_', '{', 't', 'r', 'a', 'i', 'n', '}', '$', ' ', 'i', 's', ' ', 'l', 'o', 'w', '.', ' ', '#', '#', '#', '#', ' ', 'S', 'l', 'i', 'd', 'e', ' ', '!', 'S', 'c', 'r', 'e', 'e', 'n', 's', 'h', 'o', 't', ' ', '2', '0', '2', '3', '-', '1', '0', '-', '1', '7', ' ', 'a', 't', ' ', '1', '.', '2', '6', '.', '0', '0', ' ', 'P', 'M', '.', 'p', 'n', 'g', ' ', ' ', '#', '#', '#', ' ', 'U', 'n', 'd', 'e', 'r', 's', 't', 'a', 'n', 'd', 'i', 'n', 'g', ' ', 'B', 'i', 'a', 's', ' ', 'a', 'n', 'd', ' ', 'V', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', 'T', 'a', 'k', 'e', ' ', 'd', 'e', 'g', 'r', 'e', 'e', ' ', 'o', 'f', ' ', 'p', 'o', 'l', 'y', 'n', 'o', 'm', 'i', 'a', 'l', ' ', '$', 'd', '$', ' ', 'A', 's', ' ', '$', 'd', '$', ' ', 'i', 'n', 'c', 'r', 'e', 'a', 's', 'e', 's', ',', ' ', 't', 'h', 'a', 't', ' ', 'i', 's', ' ', 't', 'o', ' ', 's', 'a', 'y', ' ', 'w', 'e', ' ', 'a', 'd', 'd', ' ', 'd', 'e', 'g', 'r', 'e', 'e', 's', ' ', 't', 'o', ' ', 't', 'h', 'e', ' ', 'p', 'o', 'l', 'y', 'n', 'o', 'm', 'i', 'a', 'l', ' ', '$', 'J', '_', '{', 't', 'r', 'a', 'i', 'n', '}', '(', '{', '\\\\', 'v', 'e', 'c', '{', 'w', '}', ',', 'b', '}', ')', '$', ' ', ' ', 'w', 'i', 'l', 'l', ' ', 'd', 'e', 'c', 'r', 'e', 'a', 's', 'e', '.', ' ', '$', 'J', '_', '{', 'c', 'v', '}', '(', '\\\\', 'v', 'e', 'c', '{', 'w', '}', ',', 'b', ')', '$', ' ', 'd', 'i', 'p', 's', ' ', 't', 'o', 'w', 'a', 'r', 'd', 's', ' ', 't', 'h', 'e', ' ', 'm', 'i', 'd', 'd', 'l', 'e', ',', ' ', 's', 'h', 'o', 'w', 'i', 'n', 'g', ' ', 't', 'h', 'e', ' ', 'i', 'd', 'e', 'a', 'l', ' ', 'd', 'e', 'g', 'r', 'e', 'e', ' ', 'f', 'o', 'r', ' ', 't', 'h', 'e', ' ', 'p', 'o', 'l', 'y', 'n', 'o', 'm', 'i', 'a', 'l', '.', ' ', ' ', '#', '#', '#', '#', ' ', 'S', 'l', 'i', 'd', 'e', ' ', '!', 'S', 'c', 'r', 'e', 'e', 'n', 's', 'h', 'o', 't', ' ', '2', '0', '2', '3', '-', '1', '0', '-', '1', '7', ' ', 'a', 't', ' ', '1', '.', '2', '9', '.', '2', '1', ' ', 'P', 'M', '.', 'p', 'n', 'g', ' ', ' ', '#', '#', '#', ' ', 'T', 'e', 'c', 'h', 'n', 'i', 'q', 'u', 'e', ' ', 'f', 'o', 'r', ' ', 'D', 'i', 'a', 'g', 'n', 'o', 's', 'i', 'n', 'g', ' ', 'B', 'i', 'a', 's', ' ', 'a', 'n', 'd', ' ', 'V', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', 'H', 'o', 'w', ' ', 'd', 'o', ' ', 'y', 'o', 'u', ' ', 't', 'e', 'l', 'l', ' ', 'i', 'f', ' ', 'y', 'o', 'u', 'r', ' ', 'a', 'l', 'g', 'o', 'r', 'i', 't', 'h', 'm', ' ', 'h', 'a', 's', ' ', 'a', ' ', 'h', 'i', 'g', 'h', ' ', 'b', 'i', 'a', 's', ' ', 'o', 'r', ' ', 'v', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', 'p', 'r', 'o', 'b', 'l', 'e', 'm', '?', ' ', ' ', 'O', 'n', 'e', ' ', 'k', 'e', 'y', ' ', 't', 'a', 'k', 'e', 'a', 'w', 'a', 'y', ' ', 'i', 's', ' ', 't', 'h', 'a', 't', ' ', 'h', 'i', 'g', 'h', ' ', 'b', 'i', 'a', 's', ' ', 'm', 'e', 'a', 'n', 's', ' ', 'i', 't', ' ', 'i', 's', ' ', 'n', 'o', 't', ' ', 'e', 'v', 'e', 'n', ' ', 'd', 'o', 'i', 'n', 'g', ' ', 'w', 'e', 'l', 'l', ' ', 'o', 'n', ' ', 't', 'h', 'e', ' ', 't', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ' ', 's', 'e', 't', ' ', 'a', 'n', 'd', ' ', 'h', 'i', 'g', 'h', ' ', 'v', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', 'm', 'e', 'a', 'n', 's', ' ', 'i', 't', ' ', 'd', 'o', 'e', 's', ' ', 'm', 'u', 'c', 'h', ' ', 'w', 'o', 'r', 's', 'e', ' ', 'o', 'n', ' ', 't', 'h', 'e', ' ', 'c', 'r', 'o', 's', 's', ' ', 'v', 'a', 'l', 'i', 'd', 'a', 't', 'i', 'o', 'n', ' ', 's', 'e', 't', ' ', 'a', 'n', 'd', ' ', 't', 'h', 'e', ' ', 't', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ' ', 's', 'e', 't', '.', ' ', '#', '#', '#', '#', ' ', 'H', 'i', 'g', 'h', ' ', 'B', 'i', 'a', 's', ' ', '(', 'u', 'n', 'd', 'e', 'r', '-', 'f', 'i', 't', ')', ' ', 'I', 'n', ' ', 'a', 'n', ' ', 'u', 'n', 'd', 'e', 'r', '-', 'f', 'i', 't', ' ', 'm', 'o', 'd', 'e', 'l', ' ', '$', 'J', '_', '{', 't', 'r', 'a', 'i', 'n', '}', '$', ' ', 'w', 'i', 'l', 'l', ' ', 'b', 'e', ' ', 'h', 'i', 'g', 'h', ' ', 'a', 'n', 'd', ' ', '$', 'J', '_', '{', 't', 'r', 'a', 'i', 'n', '}', ' ', '\\\\', 'a', 'p', 'p', 'r', 'o', 'x', ' ', 'J', '_', '{', 'c', 'v', '}', '$', ' ', ' ', '#', '#', '#', '#', ' ', 'H', 'i', 'g', 'h', ' ', 'V', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', '(', 'o', 'v', 'e', 'r', 'f', 'i', 't', ')', ' ', 'I', 'n', ' ', 'a', 'n', ' ', 'o', 'v', 'e', 'r', '-', 'f', 'i', 't', ' ', 'm', 'o', 'd', 'e', 'l', ' ', '$', 'J', '_', '{', 'c', 'v', '}', ' ', '\\\\', 'g', 'g', ' ', 'J', '_', '{', 't', 'r', 'a', 'i', 'n', '}', '$', ' ', 'a', 'n', 'd', ' ', '$', 'J', '_', '{', 't', 'r', 'a', 'i', 'n', '}', '$', ' ', 'w', 'i', 'l', 'l', ' ', 'b', 'e', ' ', 'l', 'o', 'w', '.', ' ', '#', '#', '#', '#', ' ', 'H', 'i', 'g', 'h', ' ', 'B', 'i', 'a', 's', ' ', 'a', 'n', 'd', ' ', 'H', 'i', 'g', 'h', ' ', 'V', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', 'I', 'n', ' ', 'a', ' ', 'm', 'o', 'd', 'e', 'l', ' ', 'w', 'i', 't', 'h', ' ', 'b', 'o', 't', 'h', ' ', 'h', 'i', 'g', 'h', ' ', 'b', 'i', 'a', 's', ' ', 'a', 'n', 'd', ' ', 'h', 'i', 'g', 'h', ' ', 'v', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', '$', 'J', '_', '{', 't', 'r', 'a', 'i', 'n', '}', '$', ' ', 'w', 'i', 'l', 'l', ' ', 'b', 'e', ' ', 'h', 'i', 'g', 'h', ' ', 'a', 'n', 'd', ' ', '$', 'J', '_', '{', 'c', 'v', '}', ' ', '\\\\', 'g', 'g', ' ', 'J', '_', '{', 't', 'r', 'a', 'i', 'n', '}', '$', ' ', ' ', 'T', 'h', 'i', 's', ' ', 'i', 's', ' ', 'r', 'a', 'r', 'e', ',', ' ', 'a', 'n', 'd', ' ', 'd', 'o', 'e', 's', 'n', \"'\", 't', ' ', 'r', 'e', 'a', 'l', 'l', 'y', ' ', 'h', 'a', 'p', 'p', 'e', 'n', ' ', 'f', 'o', 'r', ' ', 'l', 'i', 'n', 'e', 'a', 'r', ' ', 'm', 'o', 'd', 'e', 'l', 's', ' ', 'w', 'i', 't', 'h', ' ', 'o', 'n', 'e', ' ', '$', 'd', '$', ',', ' ', 'b', 'u', 't', ' ', 'i', 't', ' ', 'd', 'o', 'e', 's', ' ', 'h', 'a', 'p', 'p', 'e', 'n', '.', ' ', 'T', 'h', 'i', 's', ' ', 'i', 's', ' ', 'w', 'h', 'e', 'n', ' ', 'i', 't', ' ', 'o', 'v', 'e', 'r', 'f', 'i', 't', 's', ' ', 'f', 'o', 'r', ' ', 's', 'o', 'm', 'e', ' ', 'p', 'a', 'r', 't', ' ', 'o', 'f', ' ', 't', 'h', 'e', ' ', 'i', 'n', 'p', 'u', 't', ' ', 'a', 'n', 'd', ' ', 'u', 'n', 'd', 'e', 'r', '-', 'f', 'i', 't', 's', ' ', 'f', 'o', 'r', ' ', 'a', 'n', 'o', 't', 'h', 'e', 'r', ' ', 'p', 'a', 'r', 't', ' ', 'o', 'f', ' ', 't', 'h', 'e', ' ', 'i', 'n', 'p', 'u', 't', '.', ' ', '#', '#', '#', '#', ' ', 'S', 'l', 'i', 'd', 'e', ' ', '!', 'S', 'c', 'r', 'e', 'e', 'n', 's', 'h', 'o', 't', ' ', '2', '0', '2', '3', '-', '1', '0', '-', '1', '7', ' ', 'a', 't', ' ', '1', '.', '3', '8', '.', '5', '1', ' ', 'P', 'M', '.', 'p', 'n', 'g', ' ', ' ', '#', '#', ' ', 'R', 'e', 'g', 'u', 'l', 'a', 'r', 'i', 'z', 'a', 't', 'i', 'o', 'n', ' ', 'a', 'n', 'd', ' ', 'b', 'i', 'a', 's', '/', 'v', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', '#', '#', '#', ' ', 'L', 'i', 'n', 'e', 'a', 'r', ' ', 'R', 'e', 'g', 'r', 'e', 's', 's', 'i', 'o', 'n', ' ', 'w', 'i', 't', 'h', ' ', 'R', 'e', 'g', 'u', 'l', 'a', 'r', 'i', 'z', 'a', 't', 'i', 'o', 'n', ' ', 'R', 'e', 'g', 'u', 'l', 'a', 'r', 'i', 'z', 'e', 'd', ' ', 'L', 'i', 'n', 'e', 'a', 'r', ' ', 'R', 'e', 'g', 'r', 'e', 's', 's', 'i', 'o', 'n', ' ', 'm', 'o', 'd', 'e', 'l', ':', ' ', 'L', 'a', 'm', 'b', 'd', 'a', ' ', '$', '\\\\', 'l', 'a', 'm', 'b', 'd', 'a', '$', ' ', 'i', 's', ' ', 't', 'h', 'e', ' ', 'r', 'e', 'g', 'u', 'l', 'a', 'r', 'i', 'z', 'a', 't', 'i', 'o', 'n', ' ', 'p', 'a', 'r', 'a', 'm', 'e', 't', 'e', 'r', ' ', '(', 'B', 'i', 'g', ' ', '#', 'm', 'e', 'r', 'g', 'e', ' ', 'a', 'n', 'd', ' ', 'c', 'l', 'e', 'a', 'n', 'u', 'p', ' ', 'n', 'e', 'e', 'd', 'e', 'd', ' ', 'h', 'e', 'r', 'e', ' ', 'i', 'n', ' ', 't', 'h', 'e', ' ', 'r', 'e', 'g', 'u', 'l', 'a', 'r', 'i', 'z', 'a', 't', 'i', 'o', 'n', ',', ' ', 'R', 'e', 'g', 'u', 'l', 'a', 'r', 'i', 'z', 'e', 'd', ' ', 'L', 'i', 'n', 'e', 'a', 'r', ' ', 'R', 'e', 'g', 'r', 'e', 's', 's', 'i', 'o', 'n', ',', ' ', 'c', 'o', 's', 't', ',', ' ', 'C', 'o', 's', 't', ' ', 'a', 'n', 'd', ' ', 'L', 'o', 's', 's', ',', ' ', 'l', 'o', 's', 's', ',', ' ', 'L', 'o', 's', 's', ' ', 'F', 'u', 'n', 'c', 't', 'i', 'o', 'n', ',', ' ', 'e', 't', 'c', '.', ' ', 'A', 'l', 'i', 'a', 's', 'e', 's', ' ', 'a', 'r', 'e', ' ', 'n', 'e', 'e', 'd', 'e', 'd', ' ', 'i', 'n', ' ', 'm', 'a', 'n', 'y', ' ', 'o', 'f', ' ', 't', 'h', 'e', 's', 'e', ')', ' ', ' ', 'T', 'a', 'k', 'e', ' ', 't', 'h', 'i', 's', ' ', 'm', 'o', 'd', 'e', 'l', ':', ' ', '$', '$', 'f', '_', '{', '\\\\', 'v', 'e', 'c', '{', 'w', '}', ',', 'b', '}', '(', 'x', ')', '=', 'w', '_', '1', '+', 'w', '_', '2', 'x', '^', '2', ' ', '+', ' ', 'w', '_', '4', 'x', '^', '4', '+', 'b', '$', '$', ' ', 'W', 'i', 't', 'h', ' ', 't', 'h', 'i', 's', ' ', 'r', 'e', 'g', 'u', 'l', 'a', 'r', 'i', 'z', 'a', 't', 'i', 'o', 'n', '|', 'r', 'e', 'g', 'u', 'l', 'a', 'r', 'i', 'z', 'e', 'd', ' ', 'C', 'o', 's', 't', ' ', 'a', 'n', 'd', ' ', 'L', 'o', 's', 's', ' ', '$', '$', 'J', '(', '\\\\', 'v', 'e', 'c', '{', 'w', '}', ',', 'b', ')', '=', '\\\\', 'f', 'r', 'a', 'c', '{', '1', '}', '{', '2', 'm', '}', ' ', '\\\\', 's', 'u', 'm', '_', '{', 'i', '=', '1', '}', '^', '{', 'm', '}', '(', 'f', '_', '{', '\\\\', 'v', 'e', 'c', '{', 'w', '}', ',', '{', 'b', '}', '}', '(', '\\\\', 'v', 'e', 'c', '{', 'x', '}', '^', '{', '(', 'i', ')', '}', '-', 'y', '^', '{', '(', 'i', ')', '}', ')', '^', '2', '+', '\\\\', 'f', 'r', 'a', 'c', '{', '\\\\', 'l', 'a', 'm', 'b', 'd', 'a', '}', '{', '2', 'm', '}', ' ', '\\\\', 's', 'u', 'm', '_', '{', 'j', '=', '1', '}', '^', 'n', 'w', '_', 'j', '^', '2', '$', '$', ' ', ' ', 'I', 'f', ' ', 'l', 'a', 'm', 'b', 'd', 'a', ' ', 'i', 's', ' ', 'l', 'a', 'r', 'g', 'e', ',', ' ', 's', 'a', 'y', ' ', '$', '\\\\', 'l', 'a', 'm', 'b', 'd', 'a', ' ', '=', ' ', '1', '0', ',', '0', '0', '0', '$', ' ', 't', 'h', 'a', 'n', ' ', '$', 'w', '_', '1', ' ', '\\\\', 'a', 'p', 'p', 'r', 'o', 'x', ' ', '0', ',', ' ', 'W', '_', '2', ' ', '\\\\', 'a', 'p', 'p', 'r', 'o', 'x', ' ', '0', '$', ' ', 'a', 'n', 'd', ' ', '$', 'f', '_', '{', '\\\\', 'v', 'e', 'c', '{', 'x', '}', ',', 'b', '}', '\\\\', 'v', 'e', 'c', '{', 'x', '}', ' ', '\\\\', 'a', 'p', 'p', 'r', 'o', 'x', ' ', 'b', '$', ',', ' ', 't', 'h', 'u', 's', ' ', 't', 'h', 'e', ' ', 'm', 'o', 'd', 'e', 'l', ' ', 'c', 'r', 'e', 'a', 't', 'e', 's', ' ', 'a', ' ', 'f', 'l', 'a', 't', ' ', 'l', 'i', 'n', 'e', '.', ' ', 'T', 'h', 'i', 's', ' ', 'i', 's', ' ', 'u', 'n', 'd', 'e', 'r', 'f', 'i', 't', ',', ' ', '$', 'J', '_', '{', 't', 'r', 'a', 'i', 'n', '}', '(', ')', ' ', ' ', 'O', 'n', ' ', 't', 'h', 'e', ' ', 'o', 't', 'h', 'e', 'r', ' ', 'h', 'a', 'n', 'd', ',', ' ', 'i', 'f', ' ', 'w', 'e', ' ', 's', 'e', 't', ' ', '$', '\\\\', 'l', 'a', 'm', 'b', 'd', 'a', '=', '0', '$', ',', ' ', 't', 'h', 'e', 'n', ' ', 'w', 'e', ' ', 'h', 'a', 'v', 'e', ' ', 'a', ' ', 'f', 'o', 'r', 't', 'h', ' ', 'o', 'r', 'd', 'e', 'r', ' ', 'p', 'o', 'l', 'y', 'n', 'o', 'm', 'i', 'a', 'l', ' ', 'w', 'i', 't', 'h', ' ', 'n', 'o', ' ', 'r', 'e', 'g', 'u', 'l', 'a', 'r', 'i', 'z', 'a', 't', 'i', 'o', 'n', '.', ' ', 'W', 'e', ' ', 'e', 'n', 'd', ' ', 'u', 'p', ' ', 'w', 'i', 't', 'h', ' ', 'a', ' ', 'v', 'e', 'r', 'y', ' ', 'o', 'v', 'e', 'r', 'f', 'i', 't', ' ', 'c', 'u', 'r', 'v', 'e', '.', ' ', ' ', 'S', 'o', ',', ' ', 'h', 'o', 'w', ' ', 'd', 'o', ' ', 'w', 'e', ' ', 'f', 'i', 'n', 'd', ' ', 'a', ' ', 'g', 'o', 'o', 'd', ' ', 'v', 'a', 'l', 'u', 'e', ' ', 'f', 'o', 'r', ' ', '$', '\\\\', 'l', 'a', 'm', 'b', 'd', 'a', '$', '?', ' ', '#', '#', '#', '#', '#', ' ', 'S', 'l', 'i', 'd', 'e', ' ', '!', 'S', 'c', 'r', 'e', 'e', 'n', 's', 'h', 'o', 't', ' ', '2', '0', '2', '3', '-', '1', '0', '-', '1', '7', ' ', 'a', 't', ' ', '3', '.', '5', '6', '.', '5', '3', ' ', 'P', 'M', '.', 'p', 'n', 'g', ' ', '#', '#', '#', ' ', 'C', 'h', 'o', 'o', 's', 'i', 'n', 'g', ' ', 't', 'h', 'e', ' ', 'R', 'e', 'g', 'u', 'l', 'a', 'r', 'i', 'z', 'a', 't', 'i', 'o', 'n', ' ', 'p', 'a', 'r', 'a', 'm', 'e', 't', 'e', 'r', ' ', '$', '\\\\', 'l', 'a', 'm', 'b', 'd', 'a', '$', ' ', ' ', ' ', 'T', 'h', 'i', 's', ' ', 'w', 'i', 'l', 'l', ' ', 'b', 'e', ' ', 's', 'i', 'm', 'i', 'l', 'a', 'r', ' ', 't', 'o', ' ', 'c', 'h', 'o', 'o', 's', 'i', 'n', 'g', ' ', '$', 'd', '$', ' ', 'w', 'i', 't', 'h', ' ', 'c', 'r', 'o', 's', 's', ' ', 'v', 'a', 'l', 'i', 'd', 'a', 't', 'i', 'o', 'n', ' ', 'T', 'r', 'y', ' ', 'm', 'u', 'l', 't', 'i', 'p', 'l', 'e', ' ', 'v', 'a', 'l', 'u', 'e', 's', ' ', 'f', 'o', 'r', ' ', '$', '\\\\', 'l', 'a', 'm', 'b', 'd', 'a', '$', ' ', 'a', 'n', 'd', ' ', 't', 'h', 'e', 'n', ' ', 'c', 'h', 'o', 'o', 's', 'e', ' ', 't', 'h', 'e', ' ', 'o', 'p', 't', 'i', 'o', 'n', ' ', 'w', 'i', 't', 'h', ' ', 't', 'h', 'e', ' ', 'l', 'o', 'w', 'e', 's', 't', ' ', 'c', 'o', 's', 't', '.', ' ', '#', '#', '#', ' ', 'S', 'l', 'i', 'd', 'e', ' ', '!', 'S', 'c', 'r', 'e', 'e', 'n', 's', 'h', 'o', 't', ' ', '2', '0', '2', '3', '-', '1', '0', '-', '1', '7', ' ', 'a', 't', ' ', '4', '.', '1', '5', '.', '2', '7', ' ', 'P', 'M', '.', 'p', 'n', 'g', ' ', '#', '#', '#', ' ', 'B', 'i', 'a', 's', ' ', 'a', 'n', 'd', ' ', 'V', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', 'a', 's', ' ', 'a', ' ', 'F', 'u', 'n', 'c', 't', 'i', 'o', 'n', ' ', 'o', 'f', ' ', 'r', 'e', 'g', 'u', 'l', 'a', 'r', 'i', 'z', 'a', 't', 'i', 'o', 'n', ' ', 'p', 'a', 'r', 'a', 'm', 'e', 't', 'e', 'r', ' ', '$', '\\\\', 'l', 'a', 'm', 'b', 'd', 'a', '$', ' ', ' ', 'C', 'r', 'o', 's', 's', ' ', 'V', 'a', 'l', 'i', 'd', 'a', 't', 'i', 'o', 'n', ' ', 't', 'r', 'i', 'e', 's', ' ', 'o', 'u', 't', ' ', 'm', 'a', 'n', 'y', ' ', 'v', 'e', 'r', 's', 'i', 'o', 'n', 's', ' ', 'o', 'f', ' ', '$', '\\\\', 'l', 'a', 'm', 'b', 'd', 'a', '$', ' ', 'a', 'n', 'd', ' ', 't', 'h', 'e', 'n', ' ', 'c', 'h', 'o', 'o', 's', 'e', 's', ' ', 't', 'h', 'e', ' ', 'o', 'n', 'e', ' ', 'w', 'i', 't', 'h', ' ', 't', 'h', 'e', ' ', 'l', 'o', 'w', 'e', 's', 't', ' ', 'c', 'o', 's', 't', '.', ' ', 'C', 'r', 'o', 's', 's', ' ', 'v', 'a', 'l', 'i', 'd', 'a', 't', 'i', 'o', 'n', ' ', 'w', 'i', 'l', 'l', ' ', 'h', 'e', 'l', 'p', ' ', 'f', 'i', 'n', 'd', ' ', 'u', 's', ' ', 'a', ' ', 'a', ' ', 'g', 'o', 'o', 'd', ' ', 'v', 'a', 'l', 'u', 'e', ' ', 'o', 'f', ' ', '$', 'd', '$', ' ', 'a', 's', ' ', 'w', 'e', 'l', 'l', ' ', 'a', 's', ' ', '$', '\\\\', 'l', 'a', 'm', 'b', 'd', 'a', '$', ' ', ' ', '#', '#', '#', '#', ' ', 'S', 'l', 'i', 'd', 'e', ' ', '!', 'S', 'c', 'r', 'e', 'e', 'n', 's', 'h', 'o', 't', ' ', '2', '0', '2', '3', '-', '1', '0', '-', '1', '7', ' ', 'a', 't', ' ', '4', '.', '1', '4', '.', '0', '7', ' ', 'P', 'M', '.', 'p', 'n', 'g', ' ', ' ', ' ', '#', '#', ' ', 'E', 's', 't', 'a', 'b', 'l', 'i', 's', 'h', 'i', 'n', 'g', ' ', 'A', ' ', 'B', 'a', 's', 'e', 'l', 'i', 'n', 'e', ' ', 'L', 'e', 'v', 'e', 'l', ' ', 'o', 'f', ' ', 'P', 'e', 'r', 'f', 'o', 'r', 'm', 'a', 'n', 'c', 'e', ' ', '#', '#', '#', ' ', 'S', 'p', 'e', 'e', 'c', 'h', ' ', 'R', 'e', 'c', 'o', 'g', 'n', 'i', 't', 'i', 'o', 'n', ' ', 'e', 'x', 'a', 'm', 'p', 'l', 'e', ' ', 'J', 'o', 'b', ' ', 'i', 's', ' ', 't', 'o', ' ', 't', 'a', 'k', 'e', ' ', 'i', 'n', ' ', 'a', 'u', 'd', 'i', 'o', ' ', 'a', 'n', 'd', ' ', 'o', 'u', 't', 'p', 'u', 't', ' ', 't', 'h', 'e', ' ', 't', 'e', 'x', 't', ' ', 'o', 'f', ' ', 'w', 'h', 'a', 't', ' ', 'a', ' ', 'p', 'e', 'r', 's', 'o', 'n', ' ', 'i', 's', ' ', 's', 'a', 'y', 'i', 'n', 'g', ' ', ' ', 'T', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ' ', 'e', 'r', 'r', 'o', 'r', ' ', '$', 'J', '_', '{', 't', 'r', 'a', 'i', 'n', '}', '$', ' ', 'i', 's', ' ', 'p', 'e', 'r', 'c', 'e', 'n', 't', 'a', 'g', 'e', ' ', 'o', 'f', ' ', 'a', 'u', 'd', 'i', 'o', ' ', 'c', 'l', 'i', 'p', 's', ' ', 't', 'h', 'a', 't', ' ', 't', 'h', 'e', ' ', 'p', 'r', 'o', 'g', 'r', 'a', 'm', ' ', 'd', 'o', 'e', 's', ' ', 'n', 'o', 't', ' ', 't', 'r', 'a', 'n', 's', 'c', 'r', 'i', 'b', 'e', ' ', 'c', 'o', 'r', 'r', 'e', 'c', 't', 'l', 'y', ' ', 'i', 'n', ' ', 'i', 't', \"'\", 's', ' ', 'e', 'n', 't', 'i', 'r', 'e', 't', 'y', '.', ' ', 'L', 'e', 't', 's', ' ', 's', 'a', 'y', ':', ' ', '\\t', 'H', 'u', 'm', 'a', 'n', ' ', 'L', 'e', 'v', 'e', 'l', ' ', 'P', 'e', 'r', 'f', 'o', 'r', 'm', 'a', 'n', 'c', 'e', ':', ' ', '1', '0', '.', '6', ' ', '\\t', '$', 'J', '_', '{', 't', 'r', 'a', 'i', 'n', '}', '$', ':', ' ', '1', '0', '.', '8', '%', ' ', '\\t', '$', 'J', '_', '{', 'c', 'v', '}', '$', ':', ' ', '1', '4', '.', '8', '%', ' ', ' ', 'W', 'h', 'y', ' ', 'i', 's', ' ', 'h', 'u', 'm', 'a', 'n', ' ', 'l', 'e', 'v', 'e', 'l', ' ', 'e', 'r', 'r', 'o', 'r', ' ', 's', 'o', ' ', 'h', 'i', 'g', 'h', '?', ' ', 'T', 'h', 'e', 'r', 'e', ' ', 'i', 's', ' ', 'l', 'o', 't', 's', ' ', 'o', 'f', ' ', 'n', 'o', 'i', 's', 'e', ' ', 'i', 'n', ' ', 't', 'h', 'e', ' ', 'a', 'u', 'd', 'i', 'o', '.', ' ', 'I', 't', ' ', 's', 'e', 'e', 'm', 's', ' ', 'u', 'n', 'f', 'a', 'i', 'r', ' ', 't', 'o', ' ', 'e', 'x', 'p', 'e', 'c', 't', ' ', 'a', ' ', 'l', 'e', 'a', 'r', 'n', 'i', 'n', 'g', ' ', 'a', 'l', 'g', 'o', 'r', 'i', 't', 'h', 'm', ' ', 't', 'o', ' ', 'd', 'o', ' ', 'm', 'u', 'c', 'h', ' ', 'b', 'e', 't', 't', 'e', 'r', '.', ' ', 'I', 't', ' ', 'i', 's', ' ', 't', 'h', 'u', 's', ' ', 'i', 's', ' ', 'm', 'o', 'r', 'e', ' ', 'u', 's', 'e', 'f', 'u', 'l', ' ', 't', 'o', ' ', 'm', 'e', 'a', 's', 'u', 'r', 'e', ' ', 't', 'h', 'e', ' ', 't', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ' ', 'e', 'r', 'r', 'o', 'r', ' ', 'a', 'g', 'a', 'i', 'n', 's', 't', ' ', 't', 'h', 'e', ' ', 'h', 'u', 'm', 'a', 'n', ' ', 'e', 'r', 'r', 'o', 'r', '.', ' ', 'S', 'o', ',', ' ', 'l', 'o', 'o', 'k', 'i', 'n', 'g', ' ', 'a', 't', ' ', 't', 'h', 'e', 's', 'e', ' ', 'r', 'e', 's', 'u', 'l', 't', 's', ',', ' ', '$', 'J', '_', '{', 't', 'r', 'a', 'i', 'n', '}', '$', ' ', 'i', 's', ' ', 'o', 'n', 'l', 'y', ' ', '0', '.', '2', '$', ' ', 'h', 'i', 'g', 'h', 'e', 'r', ' ', 't', 'h', 'a', 'n', ' ', 't', 'h', 'e', ' ', 'h', 'u', 'm', 'a', 'n', ' ', 'l', 'e', 'v', 'e', 'l', ' ', 'p', 'e', 'r', 'f', 'o', 'r', 'm', 'a', 'n', 'c', 'e', ' ', 'w', 'h', 'e', 'r', 'e', 'a', 's', ' ', '$', 'J', '_', '{', 'c', 'v', '}', '$', ' ', 'i', 's', ' ', 'a', ' ', 'f', 'u', 'l', 'l', ' ', '4', '.', '2', '%', ' ', 'h', 'i', 'g', 'h', 'e', 'r', '.', ' ', 'W', 'e', ' ', 'c', 'a', 'n', ' ', 't', 'h', 'u', 's', ' ', 'c', 'o', 'n', 'c', 'l', 'u', 'd', 'e', ' ', 't', 'h', 'a', 't', ' ', 't', 'h', 'i', 's', ' ', 'a', 'l', 'g', 'o', 'r', 'i', 't', 'h', 'm', ' ', 'h', 'a', 's', ' ', 'm', 'o', 'r', 'e', ' ', 'o', 'f', ' ', 'a', ' ', 'v', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', 'p', 'r', 'o', 'b', 'l', 'e', 'm', ' ', 't', 'h', 'a', 'n', ' ', 'a', ' ', 'b', 'i', 'a', 's', ' ', 'p', 'r', 'o', 'b', 'l', 'e', 'm', '.', ' ', '#', '#', '#', ' ', 'E', 's', 't', 'a', 'b', 'l', 'i', 's', 'h', 'i', 'n', 'g', ' ', 'a', ' ', 'b', 'a', 's', 'e', 'l', 'i', 'n', 'e', ' ', 'l', 'e', 'v', 'e', 'l', ' ', 'o', 'f', ' ', 'p', 'e', 'r', 'f', 'o', 'r', 'm', 'a', 'n', 'c', 'e', ' ', 'W', 'h', 'a', 't', ' ', 'i', 's', ' ', 't', 'h', 'e', ' ', 'l', 'e', 'v', 'e', 'l', ' ', 'o', 'f', ' ', 'e', 'r', 'r', 'o', 'r', ' ', 'y', 'o', 'u', ' ', 'c', 'a', 'n', ' ', 'r', 'e', 'a', 's', 'o', 'n', 'a', 'b', 'l', 'y', ' ', 'h', 'o', 'p', 'e', ' ', 't', 'o', ' ', 'g', 'e', 't', ' ', 't', 'o', '?', ' ', '-', ' ', 'H', 'u', 'm', 'a', 'n', ' ', 'l', 'e', 'v', 'e', 'l', ' ', 'p', 'e', 'r', 'f', 'o', 'r', 'm', 'a', 'n', 'c', 'e', ' ', '-', ' ', 'C', 'o', 'm', 'p', 'e', 't', 'i', 'n', 'g', ' ', 'a', 'l', 'g', 'o', 'r', 'i', 't', 'h', 'm', 's', ' ', 'p', 'e', 'r', 'f', 'o', 'r', 'm', 'a', 'n', 'c', 'e', ' ', '-', ' ', 'G', 'u', 'e', 's', 's', ' ', 'b', 'a', 's', 'e', 'd', ' ', 'o', 'n', ' ', 'e', 'x', 'p', 'e', 'r', 'i', 'e', 'n', 'c', 'e', ' ', '#', '#', '#', ' ', 'B', 'i', 'a', 's', ' ', 'V', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', 'E', 'x', 'a', 'm', 'p', 'l', 'e', 's', ' ', 'G', 'a', 'p', ' ', 'b', 'e', 't', 'w', 'e', 'e', 'n', ' ', 'b', 'a', 's', 'e', 'l', 'i', 'n', 'e', ' ', 'a', 'n', 'd', ' ', 't', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ' ', 'e', 'r', 'r', 'o', 'r', ' ', 's', 'h', 'o', 'w', 's', ' ', 'h', 'i', 'g', 'h', ' ', 'b', 'i', 'a', 's', '.', ' ', 'A', ' ', 'g', 'a', 'p', ' ', 'b', 'e', 't', 'w', 'e', 'e', 'n', ' ', 't', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ' ', 'e', 'r', 'r', 'o', 'r', ' ', 'a', 'n', 'd', ' ', 'c', 'r', 'o', 's', 's', ' ', 'v', 'a', 'l', 'i', 'd', 'a', 't', 'i', 'o', 'n', ' ', 'e', 'r', 'r', 'o', 'r', ' ', 's', 'h', 'o', 'w', 's', ' ', 'h', 'i', 'g', 'h', ' ', 'v', 'a', 'r', 'i', 'a', 'n', 'c', 'e', '.', ' ', ' ', 'I', 'f', ' ', 'y', 'o', 'u', 'r', ' ', 'g', 'o', 'a', 'l', ' ', 'i', 's', ' ', 'p', 'e', 'r', 'f', 'e', 'c', 't', 'i', 'o', 'n', ',', ' ', 't', 'h', 'e', ' ', 'b', 'a', 's', 'e', 'l', 'i', 'n', 'e', ' ', 'w', 'o', 'u', 'l', 'd', ' ', 'b', 'e', ' ', 'z', 'e', 'r', 'o', '.', ' ', 'B', 'u', 't', ' ', 'f', 'o', 'r', ' ', 'a', ' ', 'l', 'o', 't', ' ', 'o', 'f', ' ', 'r', 'e', 'a', 'l', ' ', 'w', 'o', 'r', 'l', 'd', ' ', 'e', 'x', 'a', 'm', 'p', 'l', 'e', 's', ',', ' ', 'l', 'i', 'k', 'e', ' ', 'a', 'u', 'd', 'i', 'o', ' ', 'r', 'e', 'c', 'o', 'g', 'n', 'i', 't', 'i', 'o', 'n', ',', ' ', 't', 'h', 'e', 'r', 'e', ' ', 'i', 's', ' ', 'a', ' ', 'l', 'o', 't', ' ', 'o', 'f', ' ', 'n', 'o', 'i', 's', 'e', ' ', 'i', 'n', ' ', 't', 'h', 'e', ' ', 'd', 'a', 't', 'a', ',', ' ', 's', 'o', ' ', 'y', 'o', 'u', ' ', 'n', 'e', 'e', 'd', ' ', 'a', ' ', 'h', 'i', 'g', 'h', 'e', 'r', ' ', 'b', 'a', 's', 'e', 'l', 'i', 'n', 'e', '.', ' ', ' ', 'I', 'f', ' ', 't', 'h', 'e', 'r', 'e', ' ', 'i', 's', ' ', 'a', ' ', 'g', 'a', 'p', ' ', 'b', 'e', 't', 'w', 'e', 'e', 'n', ' ', 'a', 'l', 'l', ' ', 't', 'h', 'r', 'e', 'e', ' ', 'i', 's', ' ', 'h', 'i', 'g', 'h', ' ', 'y', 'o', 'u', ' ', 'h', 'a', 'v', 'e', ' ', 'b', 'o', 't', 'h', ' ', 'h', 'i', 'g', 'h', ' ', 'b', 'i', 'a', 's', ' ', 'a', 'n', 'd', ' ', 'h', 'i', 'g', 'h', ' ', 'v', 'a', 'r', 'i', 'a', 'n', 'c', 'e', '.', ' ', '#', '#', '#', '#', ' ', 'S', 'l', 'i', 'd', 'e', ' ', '!', 'S', 'c', 'r', 'e', 'e', 'n', 's', 'h', 'o', 't', ' ', '2', '0', '2', '3', '-', '1', '0', '-', '1', '7', ' ', 'a', 't', ' ', '4', '.', '4', '2', '.', '4', '9', ' ', 'P', 'M', '.', 'p', 'n', 'g', ' ', ' ', ' ', '#', '#', ' ', 'L', 'e', 'a', 'r', 'n', 'i', 'n', 'g', ' ', 'C', 'u', 'r', 'v', 'e', 's', ' ', ' ', 'N', 'o', 't', 'e', 'd', ' ', 'i', 'n', ' ', 'L', 'e', 'a', 'r', 'n', 'i', 'n', 'g', ' ', 'C', 'u', 'r', 'v', 'e', 's', ' ', '#', '#', '#', ' ', 'O', 'v', 'e', 'r', 'v', 'i', 'e', 'w', ' ', 'L', 'e', 'a', 'r', 'n', 'i', 'n', 'g', ' ', 'c', 'u', 'r', 'v', 'e', 's', ' ', 'h', 'e', 'l', 'p', ' ', 'u', 'n', 'd', 'e', 'r', 's', 't', 'a', 'n', 'd', ' ', 'h', 'o', 'w', ' ', 'y', 'o', 'u', 'r', ' ', 'l', 'e', 'a', 'r', 'n', 'i', 'n', 'g', ' ', 'a', 'l', 'g', 'o', 'r', 'i', 't', 'h', 'm', ' ', 'i', 's', ' ', 'd', 'o', 'i', 'n', 'g', ' ', 'a', 's', ' ', 'a', ' ', 'f', 'u', 'n', 'c', 't', 'i', 'o', 'n', ' ', 'o', 'f', ' ', 't', 'h', 'e', ' ', 'a', 'm', 'o', 'u', 'n', 't', ' ', 'o', 'f', ' ', 'e', 'x', 'p', 'e', 'r', 'i', 'e', 'n', 'c', 'e', ' ', 'i', 't', ' ', 'h', 'a', 's', '.', ' ', 'E', 'x', 'p', 'e', 'r', 'i', 'e', 'n', 'c', 'e', ' ', 'b', 'e', 'i', 'n', 'g', ' ', 't', 'h', 'e', ' ', 'n', 'u', 'm', 'b', 'e', 'r', ' ', 'o', 'f', ' ', 't', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ' ', 'e', 'x', 'a', 'm', 'p', 'l', 'e', 's', ' ', 'i', 't', ' ', 'h', 'a', 's', '.', ' ', ' ', 'T', 'h', 'e', ' ', 'b', 'i', 'g', 'g', 'e', 'r', ' ', 't', 'h', 'e', ' ', 't', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ' ', 's', 'e', 't', ' ', 't', 'h', 'e', ' ', 'h', 'a', 'r', 'd', 'e', 'r', ' ', 'i', 't', ' ', 'i', 's', ' ', 't', 'o', ' ', 'f', 'i', 't', ' ', 'a', 'l', 'l', ' ', 'e', 'x', 'a', 'm', 'p', 'l', 'e', 's', ' ', 'p', 'e', 'r', 'f', 'e', 'c', 't', 'l', 'y', '.', ' ', 'T', 'h', 'u', 's', ';', ' ', 'a', 's', ' ', 't', 'h', 'e', ' ', 't', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ' ', 's', 'e', 't', ' ', 'i', 'n', 'c', 'r', 'e', 'a', 's', 'e', 's', ' ', 's', 'o', ' ', 'd', 'o', 'e', 's', ' ', 't', 'h', 'e', ' ', 't', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ' ', 'e', 'r', 'r', 'o', 'r', ' ', '$', 'J', '_', '{', 't', 'r', 'a', 'i', 'n', '}', '(', '\\\\', 'v', 'e', 'c', '{', 'w', '}', ',', 'b', ')', '$', ' ', ' ', ' ', 'P', 'l', 'o', 't', 't', 'i', 'n', 'g', ' ', 'a', ' ', 'l', 'e', 'a', 'r', 'n', 'i', 'n', 'g', ' ', 'c', 'u', 'r', 'v', 'e', ' ', 'b', 'y', ' ', 't', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ' ', 'd', 'i', 'f', 'f', 'e', 'r', 'e', 'n', 't', ' ', 'm', 'o', 'd', 'e', 'l', 's', ' ', 'b', 'a', 's', 'e', 'd', ' ', 'o', 'n', ' ', 'd', 'i', 'f', 'f', 'e', 'r', 'e', 'n', 't', ' ', 's', 'u', 'b', 's', 'e', 't', 's', ' ', 'o', 'f', ' ', 't', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ' ', 'd', 'a', 't', 'a', ' ', 'i', 's', ' ', 'c', 'o', 'm', 'p', 'u', 't', 'a', 't', 'i', 'o', 'n', 'a', 'l', 'l', 'y', ' ', 'e', 'x', 'p', 'e', 'n', 's', 'i', 'v', 'e', ',', ' ', 's', 'o', ' ', 'i', 'n', ' ', 'p', 'r', 'a', 'c', 't', 'i', 'c', 'e', ' ', 'i', 't', ' ', 'i', 's', 'n', \"'\", 't', ' ', 'd', 'o', 'n', 'e', ' ', 't', 'h', 'a', 't', ' ', 'o', 'f', 't', 'e', 'n', '.', ' ', 'B', 'u', 't', ',', ' ', 'i', 't', \"'\", 's', ' ', 'a', ' ', 'g', 'o', 'o', 'd', ' ', 'm', 'e', 'n', 't', 'a', 'l', ' ', 'v', 'i', 's', 'u', 'a', 'l', 'i', 'z', 'a', 't', 'i', 'o', 'n', '.', ' ', '#', '#', '#', '#', ' ', 'S', 'l', 'i', 'd', 'e', 's', ' ', '!', 'S', 'c', 'r', 'e', 'e', 'n', 's', 'h', 'o', 't', ' ', '2', '0', '2', '3', '-', '1', '0', '-', '1', '7', ' ', 'a', 't', ' ', '5', '.', '4', '0', '.', '0', '4', ' ', 'P', 'M', '.', 'p', 'n', 'g', ' ', ' ', ' ', ' ', '#', '#', '#', ' ', 'H', 'i', 'g', 'h', ' ', 'B', 'i', 'a', 's', ' ', 'E', 'x', 'a', 'm', 'p', 'l', 'e', ' ', 'I', 'f', ' ', 'a', ' ', 'l', 'e', 'a', 'r', 'n', 'i', 'n', 'g', ' ', 'a', 'l', 'g', 'o', 'r', 'i', 't', 'h', 'm', ' ', 's', 'u', 'f', 'f', 'e', 'r', 's', ' ', 'f', 'r', 'o', 'm', ' ', 'h', 'i', 'g', 'h', ' ', 'b', 'i', 'a', 's', ',', ' ', 'g', 'e', 't', 't', 'i', 'n', 'g', ' ', 'm', 'o', 'r', 'e', ' ', 't', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ' ', 'd', 'a', 't', 'a', ' ', 'w', 'i', 'l', 'l', ' ', 'n', 'o', 't', ' ', '(', 'b', 'y', ' ', 'i', 't', 's', 'e', 'l', 'f', ')', ' ', 'h', 'e', 'l', 'p', ' ', 't', 'h', 'a', 't', ' ', 'm', 'u', 'c', 'h', '.', ' ', '#', '#', '#', '#', ' ', 'S', 'l', 'i', 'd', 'e', ' ', 'f', 'o', 'r', ' ', 'H', 'i', 'g', 'h', ' ', 'B', 'i', 'a', 's', ' ', '!', 'S', 'c', 'r', 'e', 'e', 'n', 's', 'h', 'o', 't', ' ', '2', '0', '2', '3', '-', '1', '0', '-', '1', '7', ' ', 'a', 't', ' ', '5', '.', '4', '4', '.', '2', '9', ' ', 'P', 'M', '.', 'p', 'n', 'g', ' ', ' ', '#', '#', '#', ' ', 'H', 'i', 'g', 'h', ' ', 'V', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', 'E', 'x', 'a', 'm', 'p', 'l', 'e', ' ', 'I', 'f', ' ', 'a', ' ', 'l', 'e', 'a', 'r', 'n', 'i', 'n', 'g', ' ', 'a', 'l', 'g', 'o', 'r', 'i', 't', 'h', 'm', ' ', 's', 'u', 'f', 'f', 'e', 'r', 's', ' ', 'f', 'r', 'o', 'm', ' ', 'h', 'i', 'g', 'h', ' ', 'v', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ',', ' ', 'g', 'e', 't', 't', 'i', 'n', 'g', ' ', 'm', 'o', 'r', 'e', ' ', 't', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ' ', 'd', 'a', 't', 'a', ' ', 'i', 's', ' ', 'l', 'i', 'k', 'e', 'l', 'y', ' ', 't', 'o', ' ', 'h', 'e', 'l', 'p', '.', ' ', '#', '#', '#', '#', ' ', 'S', 'l', 'i', 'd', 'e', ' ', 'f', 'o', 'r', ' ', 'H', 'i', 'g', 'h', ' ', 'V', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', '!', 'S', 'c', 'r', 'e', 'e', 'n', 's', 'h', 'o', 't', ' ', '2', '0', '2', '3', '-', '1', '0', '-', '1', '7', ' ', 'a', 't', ' ', '5', '.', '4', '9', '.', '4', '5', ' ', 'P', 'M', '.', 'p', 'n', 'g', ' ', ' ', ' ', ' ', '#', '#', ' ', 'D', 'e', 'c', 'i', 'd', 'i', 'n', 'g', ' ', 'w', 'h', 'a', 't', ' ', 't', 'o', ' ', 't', 'r', 'y', ' ', 'n', 'e', 'x', 't', ' ', 'r', 'e', 'v', 'i', 's', 'i', 't', 'e', 'd', ' ', '#', '#', '#', ' ', 'E', 'x', 'a', 'm', 'p', 'l', 'e', 's', ' ', 'w', 'h', 'e', 'n', ' ', 'D', 'e', 'b', 'u', 'g', 'g', 'i', 'n', 'g', ' ', 'a', 'n', ' ', 'A', 'l', 'g', 'o', 'r', 'i', 't', 'h', 'm', ':', ' ', 'G', 'e', 't', ' ', 'm', 'o', 'r', 'e', ' ', 't', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ' ', 'e', 'x', 'a', 'm', 'p', 'l', 'e', 's', ':', ' ', 'f', 'i', 'x', 'e', 's', ' ', 'h', 'i', 'g', 'h', ' ', 'v', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', 'T', 'r', 'y', ' ', 's', 'm', 'a', 'l', 'l', 'e', 'r', ' ', 's', 'e', 't', ' ', 'o', 'f', ' ', 'f', 'e', 'a', 't', 'u', 'r', 'e', 's', ':', ' ', 'f', 'i', 'x', 'e', 's', ' ', 'h', 'i', 'g', 'h', ' ', 'v', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', 'T', 'r', 'y', ' ', 'g', 'e', 't', 't', 'i', 'n', 'g', ' ', 'a', 'd', 'd', 'i', 't', 'i', 'o', 'n', 'a', 'l', ' ', 'f', 'e', 'a', 't', 'u', 'r', 'e', 's', ':', ' ', 'f', 'i', 'x', 'e', 's', ' ', 'h', 'i', 'g', 'h', ' ', 'b', 'i', 'a', 's', ' ', '\\t', 'E', 'x', 'a', 'm', 'p', 'l', 'e', 's', ';', ' ', 'a', 'n', ' ', 'a', 'l', 'g', 'o', 'r', 'i', 't', 'h', 'm', ' ', 't', 'h', 'a', 't', ' ', 'l', 'a', 'c', 'k', 's', ' ', 'i', 'n', 'f', 'o', 'r', 'm', 'a', 't', 'i', 'o', 'n', ' ', 'w', 'o', 'n', 't', ' ', 'e', 'v', 'e', 'n', ' ', 'd', 'o', ' ', 'w', 'e', 'l', 'l', ' ', 'o', 'n', ' ', 't', 'h', 'e', ' ', 't', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ' ', 's', 'e', 't', ' ', 'A', 'd', 'd', 'i', 'n', 'g', ' ', 'p', 'o', 'l', 'y', 'n', 'o', 'm', 'i', 'a', 'l', ' ', 'f', 'e', 'a', 't', 'u', 'r', 'e', 's', ' ', '(', '$', 'x', '_', '1', '^', '2', ',', 'X', '_', '2', '^', '2', ',', ' ', 'e', 't', 'c', '$', ')', ':', ' ', 'F', 'i', 'x', 'e', 's', ' ', 'h', 'i', 'g', 'h', ' ', 'b', 'i', 'a', 's', ' ', 'D', 'e', 'c', 'r', 'e', 'a', 's', 'i', 'n', 'g', ' ', '$', '\\\\', 'l', 'a', 'm', 'b', 'd', 'a', '$', ' ', 'F', 'i', 'x', 'e', 's', ' ', 'h', 'i', 'g', 'h', ' ', 'b', 'i', 'a', 's', ' ', 'I', 'n', 'c', 'r', 'e', 'a', 's', 'i', 'n', 'g', ' ', '$', '\\\\', 'l', 'a', 'm', 'b', 'd', 'a', '$', ' ', 'F', 'i', 'x', 'e', 's', ' ', 'h', 'i', 'g', 'h', ' ', 'v', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', '\\t', 'F', 'o', 'r', 'c', 'e', 's', ' ', 't', 'h', 'e', ' ', 'a', 'l', 'g', 'o', 'r', 'i', 't', 'h', 'm', ' ', 't', 'o', ' ', 'f', 'o', 'r', 'c', 'e', ' ', 'a', ' ', 's', 'm', 'o', 'o', 't', 'h', 'e', 'r', ' ', 'f', 'u', 'n', 'c', 't', 'i', 'o', 'n', '.', ' ', ' ', '*', '*', 'N', 'o', 't', 'e', '!', '*', '*', ' ', 'D', 'o', 'n', \"'\", 't', ' ', 'r', 'a', 'n', 'd', 'o', 'm', 'l', 'y', ' ', 't', 'h', 'r', 'o', 'w', ' ', 'a', 'w', 'a', 'y', ' ', 't', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ' ', 'e', 'x', 'a', 'm', 'p', 'l', 'e', 's', ' ', 'j', 'u', 's', 't', ' ', 't', 'o', ' ', 'f', 'i', 'x', ' ', 'a', ' ', 'h', 'i', 'g', 'h', ' ', 'b', 'i', 'a', 's', ' ', 'p', 'r', 'o', 'b', 'l', 'e', 'm', '.', ' ', '#', '#', '#', ' ', 'T', 'a', 'k', 'e', 'a', 'w', 'a', 'y', ' ', '#', 'm', 'e', 'r', 'g', 'e', ' ', 'w', 'i', 't', 'h', ' ', 'B', 'i', 'a', 's', ' ', 'a', 'n', 'd', ' ', 'V', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', '?', ' ', 'I', 'f', ' ', 'y', 'o', 'u', 'r', ' ', 'a', 'l', 'g', 'o', 'r', 'i', 't', 'h', 'm', ' ', 'h', 'a', 's', ' ', 'h', 'i', 'g', 'h', ' ', 'v', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ',', ' ', 't', 'r', 'y', ' ', 's', 'i', 'm', 'p', 'l', 'i', 'f', 'y', 'i', 'n', 'g', ' ', 'y', 'o', 'u', 'r', ' ', 'm', 'o', 'd', 'e', 'l', ' ', 'o', 'r', ' ', 'g', 'e', 't', 't', 'i', 'n', 'g', ' ', 'm', 'o', 'r', 'e', ' ', 't', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ' ', 'd', 'a', 't', 'a', '.', ' ', 'S', 'i', 'm', 'p', 'l', 'i', 'f', 'i', 'c', 'a', 't', 'i', 'o', 'n', ' ', 'c', 'a', 'n', ' ', 'm', 'e', 'a', 'n', ' ', 'a', ' ', 's', 'm', 'a', 'l', 'l', 'e', 'r', ' ', 's', 'e', 't', ' ', 'o', 'f', ' ', 'f', 'e', 'a', 't', 'u', 'r', 'e', 's', ' ', 'o', 'r', ' ', 'a', 'n', ' ', 'i', 'n', 'c', 'r', 'e', 'a', 's', 'e', 'd', ' ', 'r', 'e', 'g', 'u', 'l', 'a', 'r', 'i', 'z', 'a', 't', 'i', 'o', 'n', ' ', 'I', 'f', ' ', 'y', 'o', 'u', 'r', ' ', 'a', 'l', 'g', 'o', 'r', 'i', 't', 'h', 'm', ' ', 'h', 'a', 's', ' ', 'h', 'i', 'g', 'h', ' ', 'b', 'i', 'a', 's', ',', ' ', 't', 'h', 'a', 't', ' ', 'i', 's', ' ', 't', 'o', ' ', 's', 'a', 'y', ' ', 'i', 't', 's', ' ', 'n', 'o', 't', ' ', 'e', 'v', 'e', 'n', ' ', 'd', 'o', 'i', 'n', 'g', ' ', 'w', 'e', 'l', 'l', ' ', 'o', 'n', ' ', 't', 'h', 'e', ' ', 't', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ' ', 's', 'e', 't', ',', ' ', 'y', 'o', 'u', ' ', 'm', 'a', 'i', 'n', 'l', 'y', ' ', 'n', 'e', 'e', 'd', ' ', 't', 'o', ' ', 'm', 'a', 'k', 'e', ' ', 'y', 'o', 'u', 'r', ' ', 'm', 'o', 'd', 'e', 'l', ' ', 'm', 'o', 'r', 'e', ' ', 'p', 'o', 'w', 'e', 'r', 'f', 'u', 'l', ' ', 'a', 'n', 'd', ' ', 'f', 'l', 'e', 'x', 'i', 'b', 'l', 'e', ' ', 't', 'o', ' ', 'f', 'i', 't', ' ', 'm', 'o', 'r', 'e', ' ', 'c', 'o', 'm', 'p', 'l', 'e', 'x', ' ', 'f', 'u', 'n', 'c', 't', 'i', 'o', 'n', 's', '.', ' ', 'T', 'o', ' ', 'd', 'o', ' ', 's', 'o', ' ', 'y', 'o', 'u', ' ', 'c', 'a', 'n', ' ', 'g', 'i', 'v', 'e', ' ', 'i', 't', ' ', 'a', 'd', 'd', 'i', 't', 'i', 'o', 'n', 'a', 'l', ' ', 'f', 'e', 'a', 't', 'u', 'r', 'e', 's', ',', ' ', 'a', 'd', 'd', ' ', 'p', 'o', 'l', 'y', 'n', 'o', 'm', 'i', 'a', 'l', ' ', 'f', 'e', 'a', 't', 'u', 'r', 'e', 's', ',', ' ', 'o', 'r', ' ', 'd', 'e', 'c', 'r', 'e', 'a', 's', 'e', ' ', '$', '\\\\', 'l', 'a', 'm', 'b', 'd', 'a', '$', ' ', ' ', ' ', '#', '#', ' ', 'B', 'i', 'a', 's', '/', 'V', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', 'a', 'n', 'd', ' ', 'N', 'e', 'u', 'r', 'a', 'l', ' ', 'N', 'e', 't', 'w', 'o', 'r', 'k', 's', ' ', '#', '#', '#', ' ', 'T', 'h', 'e', ' ', 'b', 'i', 'a', 's', ' ', 'v', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', 't', 'r', 'a', 'd', 'e', 'o', 'f', 'f', ' ', 'S', 'i', 'm', 'p', 'l', 'e', ' ', 'm', 'o', 'd', 'e', 'l', ' ', '=', ' ', 'h', 'i', 'g', 'h', ' ', 'b', 'i', 'a', 's', ' ', 'C', 'o', 'm', 'p', 'l', 'e', 'x', ' ', 'm', 'o', 'd', 'e', 'l', ' ', '=', ' ', 'h', 'i', 'g', 'h', ' ', 'v', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', 'B', 'e', 'f', 'o', 'r', 'e', ' ', 'n', 'e', 'u', 'r', 'a', 'l', ' ', 'n', 'e', 't', 'w', 'o', 'r', 'k', 's', ',', ' ', 'w', 'e', ' ', 'h', 'a', 'd', ' ', 't', 'o', ' ', 'w', 'o', 'r', 'r', 'y', ' ', 'a', 'b', 'o', 'u', 't', ' ', 'b', 'a', 'l', 'a', 'n', 'c', 'i', 'n', 'g', ' ', 't', 'h', 'i', 's', ' ', 'c', 'o', 'm', 'p', 'l', 'e', 'x', 'i', 't', 'y', ' ', 'b', 'e', 't', 'w', 'e', 'e', 'n', ' ', 'b', 'i', 'a', 's', ' ', 'a', 'n', 'd', ' ', 'v', 'a', 'r', 'i', 'a', 'n', 'c', 'e', '.', ' ', 'W', 'i', 't', 'h', ' ', 'n', 'e', 'u', 'r', 'a', 'l', ' ', 'n', 'e', 't', 'w', 'o', 'r', 'k', 's', ' ', 'w', 'e', ' ', 'n', 'o', 'w', ' ', 'a', 'r', 'e', ' ', 'm', 'o', 's', 't', 'l', 'y', ' ', 'w', 'o', 'r', 'r', 'i', 'e', 'd', ' ', 'a', 'b', 'o', 'u', 't', ' ', 'h', 'i', 'g', 'h', ' ', 'v', 'a', 'r', 'i', 'a', 'n', 'c', 'e', '.', ' ', ' ', '#', '#', '#', ' ', 'N', 'e', 'u', 'r', 'a', 'l', ' ', 'N', 'e', 't', 'w', 'o', 'r', 'k', 's', ' ', 'a', 'n', 'd', ' ', 'b', 'i', 'a', 's', ' ', 'v', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', 'L', 'a', 'r', 'g', 'e', ' ', 'N', 'e', 'u', 'r', 'a', 'l', ' ', 'N', 'e', 't', 'w', 'o', 'r', 'k', '|', 'n', 'e', 'u', 'r', 'a', 'l', ' ', 'n', 'e', 't', 'w', 'o', 'r', 'k', 's', ' ', 'a', 'r', 'e', ' ', 'l', 'o', 'w', ' ', 'b', 'i', 'a', 's', ' ', 'm', 'a', 'c', 'h', 'i', 'n', 'e', 's', '.', ' ', 'I', 'f', ' ', 'y', 'o', 'u', ' ', 'm', 'a', 'k', 'e', ' ', 'y', 'o', 'u', 'r', ' ', 'n', 'e', 'u', 'r', 'a', 'l', ' ', 'n', 'e', 't', 'w', 'o', 'r', 'k', ' ', 'l', 'a', 'r', 'g', 'e', ' ', 'e', 'n', 'o', 'u', 'g', 'h', ' ', 'y', 'o', 'u', ' ', 'c', 'a', 'n', ' ', 'a', 'l', 'm', 'o', 's', 't', ' ', 'a', 'l', 'w', 'a', 'y', 's', ' ', 'f', 'i', 't', ' ', 'y', 'o', 'u', 'r', ' ', 't', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ' ', 's', 'e', 't', ' ', 'w', 'e', 'l', 'l', '.', ' ', '#', '#', '#', '#', ' ', 'R', 'e', 'c', 'i', 'p', 'e', ' ', 'f', 'o', 'r', ' ', 'd', 'e', 'c', 'r', 'e', 'a', 's', 'i', 'n', 'g', ' ', 'b', 'i', 'a', 's', ' ', 'w', 'i', 't', 'h', ' ', 'a', ' ', 'n', 'e', 'u', 'r', 'a', 'l', ' ', 'n', 'e', 't', 'w', 'o', 'r', 'k', ' ', '1', '.', ' ', 'T', 'r', 'a', 'i', 'n', ' ', 'a', ' ', 'n', 'e', 'u', 'r', 'a', 'l', ' ', 'n', 'e', 't', 'w', 'o', 'r', 'k', ' ', '2', '.', ' ', ' ', 'I', 'f', ' ', 't', 'h', 'e', ' ', 't', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ' ', 's', 'e', 't', ' ', 'e', 'r', 'r', 'o', 'r', ' ', '$', 'J', '_', '{', 't', 'r', 'a', 'i', 'n', '}', '(', '\\\\', 'v', 'e', 'c', '{', 'w', '}', ',', 'b', ')', '$', ' ', 'i', 's', ' ', 'h', 'i', 'g', 'h', ' ', 'r', 'e', 'l', 'a', 't', 'i', 'v', 'e', ' ', 't', 'o', ' ', 'y', 'o', 'u', 'r', ' ', 'b', 'a', 's', 'e', 'l', 'i', 'n', 'e', ',', ' ', 'i', 'n', 'c', 'r', 'e', 'a', 's', 'e', ' ', 't', 'h', 'e', ' ', 's', 'i', 'z', 'e', ' ', 'o', 'f', ' ', 't', 'h', 'e', ' ', 'n', 'e', 'u', 'r', 'a', 'l', ' ', 'n', 'e', 't', 'w', 'o', 'r', 'k', ' ', 'b', 'y', ' ', 'a', 'd', 'd', 'i', 'n', 'g', ' ', 'h', 'i', 'd', 'd', 'e', 'n', ' ', 'l', 'a', 'y', 'e', 'r', 's', '.', ' ', '3', '.', ' ', 'O', 'n', 'c', 'e', ' ', '$', 'J', '_', '{', 't', 'r', 'a', 'i', 'n', '}', '(', '\\\\', 'v', 'e', 'c', '{', 'w', '}', ',', 'b', ')', '$', ' ', 'i', 's', ' ', 'l', 'o', 'w', ' ', 'e', 'n', 'o', 'u', 'g', 'h', ',', ' ', 's', 'e', 'e', ' ', 'i', 'f', ' ', 'i', 't', ' ', 'd', 'o', 'e', 's', ' ', 'w', 'e', 'l', 'l', ' ', 'o', 'n', ' ', 't', 'h', 'e', ' ', 'c', 'r', 'o', 's', 's', ' ', 'v', 'a', 'l', 'i', 'd', 'a', 't', 'i', 'o', 'n', ' ', 's', 'e', 't', ' ', '4', '.', ' ', ' ', 'I', 'f', ' ', 't', 'h', 'e', ' ', 'c', 'r', 'o', 's', 's', ' ', 'v', 'a', 'l', 'i', 'd', 'a', 't', 'i', 'o', 'n', ' ', 's', 'e', 't', ' ', '$', 'J', '_', '{', 'c', 'v', '}', '(', '\\\\', 'v', 'e', 'c', '{', 'w', '}', ',', 'b', ')', '$', ' ', 'i', 's', ' ', 't', 'o', 'o', ' ', 'h', 'i', 'g', 'h', ',', ' ', 'a', 'd', 'd', ' ', 'm', 'o', 'r', 'e', ' ', 'd', 'a', 't', 'a', ',', ' ', 't', 'h', 'e', 'n', ' ', 't', 'e', 's', 't', ' ', 'a', 'g', 'a', 'i', 'n', ' ', 'f', 'r', 'o', 'm', ' ', 's', 't', 'e', 'p', ' ', '2', '.', ' ', '5', '.', ' ', 'R', 'e', 'p', 'e', 'a', 't', ' ', 'u', 'n', 't', 'i', 'l', ' ', '$', 'J', '_', '{', 'c', 'v', '}', '(', '\\\\', 'v', 'e', 'c', '{', 'w', '}', ',', 'b', ')', '$', ' ', ' ', 'i', 's', ' ', 'l', 'o', 'w', ' ', 'e', 'n', 'o', 'u', 'g', 'h', ' ', 'f', 'o', 'r', ' ', 'y', 'o', 'u', 'r', ' ', 'l', 'i', 'k', 'i', 'n', 'g', '.', ' ', '#', '#', '#', '#', '#', ' ', 'S', 'l', 'i', 'd', 'e', ' ', 'I', 'l', 'l', 'u', 's', 't', 'r', 'a', 't', 'i', 'o', 'n', ' ', '!', 'S', 'c', 'r', 'e', 'e', 'n', 's', 'h', 'o', 't', ' ', '2', '0', '2', '3', '-', '1', '0', '-', '1', '7', ' ', 'a', 't', ' ', '6', '.', '3', '8', '.', '4', '6', ' ', 'P', 'M', '.', 'p', 'n', 'g', ' ', '#', '#', '#', ' ', 'L', 'i', 'm', 'i', 't', 'a', 't', 'i', 'o', 'n', 's', ' ', 'a', 'n', 'd', ' ', 'N', 'o', 't', 'e', 's', ' ', 'B', 'i', 'g', 'g', 'e', 'r', ' ', 'n', 'e', 't', 'w', 'o', 'r', 'k', 's', ' ', 'a', 'r', 'e', ' ', 'r', 'e', 's', 't', 'r', 'i', 'c', 't', 'e', 'd', ' ', 'b', 'y', ' ', 'y', 'o', 'u', 'r', ' ', 'c', 'o', 'm', 'p', 'u', 't', 'i', 'n', 'g', ' ', 'p', 'o', 'w', 'e', 'r', ',', ' ', 'd', 'a', 't', 'a', ' ', 'i', 's', ' ', 'r', 'e', 's', 't', 'r', 'i', 'c', 't', 'e', 'd', ' ', 't', 'o', ' ', 't', 'h', 'e', ' ', 'a', 'm', 'o', 'u', 'n', 't', ' ', 'o', 'f', ' ', 'd', 'a', 't', 'a', ' ', 'y', 'o', 'u', ' ', 'h', 'a', 'v', 'e', '.', ' ', ' ', 'S', 'o', 'm', 'e', 't', 'i', 'm', 'e', 's', ' ', 'y', 'o', 'u', ' ', 'w', 'i', 'l', 'l', ' ', 'p', 'i', 'n', 'g', 'p', 'o', 'n', 'g', ' ', 'b', 'a', 'c', 'k', ' ', 'b', 'e', 't', 'w', 'e', 'e', 'n', ' ', 'h', 'i', 'g', 'h', ' ', 'b', 'i', 'a', 's', ' ', 'a', 'n', 'd', ' ', 'h', 'i', 'g', 'h', ' ', 'v', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', 'a', 's', ' ', 'y', 'o', 'u', ' ', 'm', 'o', 'v', 'e', ' ', 't', 'h', 'r', 'o', 'u', 'g', 'h', ' ', 't', 'h', 'i', 's', ' ', 'r', 'e', 'c', 'i', 'p', 'e', ' ', 'a', 'n', 'd', ' ', 'd', 'e', 'v', 'e', 'l', 'o', 'p', ' ', 'a', ' ', 'm', 'a', 'c', 'h', 'i', 'n', 'e', ' ', 'l', 'e', 'a', 'r', 'n', 'i', 'n', 'g', ' ', 'a', 'l', 'g', 'o', 'r', 'i', 't', 'h', 'm', '.', ' ', 'U', 's', 'e', ' ', 't', 'h', 'e', 's', 'e', ' ', 'o', 'b', 's', 'e', 'r', 'v', 'a', 't', 'i', 'o', 'n', 's', ' ', 't', 'o', ' ', 's', 'h', 'a', 'p', 'e', ' ', 'w', 'h', 'a', 't', ' ', 'y', 'o', 'u', ' ', 'd', 'o', ' ', 'n', 'e', 'x', 't', ' ', 'i', 'n', ' ', 't', 'h', 'e', ' ', 'p', 'r', 'o', 'c', 'e', 's', 's', '.', ' ', ' ', '#', '#', '#', ' ', 'N', 'e', 'u', 'r', 'a', 'l', ' ', 'N', 'e', 't', 'w', 'o', 'r', 'k', 's', ' ', 'a', 'n', 'd', ' ', 'R', 'e', 'g', 'u', 'l', 'a', 'r', 'i', 'z', 'a', 't', 'i', 'o', 'n', ' ', 'A', ' ', 'l', 'a', 'r', 'g', 'e', ' ', 'n', 'e', 'u', 'r', 'a', 'l', ' ', 'n', 'e', 't', 'w', 'o', 'r', 'k', ' ', 'w', 'i', 'l', 'l', ' ', 'u', 's', 'u', 'a', 'l', 'l', 'y', ' ', 'd', 'o', ' ', 'a', 's', ' ', 'w', 'e', 'l', 'l', ' ', 'o', 'r', ' ', 'b', 'e', 't', 't', 'e', 'r', ' ', 't', 'h', 'a', 'n', ' ', 'a', ' ', 's', 'm', 'a', 'l', 'l', 'e', 'r', ' ', 'o', 'n', 'e', ' ', 's', 'o', ' ', 'l', 'o', 'n', 'g', ' ', 'a', 's', ' ', 'r', 'e', 'g', 'u', 'l', 'a', 'r', 'i', 'z', 'a', 't', 'i', 'o', 'n', ' ', 'i', 's', ' ', 'c', 'h', 'o', 's', 'e', 'n', ' ', 'a', 'p', 'p', 'r', 'o', 'p', 'r', 'i', 'a', 't', 'e', 'l', 'y', '.', ' ', 'O', 'f', ' ', 'c', 'o', 'u', 'r', 's', 'e', ',', ' ', 'l', 'a', 'r', 'g', 'e', 'r', ' ', 'n', 'e', 'u', 'r', 'a', 'l', ' ', 'n', 'e', 't', 'w', 'o', 'r', 'k', 's', ' ', 'a', 'r', 'e', ' ', 'm', 'o', 'r', 'e', ' ', 'c', 'o', 'm', 'p', 'u', 't', 'a', 't', 'i', 'o', 'n', 'a', 'l', 'l', 'y', ' ', 'e', 'x', 'p', 'e', 'n', 's', 'i', 'v', 'e', '.', ' ', '#', '#', '#', ' ', 'N', 'e', 'u', 'r', 'a', 'l', ' ', 'N', 'e', 't', 'w', 'o', 'r', 'k', ' ', 'R', 'e', 'g', 'u', 'l', 'a', 'r', 'i', 'z', 'a', 't', 'i', 'o', 'n', ' ', '#', 'i', 'm', 'p', 'o', 'r', 't', 'a', 'n', 't', ' ', ' ', '#', 'f', 'u', 'n', 'c', 't', 'i', 'o', 'n', ' ', 'a', 'n', 'd', ' ', 'T', 'e', 'n', 's', 'o', 'r', 'F', 'l', 'o', 'w', ' ', 'i', 'm', 'p', 'l', 'e', 'm', 'e', 'n', 't', 'a', 't', 'i', 'o', 'n', ':', ' ', '!', 'S', 'c', 'r', 'e', 'e', 'n', 's', 'h', 'o', 't', ' ', '2', '0', '2', '3', '-', '1', '0', '-', '1', '7', ' ', 'a', 't', ' ', '6', '.', '4', '6', '.', '2', '8', ' ', 'P', 'M', '.', 'p', 'n', 'g', '*', 'N', 'o', 't', 'e', ' ', 'u', 's', 'u', 'a', 'l', 'l', 'y', ' ', 'd', 'o', 'n', \"'\", 't', ' ', 'r', 'e', 'g', 'u', 'l', 'a', 'r', 'i', 'z', 'e', ' ', 'B', ',', ' ', 'i', 't', ' ', 'd', 'o', 'e', 's', 'n', \"'\", 't', ' ', 'r', 'e', 'a', 'l', 'l', 'y', ' ', 'a', 'f', 'f', 'e', 'c', 't', ' ', 'a', 'n', 'y', 't', 'h', 'i', 'n', 'g', '*', '#', '#', ' ', 'P', 'r', 'i', 'o', 'r', 'i', 't', 'y', ' ', 'C', 'o', 'm', 'p', 'l', 'e', 't', 'e', 'd', ':', ' ', 'C', 'o', 'm', 'p', 'l', 'e', 't', 'e', ' ', 'P', 'y', 't', 'h', 'o', 'n', ' ', 'm', 'o', 'd', 'u', 'l', 'e', ' ', 'C', 'o', 'm', 'p', 'l', 'e', 't', 'e', 'd', ':', ' ', 'C', 'o', 'm', 'p', 'l', 'e', 't', 'e', ' ', 'B', 'i', 'a', 's', ' ', 'a', 'n', 'd', ' ', 'V', 'a', 'r', 'i', 'a', 'n', 'c', 'e', '#', '#', ' ', 'B', 'o', 'n', 'u', 's', '/', 't', 'o', 'm', 'o', 'r', 'r', 'o', 'w', ':', ' ', 'C', 'o', 'm', 'p', 'l', 'e', 't', 'e', 'd', ':', ' ', 'N', 'o', 't', 'e', ' ', 'c', 'l', 'e', 'a', 'n', 'u', 'p', ' ', 'w', 'i', 't', 'h', ' ', 'A', 'l', 'i', 'a', 's', 'e', 's', ' ', '(', 's', 'e', 'e', ' ', 'C', 'a', 'r', 'e', 'e', 'r', ' ', 'a', 'n', 'd', ' ', 'S', 't', 'u', 'd', 'y', ' ', 'T', 'o', ' ', '-', ' ', 'D', 'o', ')', ' ', 'T', 'o', ' ', 'D', 'o', ':', ' ', 'L', 'i', 'n', 'e', 'a', 'r', ' ', 'A', 'l', 'g', 'e', 'b', 'r', 'a', ' ', 'N', 'o', 't', 'e', ' ', 'M', 'i', 'g', 'r', 'a', 't', 'i', 'o', 'n', ' ', 'C', 'o', 'm', 'p', 'l', 'e', 't', 'e', 'd', ':', ' ', 'I', 'n', 't', 'e', 'g', 'r', 'a', 't', 'e', ' ', 'w', 'i', 't', 'h', ' ', 't', 'h', 'i', 's', ' ', 'd', 'a', 'i', 'l', 'y', ' ', 'n', 'o', 't', 'e', ' ', '\\t', '\\t', 'T', 'o', ' ', 'D', 'o', ':', ' ', 'T', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ',', ' ', 'T', 'e', 's', 't', ',', ' ', 'a', 'n', 'd', ' ', 'D', 'e', 'v', ' ', 'S', 'e', 't', 's', ' ', '\\t', '\\t', 'T', 'o', ' ', 'D', 'o', ':', ' ', 'r', 'e', 'g', 'u', 'l', 'a', 'r', 'i', 'z', 'a', 't', 'i', 'o', 'n', ' ', '(', '#', 'm', 'e', 'r', 'g', 'e', ' ', 'a', 'n', 'd', ' ', 'i', 'n', 't', 'e', 'g', 'r', 'a', 't', 'e', ')', ' ', '\\t', '\\t', 'T', 'o', ' ', 'D', 'o', ':', ' ', 'C', 'o', 's', 't', ' ', 'a', 'n', 'd', ' ', 'L', 'o', 's', 's', ' ', 'C', 'o', 'm', 'p', 'l', 'e', 't', 'e', 'd', ':', ' ', 'B', 'i', 'g', ' ', 'o', 'v', 'e', 'r', 'h', 'a', 'u', 'l', ' ', 'o', 'f', ' ', 'B', 'i', 'a', 's', ' ', 'a', 'n', 'd', ' ', 'V', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', 'w', 'i', 't', 'h', ' ', 'r', 'e', 'g', 'a', 'r', 'd', 's', ' ', 't', 'o', ' ', 't', 'o', 'd', 'a', 'y', 's', ' ', 'n', 'o', 't', 'e', 's', '.', ' ', ' ', '#', ' ', 'M', 'a', 'c', 'h', 'i', 'n', 'e', ' ', 'L', 'e', 'a', 'r', 'n', 'i', 'n', 'g', ' ', 'S', 'p', 'e', 'c', 'i', 'a', 'l', 'i', 'z', 'a', 't', 'i', 'o', 'n', ' ', 'N', 'o', 't', 'e', 's', ':', ' ', 'T', 'h', 'e', 's', 'e', ' ', 'n', 'o', 't', 'e', 's', ' ', 'w', 'i', 'l', 'l', ' ', 'b', 'e', ' ', 'i', 'n', 't', 'e', 'g', 'r', 'a', 't', 'e', 'd', ' ', 'i', 'n', 't', 'o', ' ', 'o', 't', 'h', 'e', 'r', ' ', 'n', 'o', 't', 'e', 's', ' ', 'b', 'u', 't', ' ', 'k', 'e', 'p', 't', ' ', 'h', 'e', 'r', 'e', ' ', 'i', 'n', ' ', 't', 'h', 'e', 'i', 'r', ' ', 'e', 'n', 't', 'i', 'r', 'e', 't', 'y', '.', '#', '#', ' ', 'B', 'i', 'a', 's', ' ', 'a', 'n', 'd', ' ', 'V', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', 'M', 'a', 'c', 'h', 'i', 'n', 'e', ' ', 'L', 'e', 'a', 'r', 'n', 'i', 'n', 'g', ' ', 'S', 'p', 'e', 'c', 'i', 'a', 'l', 'i', 'z', 'a', 't', 'i', 'o', 'n', ' ', 'A', 'd', 'v', 'a', 'n', 'c', 'e', 'd', ' ', 'M', 'a', 'c', 'h', 'i', 'n', 'e', ' ', 'L', 'e', 'a', 'r', 'n', 'i', 'n', 'g', ' ', 'A', 'l', 'g', 'o', 'r', 'i', 't', 'h', 'm', 's', ',', ' ', 'W', 'e', 'e', 'k', ' ', '3', ' ', ' ', 'M', 'o', 'd', 'e', 'l', 's', ' ', 'a', 'l', 'm', 'o', 's', 't', ' ', 'n', 'e', 'v', 'e', 'r', ' ', 'w', 'o', 'r', 'k', ' ', 't', 'h', 'e', ' ', 'f', 'i', 'r', 's', 't', ' ', 't', 'i', 'm', 'e', ' ', 'y', 'o', 'u', ' ', 't', 'r', 'y', ' ', 't', 'h', 'e', 'm', ' ', 'o', 'u', 't', '.', ' ', 'L', 'e', 't', \"'\", 's', ' ', 's', 'e', 'e', ' ', 'h', 'o', 'w', ' ', 'w', 'e', ' ', 'c', 'a', 'n', ' ', 'f', 'i', 'x', ' ', 't', 'h', 'e', 'm', '.', ' ', ' ', '#', '#', '#', ' ', 'D', 'i', 'a', 'g', 'n', 'o', 's', 'i', 'n', 'g', ' ', 'B', 'i', 'a', 's', '/', 'V', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', 'S', 'e', 'e', ' ', 'B', 'i', 'a', 's', ' ', 'a', 'n', 'd', ' ', 'V', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', ' ', 'I', 'f', ' ', 'y', 'o', 'u', ' ', 'h', 'a', 'v', 'e', ' ', 'm', 'o', 'r', 'e', ' ', 'f', 'e', 'a', 't', 'u', 'r', 'e', 's', ',', ' ', 'y', 'o', 'u', ' ', 'c', 'a', 'n', \"'\", 't', ' ', 'v', 'i', 's', 'u', 'a', 'l', 'i', 'z', 'e', ' ', 'B', 'i', 'a', 's', ' ', 'a', 'n', 'd', ' ', 'V', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', ' ', 'A', ' ', 'm', 'o', 'r', 'e', ' ', 's', 'y', 's', 't', 'e', 'm', 'a', 't', 'i', 'c', ' ', 'w', 'a', 'y', ' ', 't', 'o', ' ', 's', 'e', 'e', ' ', 'i', 'f', ' ', 'y', 'o', 'u', ' ', 'h', 'a', 'v', 'e', ' ', 'h', 'i', 'g', 'h', ' ', 'b', 'i', 'a', 's', ' ', 'o', 'r', ' ', 'h', 'i', 'g', 'h', ' ', 'v', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', 'i', 's', ' ', 't', 'o', ' ', 'l', 'o', 'o', 'k', ' ', 'a', 't', ' ', 't', 'h', 'e', ' ', 'p', 'e', 'r', 'f', 'o', 'r', 'm', 'a', 'n', 'c', 'e', ' ', 'o', 'f', ' ', 't', 'h', 'e', ' ', 'a', 'l', 'g', 'o', 'r', 'i', 't', 'h', 'm', ' ', 'o', 'n', ' ', 't', 'h', 'e', ' ', 'T', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ',', ' ', 'T', 'e', 's', 't', ',', ' ', 'a', 'n', 'd', ' ', 'D', 'e', 'v', ' ', 'S', 'e', 't', 's', '|', 't', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ' ', 's', 'e', 't', ' ', 'a', 'n', 'd', ' ', 'd', 'e', 'v', ' ', 's', 'e', 't', ' ', ' ', 'A', ' ', 'c', 'h', 'a', 'r', 'a', 'c', 't', 'e', 'r', 'i', 's', 't', 'i', 'c', ' ', 'o', 'f', ' ', 'a', ' ', 'H', 'i', 'g', 'h', ' ', 'B', 'i', 'a', 's', ' ', '(', 'u', 'n', 'd', 'e', 'r', '-', 'f', 'i', 't', ')', ' ', 'm', 'o', 'd', 'e', 'l', ' ', 'i', 's', ' ', 't', 'h', 'a', 't', ' ', '$', 'J', 't', 'r', 'a', 'i', 'n', '$', ' ', '(', 'c', 'o', 's', 't', ' ', 'o', 'f', ' ', 't', 'h', 'e', ' ', 'T', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ',', ' ', 'T', 'e', 's', 't', ',', ' ', 'a', 'n', 'd', ' ', 'D', 'e', 'v', ' ', 'S', 'e', 't', 's', '|', 't', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ' ', 's', 'e', 't', ')', ' ', 'i', 's', ' ', 'h', 'i', 'g', 'h', '.', ' ', 'A', ' ', 'c', 'h', 'a', 'r', 'a', 'c', 't', 'e', 'r', 'i', 's', 't', 'i', 'c', ' ', 'o', 'f', ' ', 'a', ' ', 'H', 'i', 'g', 'h', ' ', 'V', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', '(', 'o', 'v', 'e', 'r', 'f', 'i', 't', ')', ' ', 'm', 'o', 'd', 'e', 'l', ' ', 'i', 's', ' ', 't', 'h', 'a', 't', ' ', '$', 'J', '_', '{', 'c', 'v', '}', '$', ' ', 'i', 's', ' ', 'h', 'i', 'g', 'h', ' ', 'b', 'u', 't', ' ', '$', 'J', '_', '{', 't', 'r', 'a', 'i', 'n', '}', '$', ' ', 'i', 's', ' ', 'l', 'o', 'w', '.', ' ', '#', '#', '#', '#', ' ', 'S', 'l', 'i', 'd', 'e', ' ', '!', 'S', 'c', 'r', 'e', 'e', 'n', 's', 'h', 'o', 't', ' ', '2', '0', '2', '3', '-', '1', '0', '-', '1', '7', ' ', 'a', 't', ' ', '1', '.', '2', '6', '.', '0', '0', ' ', 'P', 'M', '.', 'p', 'n', 'g', ' ', ' ', '#', '#', '#', ' ', 'U', 'n', 'd', 'e', 'r', 's', 't', 'a', 'n', 'd', 'i', 'n', 'g', ' ', 'B', 'i', 'a', 's', ' ', 'a', 'n', 'd', ' ', 'V', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', 'T', 'a', 'k', 'e', ' ', 'd', 'e', 'g', 'r', 'e', 'e', ' ', 'o', 'f', ' ', 'p', 'o', 'l', 'y', 'n', 'o', 'm', 'i', 'a', 'l', ' ', '$', 'd', '$', ' ', 'A', 's', ' ', '$', 'd', '$', ' ', 'i', 'n', 'c', 'r', 'e', 'a', 's', 'e', 's', ',', ' ', 't', 'h', 'a', 't', ' ', 'i', 's', ' ', 't', 'o', ' ', 's', 'a', 'y', ' ', 'w', 'e', ' ', 'a', 'd', 'd', ' ', 'd', 'e', 'g', 'r', 'e', 'e', 's', ' ', 't', 'o', ' ', 't', 'h', 'e', ' ', 'p', 'o', 'l', 'y', 'n', 'o', 'm', 'i', 'a', 'l', ' ', '$', 'J', '_', '{', 't', 'r', 'a', 'i', 'n', '}', '(', '{', '\\\\', 'v', 'e', 'c', '{', 'w', '}', ',', 'b', '}', ')', '$', ' ', ' ', 'w', 'i', 'l', 'l', ' ', 'd', 'e', 'c', 'r', 'e', 'a', 's', 'e', '.', ' ', '$', 'J', '_', '{', 'c', 'v', '}', '(', '\\\\', 'v', 'e', 'c', '{', 'w', '}', ',', 'b', ')', '$', ' ', 'd', 'i', 'p', 's', ' ', 't', 'o', 'w', 'a', 'r', 'd', 's', ' ', 't', 'h', 'e', ' ', 'm', 'i', 'd', 'd', 'l', 'e', ',', ' ', 's', 'h', 'o', 'w', 'i', 'n', 'g', ' ', 't', 'h', 'e', ' ', 'i', 'd', 'e', 'a', 'l', ' ', 'd', 'e', 'g', 'r', 'e', 'e', ' ', 'f', 'o', 'r', ' ', 't', 'h', 'e', ' ', 'p', 'o', 'l', 'y', 'n', 'o', 'm', 'i', 'a', 'l', '.', ' ', ' ', '#', '#', '#', '#', ' ', 'S', 'l', 'i', 'd', 'e', ' ', '!', 'S', 'c', 'r', 'e', 'e', 'n', 's', 'h', 'o', 't', ' ', '2', '0', '2', '3', '-', '1', '0', '-', '1', '7', ' ', 'a', 't', ' ', '1', '.', '2', '9', '.', '2', '1', ' ', 'P', 'M', '.', 'p', 'n', 'g', ' ', ' ', '#', '#', '#', ' ', 'T', 'e', 'c', 'h', 'n', 'i', 'q', 'u', 'e', ' ', 'f', 'o', 'r', ' ', 'D', 'i', 'a', 'g', 'n', 'o', 's', 'i', 'n', 'g', ' ', 'B', 'i', 'a', 's', ' ', 'a', 'n', 'd', ' ', 'V', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', 'H', 'o', 'w', ' ', 'd', 'o', ' ', 'y', 'o', 'u', ' ', 't', 'e', 'l', 'l', ' ', 'i', 'f', ' ', 'y', 'o', 'u', 'r', ' ', 'a', 'l', 'g', 'o', 'r', 'i', 't', 'h', 'm', ' ', 'h', 'a', 's', ' ', 'a', ' ', 'h', 'i', 'g', 'h', ' ', 'b', 'i', 'a', 's', ' ', 'o', 'r', ' ', 'v', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', 'p', 'r', 'o', 'b', 'l', 'e', 'm', '?', ' ', ' ', 'O', 'n', 'e', ' ', 'k', 'e', 'y', ' ', 't', 'a', 'k', 'e', 'a', 'w', 'a', 'y', ' ', 'i', 's', ' ', 't', 'h', 'a', 't', ' ', 'h', 'i', 'g', 'h', ' ', 'b', 'i', 'a', 's', ' ', 'm', 'e', 'a', 'n', 's', ' ', 'i', 't', ' ', 'i', 's', ' ', 'n', 'o', 't', ' ', 'e', 'v', 'e', 'n', ' ', 'd', 'o', 'i', 'n', 'g', ' ', 'w', 'e', 'l', 'l', ' ', 'o', 'n', ' ', 't', 'h', 'e', ' ', 't', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ' ', 's', 'e', 't', ' ', 'a', 'n', 'd', ' ', 'h', 'i', 'g', 'h', ' ', 'v', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', 'm', 'e', 'a', 'n', 's', ' ', 'i', 't', ' ', 'd', 'o', 'e', 's', ' ', 'm', 'u', 'c', 'h', ' ', 'w', 'o', 'r', 's', 'e', ' ', 'o', 'n', ' ', 't', 'h', 'e', ' ', 'c', 'r', 'o', 's', 's', ' ', 'v', 'a', 'l', 'i', 'd', 'a', 't', 'i', 'o', 'n', ' ', 's', 'e', 't', ' ', 'a', 'n', 'd', ' ', 't', 'h', 'e', ' ', 't', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ' ', 's', 'e', 't', '.', ' ', '#', '#', '#', '#', ' ', 'H', 'i', 'g', 'h', ' ', 'B', 'i', 'a', 's', ' ', '(', 'u', 'n', 'd', 'e', 'r', '-', 'f', 'i', 't', ')', ' ', 'I', 'n', ' ', 'a', 'n', ' ', 'u', 'n', 'd', 'e', 'r', '-', 'f', 'i', 't', ' ', 'm', 'o', 'd', 'e', 'l', ' ', '$', 'J', '_', '{', 't', 'r', 'a', 'i', 'n', '}', '$', ' ', 'w', 'i', 'l', 'l', ' ', 'b', 'e', ' ', 'h', 'i', 'g', 'h', ' ', 'a', 'n', 'd', ' ', '$', 'J', '_', '{', 't', 'r', 'a', 'i', 'n', '}', ' ', '\\\\', 'a', 'p', 'p', 'r', 'o', 'x', ' ', 'J', '_', '{', 'c', 'v', '}', '$', ' ', ' ', '#', '#', '#', '#', ' ', 'H', 'i', 'g', 'h', ' ', 'V', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', '(', 'o', 'v', 'e', 'r', 'f', 'i', 't', ')', ' ', 'I', 'n', ' ', 'a', 'n', ' ', 'o', 'v', 'e', 'r', '-', 'f', 'i', 't', ' ', 'm', 'o', 'd', 'e', 'l', ' ', '$', 'J', '_', '{', 'c', 'v', '}', ' ', '\\\\', 'g', 'g', ' ', 'J', '_', '{', 't', 'r', 'a', 'i', 'n', '}', '$', ' ', 'a', 'n', 'd', ' ', '$', 'J', '_', '{', 't', 'r', 'a', 'i', 'n', '}', '$', ' ', 'w', 'i', 'l', 'l', ' ', 'b', 'e', ' ', 'l', 'o', 'w', '.', ' ', '#', '#', '#', '#', ' ', 'H', 'i', 'g', 'h', ' ', 'B', 'i', 'a', 's', ' ', 'a', 'n', 'd', ' ', 'H', 'i', 'g', 'h', ' ', 'V', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', 'I', 'n', ' ', 'a', ' ', 'm', 'o', 'd', 'e', 'l', ' ', 'w', 'i', 't', 'h', ' ', 'b', 'o', 't', 'h', ' ', 'h', 'i', 'g', 'h', ' ', 'b', 'i', 'a', 's', ' ', 'a', 'n', 'd', ' ', 'h', 'i', 'g', 'h', ' ', 'v', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', '$', 'J', '_', '{', 't', 'r', 'a', 'i', 'n', '}', '$', ' ', 'w', 'i', 'l', 'l', ' ', 'b', 'e', ' ', 'h', 'i', 'g', 'h', ' ', 'a', 'n', 'd', ' ', '$', 'J', '_', '{', 'c', 'v', '}', ' ', '\\\\', 'g', 'g', ' ', 'J', '_', '{', 't', 'r', 'a', 'i', 'n', '}', '$', ' ', ' ', 'T', 'h', 'i', 's', ' ', 'i', 's', ' ', 'r', 'a', 'r', 'e', ',', ' ', 'a', 'n', 'd', ' ', 'd', 'o', 'e', 's', 'n', \"'\", 't', ' ', 'r', 'e', 'a', 'l', 'l', 'y', ' ', 'h', 'a', 'p', 'p', 'e', 'n', ' ', 'f', 'o', 'r', ' ', 'l', 'i', 'n', 'e', 'a', 'r', ' ', 'm', 'o', 'd', 'e', 'l', 's', ' ', 'w', 'i', 't', 'h', ' ', 'o', 'n', 'e', ' ', '$', 'd', '$', ',', ' ', 'b', 'u', 't', ' ', 'i', 't', ' ', 'd', 'o', 'e', 's', ' ', 'h', 'a', 'p', 'p', 'e', 'n', '.', ' ', 'T', 'h', 'i', 's', ' ', 'i', 's', ' ', 'w', 'h', 'e', 'n', ' ', 'i', 't', ' ', 'o', 'v', 'e', 'r', 'f', 'i', 't', 's', ' ', 'f', 'o', 'r', ' ', 's', 'o', 'm', 'e', ' ', 'p', 'a', 'r', 't', ' ', 'o', 'f', ' ', 't', 'h', 'e', ' ', 'i', 'n', 'p', 'u', 't', ' ', 'a', 'n', 'd', ' ', 'u', 'n', 'd', 'e', 'r', '-', 'f', 'i', 't', 's', ' ', 'f', 'o', 'r', ' ', 'a', 'n', 'o', 't', 'h', 'e', 'r', ' ', 'p', 'a', 'r', 't', ' ', 'o', 'f', ' ', 't', 'h', 'e', ' ', 'i', 'n', 'p', 'u', 't', '.', ' ', '#', '#', '#', '#', ' ', 'S', 'l', 'i', 'd', 'e', ' ', '!', 'S', 'c', 'r', 'e', 'e', 'n', 's', 'h', 'o', 't', ' ', '2', '0', '2', '3', '-', '1', '0', '-', '1', '7', ' ', 'a', 't', ' ', '1', '.', '3', '8', '.', '5', '1', ' ', 'P', 'M', '.', 'p', 'n', 'g', ' ', '#', '#', ' ', 'R', 'e', 'g', 'u', 'l', 'a', 'r', 'i', 'z', 'a', 't', 'i', 'o', 'n', ' ', 'a', 'n', 'd', ' ', 'b', 'i', 'a', 's', '/', 'v', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', '#', '#', '#', ' ', 'L', 'i', 'n', 'e', 'a', 'r', ' ', 'R', 'e', 'g', 'r', 'e', 's', 's', 'i', 'o', 'n', ' ', 'w', 'i', 't', 'h', ' ', 'R', 'e', 'g', 'u', 'l', 'a', 'r', 'i', 'z', 'a', 't', 'i', 'o', 'n', ' ', 'R', 'e', 'g', 'u', 'l', 'a', 'r', 'i', 'z', 'e', 'd', ' ', 'L', 'i', 'n', 'e', 'a', 'r', ' ', 'R', 'e', 'g', 'r', 'e', 's', 's', 'i', 'o', 'n', ' ', 'm', 'o', 'd', 'e', 'l', ':', ' ', 'L', 'a', 'm', 'b', 'd', 'a', ' ', '$', '\\\\', 'l', 'a', 'm', 'b', 'd', 'a', '$', ' ', 'i', 's', ' ', 't', 'h', 'e', ' ', 'r', 'e', 'g', 'u', 'l', 'a', 'r', 'i', 'z', 'a', 't', 'i', 'o', 'n', ' ', 'p', 'a', 'r', 'a', 'm', 'e', 't', 'e', 'r', ' ', '(', 'B', 'i', 'g', ' ', '#', 'm', 'e', 'r', 'g', 'e', ' ', 'a', 'n', 'd', ' ', 'c', 'l', 'e', 'a', 'n', 'u', 'p', ' ', 'n', 'e', 'e', 'd', 'e', 'd', ' ', 'h', 'e', 'r', 'e', ' ', 'i', 'n', ' ', 't', 'h', 'e', ' ', 'r', 'e', 'g', 'u', 'l', 'a', 'r', 'i', 'z', 'a', 't', 'i', 'o', 'n', ',', ' ', 'R', 'e', 'g', 'u', 'l', 'a', 'r', 'i', 'z', 'e', 'd', ' ', 'L', 'i', 'n', 'e', 'a', 'r', ' ', 'R', 'e', 'g', 'r', 'e', 's', 's', 'i', 'o', 'n', ',', ' ', 'c', 'o', 's', 't', ',', ' ', 'C', 'o', 's', 't', ' ', 'a', 'n', 'd', ' ', 'L', 'o', 's', 's', ',', ' ', 'l', 'o', 's', 's', ',', ' ', 'L', 'o', 's', 's', ' ', 'F', 'u', 'n', 'c', 't', 'i', 'o', 'n', ',', ' ', 'e', 't', 'c', '.', ' ', 'A', 'l', 'i', 'a', 's', 'e', 's', ' ', 'a', 'r', 'e', ' ', 'n', 'e', 'e', 'd', 'e', 'd', ' ', 'i', 'n', ' ', 'm', 'a', 'n', 'y', ' ', 'o', 'f', ' ', 't', 'h', 'e', 's', 'e', ')', ' ', ' ', 'T', 'a', 'k', 'e', ' ', 't', 'h', 'i', 's', ' ', 'm', 'o', 'd', 'e', 'l', ':', ' ', '$', '$', 'f', '_', '{', '\\\\', 'v', 'e', 'c', '{', 'w', '}', ',', 'b', '}', '(', 'x', ')', '=', 'w', '_', '1', '+', 'w', '_', '2', 'x', '^', '2', ' ', '+', ' ', 'w', '_', '4', 'x', '^', '4', '+', 'b', '$', '$', ' ', 'W', 'i', 't', 'h', ' ', 't', 'h', 'i', 's', ' ', 'r', 'e', 'g', 'u', 'l', 'a', 'r', 'i', 'z', 'a', 't', 'i', 'o', 'n', '|', 'r', 'e', 'g', 'u', 'l', 'a', 'r', 'i', 'z', 'e', 'd', ' ', 'C', 'o', 's', 't', ' ', 'a', 'n', 'd', ' ', 'L', 'o', 's', 's', ' ', '$', '$', 'J', '(', '\\\\', 'v', 'e', 'c', '{', 'w', '}', ',', 'b', ')', '=', '\\\\', 'f', 'r', 'a', 'c', '{', '1', '}', '{', '2', 'm', '}', ' ', '\\\\', 's', 'u', 'm', '_', '{', 'i', '=', '1', '}', '^', '{', 'm', '}', '(', 'f', '_', '{', '\\\\', 'v', 'e', 'c', '{', 'w', '}', ',', '{', 'b', '}', '}', '(', '\\\\', 'v', 'e', 'c', '{', 'x', '}', '^', '{', '(', 'i', ')', '}', '-', 'y', '^', '{', '(', 'i', ')', '}', ')', '^', '2', '+', '\\\\', 'f', 'r', 'a', 'c', '{', '\\\\', 'l', 'a', 'm', 'b', 'd', 'a', '}', '{', '2', 'm', '}', ' ', '\\\\', 's', 'u', 'm', '_', '{', 'j', '=', '1', '}', '^', 'n', 'w', '_', 'j', '^', '2', '$', '$', ' ', ' ', 'I', 'f', ' ', 'l', 'a', 'm', 'b', 'd', 'a', ' ', 'i', 's', ' ', 'l', 'a', 'r', 'g', 'e', ',', ' ', 's', 'a', 'y', ' ', '$', '\\\\', 'l', 'a', 'm', 'b', 'd', 'a', ' ', '=', ' ', '1', '0', ',', '0', '0', '0', '$', ' ', 't', 'h', 'a', 'n', ' ', '$', 'w', '_', '1', ' ', '\\\\', 'a', 'p', 'p', 'r', 'o', 'x', ' ', '0', ',', ' ', 'W', '_', '2', ' ', '\\\\', 'a', 'p', 'p', 'r', 'o', 'x', ' ', '0', '$', ' ', 'a', 'n', 'd', ' ', '$', 'f', '_', '{', '\\\\', 'v', 'e', 'c', '{', 'x', '}', ',', 'b', '}', '\\\\', 'v', 'e', 'c', '{', 'x', '}', ' ', '\\\\', 'a', 'p', 'p', 'r', 'o', 'x', ' ', 'b', '$', ',', ' ', 't', 'h', 'u', 's', ' ', 't', 'h', 'e', ' ', 'm', 'o', 'd', 'e', 'l', ' ', 'c', 'r', 'e', 'a', 't', 'e', 's', ' ', 'a', ' ', 'f', 'l', 'a', 't', ' ', 'l', 'i', 'n', 'e', '.', ' ', 'T', 'h', 'i', 's', ' ', 'i', 's', ' ', 'u', 'n', 'd', 'e', 'r', 'f', 'i', 't', ',', ' ', '$', 'J', '_', '{', 't', 'r', 'a', 'i', 'n', '}', '(', ')', ' ', ' ', 'O', 'n', ' ', 't', 'h', 'e', ' ', 'o', 't', 'h', 'e', 'r', ' ', 'h', 'a', 'n', 'd', ',', ' ', 'i', 'f', ' ', 'w', 'e', ' ', 's', 'e', 't', ' ', '$', '\\\\', 'l', 'a', 'm', 'b', 'd', 'a', '=', '0', '$', ',', ' ', 't', 'h', 'e', 'n', ' ', 'w', 'e', ' ', 'h', 'a', 'v', 'e', ' ', 'a', ' ', 'f', 'o', 'r', 't', 'h', ' ', 'o', 'r', 'd', 'e', 'r', ' ', 'p', 'o', 'l', 'y', 'n', 'o', 'm', 'i', 'a', 'l', ' ', 'w', 'i', 't', 'h', ' ', 'n', 'o', ' ', 'r', 'e', 'g', 'u', 'l', 'a', 'r', 'i', 'z', 'a', 't', 'i', 'o', 'n', '.', ' ', 'W', 'e', ' ', 'e', 'n', 'd', ' ', 'u', 'p', ' ', 'w', 'i', 't', 'h', ' ', 'a', ' ', 'v', 'e', 'r', 'y', ' ', 'o', 'v', 'e', 'r', 'f', 'i', 't', ' ', 'c', 'u', 'r', 'v', 'e', '.', ' ', ' ', 'S', 'o', ',', ' ', 'h', 'o', 'w', ' ', 'd', 'o', ' ', 'w', 'e', ' ', 'f', 'i', 'n', 'd', ' ', 'a', ' ', 'g', 'o', 'o', 'd', ' ', 'v', 'a', 'l', 'u', 'e', ' ', 'f', 'o', 'r', ' ', '$', '\\\\', 'l', 'a', 'm', 'b', 'd', 'a', '$', '?', ' ', '#', '#', '#', '#', '#', ' ', 'S', 'l', 'i', 'd', 'e', ' ', '!', 'S', 'c', 'r', 'e', 'e', 'n', 's', 'h', 'o', 't', ' ', '2', '0', '2', '3', '-', '1', '0', '-', '1', '7', ' ', 'a', 't', ' ', '3', '.', '5', '6', '.', '5', '3', ' ', 'P', 'M', '.', 'p', 'n', 'g', ' ', '#', '#', '#', ' ', 'C', 'h', 'o', 'o', 's', 'i', 'n', 'g', ' ', 't', 'h', 'e', ' ', 'R', 'e', 'g', 'u', 'l', 'a', 'r', 'i', 'z', 'a', 't', 'i', 'o', 'n', ' ', 'p', 'a', 'r', 'a', 'm', 'e', 't', 'e', 'r', ' ', '$', '\\\\', 'l', 'a', 'm', 'b', 'd', 'a', '$', ' ', ' ', ' ', 'T', 'h', 'i', 's', ' ', 'w', 'i', 'l', 'l', ' ', 'b', 'e', ' ', 's', 'i', 'm', 'i', 'l', 'a', 'r', ' ', 't', 'o', ' ', 'c', 'h', 'o', 'o', 's', 'i', 'n', 'g', ' ', '$', 'd', '$', ' ', 'w', 'i', 't', 'h', ' ', 'c', 'r', 'o', 's', 's', ' ', 'v', 'a', 'l', 'i', 'd', 'a', 't', 'i', 'o', 'n', ' ', 'T', 'r', 'y', ' ', 'm', 'u', 'l', 't', 'i', 'p', 'l', 'e', ' ', 'v', 'a', 'l', 'u', 'e', 's', ' ', 'f', 'o', 'r', ' ', '$', '\\\\', 'l', 'a', 'm', 'b', 'd', 'a', '$', ' ', 'a', 'n', 'd', ' ', 't', 'h', 'e', 'n', ' ', 'c', 'h', 'o', 'o', 's', 'e', ' ', 't', 'h', 'e', ' ', 'o', 'p', 't', 'i', 'o', 'n', ' ', 'w', 'i', 't', 'h', ' ', 't', 'h', 'e', ' ', 'l', 'o', 'w', 'e', 's', 't', ' ', 'c', 'o', 's', 't', '.', ' ', '#', '#', '#', ' ', 'S', 'l', 'i', 'd', 'e', ' ', '!', 'S', 'c', 'r', 'e', 'e', 'n', 's', 'h', 'o', 't', ' ', '2', '0', '2', '3', '-', '1', '0', '-', '1', '7', ' ', 'a', 't', ' ', '4', '.', '1', '5', '.', '2', '7', ' ', 'P', 'M', '.', 'p', 'n', 'g', ' ', '#', '#', '#', ' ', 'B', 'i', 'a', 's', ' ', 'a', 'n', 'd', ' ', 'V', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', 'a', 's', ' ', 'a', ' ', 'F', 'u', 'n', 'c', 't', 'i', 'o', 'n', ' ', 'o', 'f', ' ', 'r', 'e', 'g', 'u', 'l', 'a', 'r', 'i', 'z', 'a', 't', 'i', 'o', 'n', ' ', 'p', 'a', 'r', 'a', 'm', 'e', 't', 'e', 'r', ' ', '$', '\\\\', 'l', 'a', 'm', 'b', 'd', 'a', '$', ' ', ' ', 'C', 'r', 'o', 's', 's', ' ', 'V', 'a', 'l', 'i', 'd', 'a', 't', 'i', 'o', 'n', ' ', 't', 'r', 'i', 'e', 's', ' ', 'o', 'u', 't', ' ', 'm', 'a', 'n', 'y', ' ', 'v', 'e', 'r', 's', 'i', 'o', 'n', 's', ' ', 'o', 'f', ' ', '$', '\\\\', 'l', 'a', 'm', 'b', 'd', 'a', '$', ' ', 'a', 'n', 'd', ' ', 't', 'h', 'e', 'n', ' ', 'c', 'h', 'o', 'o', 's', 'e', 's', ' ', 't', 'h', 'e', ' ', 'o', 'n', 'e', ' ', 'w', 'i', 't', 'h', ' ', 't', 'h', 'e', ' ', 'l', 'o', 'w', 'e', 's', 't', ' ', 'c', 'o', 's', 't', '.', ' ', 'C', 'r', 'o', 's', 's', ' ', 'v', 'a', 'l', 'i', 'd', 'a', 't', 'i', 'o', 'n', ' ', 'w', 'i', 'l', 'l', ' ', 'h', 'e', 'l', 'p', ' ', 'f', 'i', 'n', 'd', ' ', 'u', 's', ' ', 'a', ' ', 'a', ' ', 'g', 'o', 'o', 'd', ' ', 'v', 'a', 'l', 'u', 'e', ' ', 'o', 'f', ' ', '$', 'd', '$', ' ', 'a', 's', ' ', 'w', 'e', 'l', 'l', ' ', 'a', 's', ' ', '$', '\\\\', 'l', 'a', 'm', 'b', 'd', 'a', '$', ' ', ' ', '#', '#', '#', '#', ' ', 'S', 'l', 'i', 'd', 'e', ' ', '!', 'S', 'c', 'r', 'e', 'e', 'n', 's', 'h', 'o', 't', ' ', '2', '0', '2', '3', '-', '1', '0', '-', '1', '7', ' ', 'a', 't', ' ', '4', '.', '1', '4', '.', '0', '7', ' ', 'P', 'M', '.', 'p', 'n', 'g', ' ', ' ', '#', '#', ' ', 'E', 's', 't', 'a', 'b', 'l', 'i', 's', 'h', 'i', 'n', 'g', ' ', 'A', ' ', 'B', 'a', 's', 'e', 'l', 'i', 'n', 'e', ' ', 'L', 'e', 'v', 'e', 'l', ' ', 'o', 'f', ' ', 'P', 'e', 'r', 'f', 'o', 'r', 'm', 'a', 'n', 'c', 'e', ' ', '#', '#', '#', ' ', 'S', 'p', 'e', 'e', 'c', 'h', ' ', 'R', 'e', 'c', 'o', 'g', 'n', 'i', 't', 'i', 'o', 'n', ' ', 'e', 'x', 'a', 'm', 'p', 'l', 'e', ' ', 'J', 'o', 'b', ' ', 'i', 's', ' ', 't', 'o', ' ', 't', 'a', 'k', 'e', ' ', 'i', 'n', ' ', 'a', 'u', 'd', 'i', 'o', ' ', 'a', 'n', 'd', ' ', 'o', 'u', 't', 'p', 'u', 't', ' ', 't', 'h', 'e', ' ', 't', 'e', 'x', 't', ' ', 'o', 'f', ' ', 'w', 'h', 'a', 't', ' ', 'a', ' ', 'p', 'e', 'r', 's', 'o', 'n', ' ', 'i', 's', ' ', 's', 'a', 'y', 'i', 'n', 'g', ' ', ' ', 'T', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ' ', 'e', 'r', 'r', 'o', 'r', ' ', '$', 'J', '_', '{', 't', 'r', 'a', 'i', 'n', '}', '$', ' ', 'i', 's', ' ', 'p', 'e', 'r', 'c', 'e', 'n', 't', 'a', 'g', 'e', ' ', 'o', 'f', ' ', 'a', 'u', 'd', 'i', 'o', ' ', 'c', 'l', 'i', 'p', 's', ' ', 't', 'h', 'a', 't', ' ', 't', 'h', 'e', ' ', 'p', 'r', 'o', 'g', 'r', 'a', 'm', ' ', 'd', 'o', 'e', 's', ' ', 'n', 'o', 't', ' ', 't', 'r', 'a', 'n', 's', 'c', 'r', 'i', 'b', 'e', ' ', 'c', 'o', 'r', 'r', 'e', 'c', 't', 'l', 'y', ' ', 'i', 'n', ' ', 'i', 't', \"'\", 's', ' ', 'e', 'n', 't', 'i', 'r', 'e', 't', 'y', '.', ' ', 'L', 'e', 't', 's', ' ', 's', 'a', 'y', ':', ' ', '\\t', 'H', 'u', 'm', 'a', 'n', ' ', 'L', 'e', 'v', 'e', 'l', ' ', 'P', 'e', 'r', 'f', 'o', 'r', 'm', 'a', 'n', 'c', 'e', ':', ' ', '1', '0', '.', '6', ' ', '\\t', '$', 'J', '_', '{', 't', 'r', 'a', 'i', 'n', '}', '$', ':', ' ', '1', '0', '.', '8', '%', ' ', '\\t', '$', 'J', '_', '{', 'c', 'v', '}', '$', ':', ' ', '1', '4', '.', '8', '%', ' ', ' ', 'W', 'h', 'y', ' ', 'i', 's', ' ', 'h', 'u', 'm', 'a', 'n', ' ', 'l', 'e', 'v', 'e', 'l', ' ', 'e', 'r', 'r', 'o', 'r', ' ', 's', 'o', ' ', 'h', 'i', 'g', 'h', '?', ' ', 'T', 'h', 'e', 'r', 'e', ' ', 'i', 's', ' ', 'l', 'o', 't', 's', ' ', 'o', 'f', ' ', 'n', 'o', 'i', 's', 'e', ' ', 'i', 'n', ' ', 't', 'h', 'e', ' ', 'a', 'u', 'd', 'i', 'o', '.', ' ', 'I', 't', ' ', 's', 'e', 'e', 'm', 's', ' ', 'u', 'n', 'f', 'a', 'i', 'r', ' ', 't', 'o', ' ', 'e', 'x', 'p', 'e', 'c', 't', ' ', 'a', ' ', 'l', 'e', 'a', 'r', 'n', 'i', 'n', 'g', ' ', 'a', 'l', 'g', 'o', 'r', 'i', 't', 'h', 'm', ' ', 't', 'o', ' ', 'd', 'o', ' ', 'm', 'u', 'c', 'h', ' ', 'b', 'e', 't', 't', 'e', 'r', '.', ' ', 'I', 't', ' ', 'i', 's', ' ', 't', 'h', 'u', 's', ' ', 'i', 's', ' ', 'm', 'o', 'r', 'e', ' ', 'u', 's', 'e', 'f', 'u', 'l', ' ', 't', 'o', ' ', 'm', 'e', 'a', 's', 'u', 'r', 'e', ' ', 't', 'h', 'e', ' ', 't', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ' ', 'e', 'r', 'r', 'o', 'r', ' ', 'a', 'g', 'a', 'i', 'n', 's', 't', ' ', 't', 'h', 'e', ' ', 'h', 'u', 'm', 'a', 'n', ' ', 'e', 'r', 'r', 'o', 'r', '.', ' ', 'S', 'o', ',', ' ', 'l', 'o', 'o', 'k', 'i', 'n', 'g', ' ', 'a', 't', ' ', 't', 'h', 'e', 's', 'e', ' ', 'r', 'e', 's', 'u', 'l', 't', 's', ',', ' ', '$', 'J', '_', '{', 't', 'r', 'a', 'i', 'n', '}', '$', ' ', 'i', 's', ' ', 'o', 'n', 'l', 'y', ' ', '0', '.', '2', '$', ' ', 'h', 'i', 'g', 'h', 'e', 'r', ' ', 't', 'h', 'a', 'n', ' ', 't', 'h', 'e', ' ', 'h', 'u', 'm', 'a', 'n', ' ', 'l', 'e', 'v', 'e', 'l', ' ', 'p', 'e', 'r', 'f', 'o', 'r', 'm', 'a', 'n', 'c', 'e', ' ', 'w', 'h', 'e', 'r', 'e', 'a', 's', ' ', '$', 'J', '_', '{', 'c', 'v', '}', '$', ' ', 'i', 's', ' ', 'a', ' ', 'f', 'u', 'l', 'l', ' ', '4', '.', '2', '%', ' ', 'h', 'i', 'g', 'h', 'e', 'r', '.', ' ', 'W', 'e', ' ', 'c', 'a', 'n', ' ', 't', 'h', 'u', 's', ' ', 'c', 'o', 'n', 'c', 'l', 'u', 'd', 'e', ' ', 't', 'h', 'a', 't', ' ', 't', 'h', 'i', 's', ' ', 'a', 'l', 'g', 'o', 'r', 'i', 't', 'h', 'm', ' ', 'h', 'a', 's', ' ', 'm', 'o', 'r', 'e', ' ', 'o', 'f', ' ', 'a', ' ', 'v', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', 'p', 'r', 'o', 'b', 'l', 'e', 'm', ' ', 't', 'h', 'a', 'n', ' ', 'a', ' ', 'b', 'i', 'a', 's', ' ', 'p', 'r', 'o', 'b', 'l', 'e', 'm', '.', ' ', '#', '#', '#', ' ', 'E', 's', 't', 'a', 'b', 'l', 'i', 's', 'h', 'i', 'n', 'g', ' ', 'a', ' ', 'b', 'a', 's', 'e', 'l', 'i', 'n', 'e', ' ', 'l', 'e', 'v', 'e', 'l', ' ', 'o', 'f', ' ', 'p', 'e', 'r', 'f', 'o', 'r', 'm', 'a', 'n', 'c', 'e', ' ', 'W', 'h', 'a', 't', ' ', 'i', 's', ' ', 't', 'h', 'e', ' ', 'l', 'e', 'v', 'e', 'l', ' ', 'o', 'f', ' ', 'e', 'r', 'r', 'o', 'r', ' ', 'y', 'o', 'u', ' ', 'c', 'a', 'n', ' ', 'r', 'e', 'a', 's', 'o', 'n', 'a', 'b', 'l', 'y', ' ', 'h', 'o', 'p', 'e', ' ', 't', 'o', ' ', 'g', 'e', 't', ' ', 't', 'o', '?', ' ', '-', ' ', 'H', 'u', 'm', 'a', 'n', ' ', 'l', 'e', 'v', 'e', 'l', ' ', 'p', 'e', 'r', 'f', 'o', 'r', 'm', 'a', 'n', 'c', 'e', ' ', '-', ' ', 'C', 'o', 'm', 'p', 'e', 't', 'i', 'n', 'g', ' ', 'a', 'l', 'g', 'o', 'r', 'i', 't', 'h', 'm', 's', ' ', 'p', 'e', 'r', 'f', 'o', 'r', 'm', 'a', 'n', 'c', 'e', ' ', '-', ' ', 'G', 'u', 'e', 's', 's', ' ', 'b', 'a', 's', 'e', 'd', ' ', 'o', 'n', ' ', 'e', 'x', 'p', 'e', 'r', 'i', 'e', 'n', 'c', 'e', ' ', '#', '#', '#', ' ', 'B', 'i', 'a', 's', ' ', 'V', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', 'E', 'x', 'a', 'm', 'p', 'l', 'e', 's', ' ', 'G', 'a', 'p', ' ', 'b', 'e', 't', 'w', 'e', 'e', 'n', ' ', 'b', 'a', 's', 'e', 'l', 'i', 'n', 'e', ' ', 'a', 'n', 'd', ' ', 't', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ' ', 'e', 'r', 'r', 'o', 'r', ' ', 's', 'h', 'o', 'w', 's', ' ', 'h', 'i', 'g', 'h', ' ', 'b', 'i', 'a', 's', '.', ' ', 'A', ' ', 'g', 'a', 'p', ' ', 'b', 'e', 't', 'w', 'e', 'e', 'n', ' ', 't', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ' ', 'e', 'r', 'r', 'o', 'r', ' ', 'a', 'n', 'd', ' ', 'c', 'r', 'o', 's', 's', ' ', 'v', 'a', 'l', 'i', 'd', 'a', 't', 'i', 'o', 'n', ' ', 'e', 'r', 'r', 'o', 'r', ' ', 's', 'h', 'o', 'w', 's', ' ', 'h', 'i', 'g', 'h', ' ', 'v', 'a', 'r', 'i', 'a', 'n', 'c', 'e', '.', ' ', ' ', 'I', 'f', ' ', 'y', 'o', 'u', 'r', ' ', 'g', 'o', 'a', 'l', ' ', 'i', 's', ' ', 'p', 'e', 'r', 'f', 'e', 'c', 't', 'i', 'o', 'n', ',', ' ', 't', 'h', 'e', ' ', 'b', 'a', 's', 'e', 'l', 'i', 'n', 'e', ' ', 'w', 'o', 'u', 'l', 'd', ' ', 'b', 'e', ' ', 'z', 'e', 'r', 'o', '.', ' ', 'B', 'u', 't', ' ', 'f', 'o', 'r', ' ', 'a', ' ', 'l', 'o', 't', ' ', 'o', 'f', ' ', 'r', 'e', 'a', 'l', ' ', 'w', 'o', 'r', 'l', 'd', ' ', 'e', 'x', 'a', 'm', 'p', 'l', 'e', 's', ',', ' ', 'l', 'i', 'k', 'e', ' ', 'a', 'u', 'd', 'i', 'o', ' ', 'r', 'e', 'c', 'o', 'g', 'n', 'i', 't', 'i', 'o', 'n', ',', ' ', 't', 'h', 'e', 'r', 'e', ' ', 'i', 's', ' ', 'a', ' ', 'l', 'o', 't', ' ', 'o', 'f', ' ', 'n', 'o', 'i', 's', 'e', ' ', 'i', 'n', ' ', 't', 'h', 'e', ' ', 'd', 'a', 't', 'a', ',', ' ', 's', 'o', ' ', 'y', 'o', 'u', ' ', 'n', 'e', 'e', 'd', ' ', 'a', ' ', 'h', 'i', 'g', 'h', 'e', 'r', ' ', 'b', 'a', 's', 'e', 'l', 'i', 'n', 'e', '.', ' ', ' ', 'I', 'f', ' ', 't', 'h', 'e', 'r', 'e', ' ', 'i', 's', ' ', 'a', ' ', 'g', 'a', 'p', ' ', 'b', 'e', 't', 'w', 'e', 'e', 'n', ' ', 'a', 'l', 'l', ' ', 't', 'h', 'r', 'e', 'e', ' ', 'i', 's', ' ', 'h', 'i', 'g', 'h', ' ', 'y', 'o', 'u', ' ', 'h', 'a', 'v', 'e', ' ', 'b', 'o', 't', 'h', ' ', 'h', 'i', 'g', 'h', ' ', 'b', 'i', 'a', 's', ' ', 'a', 'n', 'd', ' ', 'h', 'i', 'g', 'h', ' ', 'v', 'a', 'r', 'i', 'a', 'n', 'c', 'e', '.', ' ', '#', '#', '#', '#', ' ', 'S', 'l', 'i', 'd', 'e', ' ', '!', 'S', 'c', 'r', 'e', 'e', 'n', 's', 'h', 'o', 't', ' ', '2', '0', '2', '3', '-', '1', '0', '-', '1', '7', ' ', 'a', 't', ' ', '4', '.', '4', '2', '.', '4', '9', ' ', 'P', 'M', '.', 'p', 'n', 'g', ' ', ' ', '#', '#', ' ', 'L', 'e', 'a', 'r', 'n', 'i', 'n', 'g', ' ', 'C', 'u', 'r', 'v', 'e', 's', ' ', ' ', 'N', 'o', 't', 'e', 'd', ' ', 'i', 'n', ' ', 'L', 'e', 'a', 'r', 'n', 'i', 'n', 'g', ' ', 'C', 'u', 'r', 'v', 'e', 's', ' ', '#', '#', '#', ' ', 'O', 'v', 'e', 'r', 'v', 'i', 'e', 'w', ' ', 'L', 'e', 'a', 'r', 'n', 'i', 'n', 'g', ' ', 'c', 'u', 'r', 'v', 'e', 's', ' ', 'h', 'e', 'l', 'p', ' ', 'u', 'n', 'd', 'e', 'r', 's', 't', 'a', 'n', 'd', ' ', 'h', 'o', 'w', ' ', 'y', 'o', 'u', 'r', ' ', 'l', 'e', 'a', 'r', 'n', 'i', 'n', 'g', ' ', 'a', 'l', 'g', 'o', 'r', 'i', 't', 'h', 'm', ' ', 'i', 's', ' ', 'd', 'o', 'i', 'n', 'g', ' ', 'a', 's', ' ', 'a', ' ', 'f', 'u', 'n', 'c', 't', 'i', 'o', 'n', ' ', 'o', 'f', ' ', 't', 'h', 'e', ' ', 'a', 'm', 'o', 'u', 'n', 't', ' ', 'o', 'f', ' ', 'e', 'x', 'p', 'e', 'r', 'i', 'e', 'n', 'c', 'e', ' ', 'i', 't', ' ', 'h', 'a', 's', '.', ' ', 'E', 'x', 'p', 'e', 'r', 'i', 'e', 'n', 'c', 'e', ' ', 'b', 'e', 'i', 'n', 'g', ' ', 't', 'h', 'e', ' ', 'n', 'u', 'm', 'b', 'e', 'r', ' ', 'o', 'f', ' ', 't', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ' ', 'e', 'x', 'a', 'm', 'p', 'l', 'e', 's', ' ', 'i', 't', ' ', 'h', 'a', 's', '.', ' ', ' ', 'T', 'h', 'e', ' ', 'b', 'i', 'g', 'g', 'e', 'r', ' ', 't', 'h', 'e', ' ', 't', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ' ', 's', 'e', 't', ' ', 't', 'h', 'e', ' ', 'h', 'a', 'r', 'd', 'e', 'r', ' ', 'i', 't', ' ', 'i', 's', ' ', 't', 'o', ' ', 'f', 'i', 't', ' ', 'a', 'l', 'l', ' ', 'e', 'x', 'a', 'm', 'p', 'l', 'e', 's', ' ', 'p', 'e', 'r', 'f', 'e', 'c', 't', 'l', 'y', '.', ' ', 'T', 'h', 'u', 's', ';', ' ', 'a', 's', ' ', 't', 'h', 'e', ' ', 't', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ' ', 's', 'e', 't', ' ', 'i', 'n', 'c', 'r', 'e', 'a', 's', 'e', 's', ' ', 's', 'o', ' ', 'd', 'o', 'e', 's', ' ', 't', 'h', 'e', ' ', 't', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ' ', 'e', 'r', 'r', 'o', 'r', ' ', '$', 'J', '_', '{', 't', 'r', 'a', 'i', 'n', '}', '(', '\\\\', 'v', 'e', 'c', '{', 'w', '}', ',', 'b', ')', '$', ' ', ' ', ' ', 'P', 'l', 'o', 't', 't', 'i', 'n', 'g', ' ', 'a', ' ', 'l', 'e', 'a', 'r', 'n', 'i', 'n', 'g', ' ', 'c', 'u', 'r', 'v', 'e', ' ', 'b', 'y', ' ', 't', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ' ', 'd', 'i', 'f', 'f', 'e', 'r', 'e', 'n', 't', ' ', 'm', 'o', 'd', 'e', 'l', 's', ' ', 'b', 'a', 's', 'e', 'd', ' ', 'o', 'n', ' ', 'd', 'i', 'f', 'f', 'e', 'r', 'e', 'n', 't', ' ', 's', 'u', 'b', 's', 'e', 't', 's', ' ', 'o', 'f', ' ', 't', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ' ', 'd', 'a', 't', 'a', ' ', 'i', 's', ' ', 'c', 'o', 'm', 'p', 'u', 't', 'a', 't', 'i', 'o', 'n', 'a', 'l', 'l', 'y', ' ', 'e', 'x', 'p', 'e', 'n', 's', 'i', 'v', 'e', ',', ' ', 's', 'o', ' ', 'i', 'n', ' ', 'p', 'r', 'a', 'c', 't', 'i', 'c', 'e', ' ', 'i', 't', ' ', 'i', 's', 'n', \"'\", 't', ' ', 'd', 'o', 'n', 'e', ' ', 't', 'h', 'a', 't', ' ', 'o', 'f', 't', 'e', 'n', '.', ' ', 'B', 'u', 't', ',', ' ', 'i', 't', \"'\", 's', ' ', 'a', ' ', 'g', 'o', 'o', 'd', ' ', 'm', 'e', 'n', 't', 'a', 'l', ' ', 'v', 'i', 's', 'u', 'a', 'l', 'i', 'z', 'a', 't', 'i', 'o', 'n', '.', ' ', '#', '#', '#', '#', ' ', 'S', 'l', 'i', 'd', 'e', 's', ' ', '!', 'S', 'c', 'r', 'e', 'e', 'n', 's', 'h', 'o', 't', ' ', '2', '0', '2', '3', '-', '1', '0', '-', '1', '7', ' ', 'a', 't', ' ', '5', '.', '4', '0', '.', '0', '4', ' ', 'P', 'M', '.', 'p', 'n', 'g', ' ', ' ', ' ', ' ', '#', '#', '#', ' ', 'H', 'i', 'g', 'h', ' ', 'B', 'i', 'a', 's', ' ', 'E', 'x', 'a', 'm', 'p', 'l', 'e', ' ', 'I', 'f', ' ', 'a', ' ', 'l', 'e', 'a', 'r', 'n', 'i', 'n', 'g', ' ', 'a', 'l', 'g', 'o', 'r', 'i', 't', 'h', 'm', ' ', 's', 'u', 'f', 'f', 'e', 'r', 's', ' ', 'f', 'r', 'o', 'm', ' ', 'h', 'i', 'g', 'h', ' ', 'b', 'i', 'a', 's', ',', ' ', 'g', 'e', 't', 't', 'i', 'n', 'g', ' ', 'm', 'o', 'r', 'e', ' ', 't', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ' ', 'd', 'a', 't', 'a', ' ', 'w', 'i', 'l', 'l', ' ', 'n', 'o', 't', ' ', '(', 'b', 'y', ' ', 'i', 't', 's', 'e', 'l', 'f', ')', ' ', 'h', 'e', 'l', 'p', ' ', 't', 'h', 'a', 't', ' ', 'm', 'u', 'c', 'h', '.', ' ', '#', '#', '#', '#', ' ', 'S', 'l', 'i', 'd', 'e', ' ', 'f', 'o', 'r', ' ', 'H', 'i', 'g', 'h', ' ', 'B', 'i', 'a', 's', ' ', '!', 'S', 'c', 'r', 'e', 'e', 'n', 's', 'h', 'o', 't', ' ', '2', '0', '2', '3', '-', '1', '0', '-', '1', '7', ' ', 'a', 't', ' ', '5', '.', '4', '4', '.', '2', '9', ' ', 'P', 'M', '.', 'p', 'n', 'g', ' ', ' ', '#', '#', '#', ' ', 'H', 'i', 'g', 'h', ' ', 'V', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', 'E', 'x', 'a', 'm', 'p', 'l', 'e', ' ', 'I', 'f', ' ', 'a', ' ', 'l', 'e', 'a', 'r', 'n', 'i', 'n', 'g', ' ', 'a', 'l', 'g', 'o', 'r', 'i', 't', 'h', 'm', ' ', 's', 'u', 'f', 'f', 'e', 'r', 's', ' ', 'f', 'r', 'o', 'm', ' ', 'h', 'i', 'g', 'h', ' ', 'v', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ',', ' ', 'g', 'e', 't', 't', 'i', 'n', 'g', ' ', 'm', 'o', 'r', 'e', ' ', 't', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ' ', 'd', 'a', 't', 'a', ' ', 'i', 's', ' ', 'l', 'i', 'k', 'e', 'l', 'y', ' ', 't', 'o', ' ', 'h', 'e', 'l', 'p', '.', ' ', '#', '#', '#', '#', ' ', 'S', 'l', 'i', 'd', 'e', ' ', 'f', 'o', 'r', ' ', 'H', 'i', 'g', 'h', ' ', 'V', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', '!', 'S', 'c', 'r', 'e', 'e', 'n', 's', 'h', 'o', 't', ' ', '2', '0', '2', '3', '-', '1', '0', '-', '1', '7', ' ', 'a', 't', ' ', '5', '.', '4', '9', '.', '4', '5', ' ', 'P', 'M', '.', 'p', 'n', 'g', ' ', ' ', ' ', '#', '#', ' ', 'D', 'e', 'c', 'i', 'd', 'i', 'n', 'g', ' ', 'w', 'h', 'a', 't', ' ', 't', 'o', ' ', 't', 'r', 'y', ' ', 'n', 'e', 'x', 't', ' ', 'r', 'e', 'v', 'i', 's', 'i', 't', 'e', 'd', ' ', '#', '#', '#', ' ', 'E', 'x', 'a', 'm', 'p', 'l', 'e', 's', ' ', 'w', 'h', 'e', 'n', ' ', 'D', 'e', 'b', 'u', 'g', 'g', 'i', 'n', 'g', ' ', 'a', 'n', ' ', 'A', 'l', 'g', 'o', 'r', 'i', 't', 'h', 'm', ':', ' ', 'G', 'e', 't', ' ', 'm', 'o', 'r', 'e', ' ', 't', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ' ', 'e', 'x', 'a', 'm', 'p', 'l', 'e', 's', ':', ' ', 'f', 'i', 'x', 'e', 's', ' ', 'h', 'i', 'g', 'h', ' ', 'v', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', 'T', 'r', 'y', ' ', 's', 'm', 'a', 'l', 'l', 'e', 'r', ' ', 's', 'e', 't', ' ', 'o', 'f', ' ', 'f', 'e', 'a', 't', 'u', 'r', 'e', 's', ':', ' ', 'f', 'i', 'x', 'e', 's', ' ', 'h', 'i', 'g', 'h', ' ', 'v', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', 'T', 'r', 'y', ' ', 'g', 'e', 't', 't', 'i', 'n', 'g', ' ', 'a', 'd', 'd', 'i', 't', 'i', 'o', 'n', 'a', 'l', ' ', 'f', 'e', 'a', 't', 'u', 'r', 'e', 's', ':', ' ', 'f', 'i', 'x', 'e', 's', ' ', 'h', 'i', 'g', 'h', ' ', 'b', 'i', 'a', 's', ' ', '\\t', 'E', 'x', 'a', 'm', 'p', 'l', 'e', 's', ';', ' ', 'a', 'n', ' ', 'a', 'l', 'g', 'o', 'r', 'i', 't', 'h', 'm', ' ', 't', 'h', 'a', 't', ' ', 'l', 'a', 'c', 'k', 's', ' ', 'i', 'n', 'f', 'o', 'r', 'm', 'a', 't', 'i', 'o', 'n', ' ', 'w', 'o', 'n', 't', ' ', 'e', 'v', 'e', 'n', ' ', 'd', 'o', ' ', 'w', 'e', 'l', 'l', ' ', 'o', 'n', ' ', 't', 'h', 'e', ' ', 't', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ' ', 's', 'e', 't', ' ', 'A', 'd', 'd', 'i', 'n', 'g', ' ', 'p', 'o', 'l', 'y', 'n', 'o', 'm', 'i', 'a', 'l', ' ', 'f', 'e', 'a', 't', 'u', 'r', 'e', 's', ' ', '(', '$', 'x', '_', '1', '^', '2', ',', 'X', '_', '2', '^', '2', ',', ' ', 'e', 't', 'c', '$', ')', ':', ' ', 'F', 'i', 'x', 'e', 's', ' ', 'h', 'i', 'g', 'h', ' ', 'b', 'i', 'a', 's', ' ', 'D', 'e', 'c', 'r', 'e', 'a', 's', 'i', 'n', 'g', ' ', '$', '\\\\', 'l', 'a', 'm', 'b', 'd', 'a', '$', ' ', 'F', 'i', 'x', 'e', 's', ' ', 'h', 'i', 'g', 'h', ' ', 'b', 'i', 'a', 's', ' ', 'I', 'n', 'c', 'r', 'e', 'a', 's', 'i', 'n', 'g', ' ', '$', '\\\\', 'l', 'a', 'm', 'b', 'd', 'a', '$', ' ', 'F', 'i', 'x', 'e', 's', ' ', 'h', 'i', 'g', 'h', ' ', 'v', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', '\\t', 'F', 'o', 'r', 'c', 'e', 's', ' ', 't', 'h', 'e', ' ', 'a', 'l', 'g', 'o', 'r', 'i', 't', 'h', 'm', ' ', 't', 'o', ' ', 'f', 'o', 'r', 'c', 'e', ' ', 'a', ' ', 's', 'm', 'o', 'o', 't', 'h', 'e', 'r', ' ', 'f', 'u', 'n', 'c', 't', 'i', 'o', 'n', '.', ' ', ' ', '*', '*', 'N', 'o', 't', 'e', '!', '*', '*', ' ', 'D', 'o', 'n', \"'\", 't', ' ', 'r', 'a', 'n', 'd', 'o', 'm', 'l', 'y', ' ', 't', 'h', 'r', 'o', 'w', ' ', 'a', 'w', 'a', 'y', ' ', 't', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ' ', 'e', 'x', 'a', 'm', 'p', 'l', 'e', 's', ' ', 'j', 'u', 's', 't', ' ', 't', 'o', ' ', 'f', 'i', 'x', ' ', 'a', ' ', 'h', 'i', 'g', 'h', ' ', 'b', 'i', 'a', 's', ' ', 'p', 'r', 'o', 'b', 'l', 'e', 'm', '.', ' ', '#', '#', '#', ' ', 'T', 'a', 'k', 'e', 'a', 'w', 'a', 'y', ' ', '#', 'm', 'e', 'r', 'g', 'e', ' ', 'w', 'i', 't', 'h', ' ', 'B', 'i', 'a', 's', ' ', 'a', 'n', 'd', ' ', 'V', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', '?', ' ', 'I', 'f', ' ', 'y', 'o', 'u', 'r', ' ', 'a', 'l', 'g', 'o', 'r', 'i', 't', 'h', 'm', ' ', 'h', 'a', 's', ' ', 'h', 'i', 'g', 'h', ' ', 'v', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ',', ' ', 't', 'r', 'y', ' ', 's', 'i', 'm', 'p', 'l', 'i', 'f', 'y', 'i', 'n', 'g', ' ', 'y', 'o', 'u', 'r', ' ', 'm', 'o', 'd', 'e', 'l', ' ', 'o', 'r', ' ', 'g', 'e', 't', 't', 'i', 'n', 'g', ' ', 'm', 'o', 'r', 'e', ' ', 't', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ' ', 'd', 'a', 't', 'a', '.', ' ', 'S', 'i', 'm', 'p', 'l', 'i', 'f', 'i', 'c', 'a', 't', 'i', 'o', 'n', ' ', 'c', 'a', 'n', ' ', 'm', 'e', 'a', 'n', ' ', 'a', ' ', 's', 'm', 'a', 'l', 'l', 'e', 'r', ' ', 's', 'e', 't', ' ', 'o', 'f', ' ', 'f', 'e', 'a', 't', 'u', 'r', 'e', 's', ' ', 'o', 'r', ' ', 'a', 'n', ' ', 'i', 'n', 'c', 'r', 'e', 'a', 's', 'e', 'd', ' ', 'r', 'e', 'g', 'u', 'l', 'a', 'r', 'i', 'z', 'a', 't', 'i', 'o', 'n', ' ', 'I', 'f', ' ', 'y', 'o', 'u', 'r', ' ', 'a', 'l', 'g', 'o', 'r', 'i', 't', 'h', 'm', ' ', 'h', 'a', 's', ' ', 'h', 'i', 'g', 'h', ' ', 'b', 'i', 'a', 's', ',', ' ', 't', 'h', 'a', 't', ' ', 'i', 's', ' ', 't', 'o', ' ', 's', 'a', 'y', ' ', 'i', 't', 's', ' ', 'n', 'o', 't', ' ', 'e', 'v', 'e', 'n', ' ', 'd', 'o', 'i', 'n', 'g', ' ', 'w', 'e', 'l', 'l', ' ', 'o', 'n', ' ', 't', 'h', 'e', ' ', 't', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ' ', 's', 'e', 't', ',', ' ', 'y', 'o', 'u', ' ', 'm', 'a', 'i', 'n', 'l', 'y', ' ', 'n', 'e', 'e', 'd', ' ', 't', 'o', ' ', 'm', 'a', 'k', 'e', ' ', 'y', 'o', 'u', 'r', ' ', 'm', 'o', 'd', 'e', 'l', ' ', 'm', 'o', 'r', 'e', ' ', 'p', 'o', 'w', 'e', 'r', 'f', 'u', 'l', ' ', 'a', 'n', 'd', ' ', 'f', 'l', 'e', 'x', 'i', 'b', 'l', 'e', ' ', 't', 'o', ' ', 'f', 'i', 't', ' ', 'm', 'o', 'r', 'e', ' ', 'c', 'o', 'm', 'p', 'l', 'e', 'x', ' ', 'f', 'u', 'n', 'c', 't', 'i', 'o', 'n', 's', '.', ' ', 'T', 'o', ' ', 'd', 'o', ' ', 's', 'o', ' ', 'y', 'o', 'u', ' ', 'c', 'a', 'n', ' ', 'g', 'i', 'v', 'e', ' ', 'i', 't', ' ', 'a', 'd', 'd', 'i', 't', 'i', 'o', 'n', 'a', 'l', ' ', 'f', 'e', 'a', 't', 'u', 'r', 'e', 's', ',', ' ', 'a', 'd', 'd', ' ', 'p', 'o', 'l', 'y', 'n', 'o', 'm', 'i', 'a', 'l', ' ', 'f', 'e', 'a', 't', 'u', 'r', 'e', 's', ',', ' ', 'o', 'r', ' ', 'd', 'e', 'c', 'r', 'e', 'a', 's', 'e', ' ', '$', '\\\\', 'l', 'a', 'm', 'b', 'd', 'a', '$', ' ', ' ', '#', '#', ' ', 'B', 'i', 'a', 's', '/', 'V', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', 'a', 'n', 'd', ' ', 'N', 'e', 'u', 'r', 'a', 'l', ' ', 'N', 'e', 't', 'w', 'o', 'r', 'k', 's', ' ', '#', '#', '#', ' ', 'T', 'h', 'e', ' ', 'b', 'i', 'a', 's', ' ', 'v', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', 't', 'r', 'a', 'd', 'e', 'o', 'f', 'f', ' ', 'S', 'i', 'm', 'p', 'l', 'e', ' ', 'm', 'o', 'd', 'e', 'l', ' ', '=', ' ', 'h', 'i', 'g', 'h', ' ', 'b', 'i', 'a', 's', ' ', 'C', 'o', 'm', 'p', 'l', 'e', 'x', ' ', 'm', 'o', 'd', 'e', 'l', ' ', '=', ' ', 'h', 'i', 'g', 'h', ' ', 'v', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', 'B', 'e', 'f', 'o', 'r', 'e', ' ', 'n', 'e', 'u', 'r', 'a', 'l', ' ', 'n', 'e', 't', 'w', 'o', 'r', 'k', 's', ',', ' ', 'w', 'e', ' ', 'h', 'a', 'd', ' ', 't', 'o', ' ', 'w', 'o', 'r', 'r', 'y', ' ', 'a', 'b', 'o', 'u', 't', ' ', 'b', 'a', 'l', 'a', 'n', 'c', 'i', 'n', 'g', ' ', 't', 'h', 'i', 's', ' ', 'c', 'o', 'm', 'p', 'l', 'e', 'x', 'i', 't', 'y', ' ', 'b', 'e', 't', 'w', 'e', 'e', 'n', ' ', 'b', 'i', 'a', 's', ' ', 'a', 'n', 'd', ' ', 'v', 'a', 'r', 'i', 'a', 'n', 'c', 'e', '.', ' ', 'W', 'i', 't', 'h', ' ', 'n', 'e', 'u', 'r', 'a', 'l', ' ', 'n', 'e', 't', 'w', 'o', 'r', 'k', 's', ' ', 'w', 'e', ' ', 'n', 'o', 'w', ' ', 'a', 'r', 'e', ' ', 'm', 'o', 's', 't', 'l', 'y', ' ', 'w', 'o', 'r', 'r', 'i', 'e', 'd', ' ', 'a', 'b', 'o', 'u', 't', ' ', 'h', 'i', 'g', 'h', ' ', 'v', 'a', 'r', 'i', 'a', 'n', 'c', 'e', '.', ' ', ' ', '#', '#', '#', ' ', 'N', 'e', 'u', 'r', 'a', 'l', ' ', 'N', 'e', 't', 'w', 'o', 'r', 'k', 's', ' ', 'a', 'n', 'd', ' ', 'b', 'i', 'a', 's', ' ', 'v', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', 'L', 'a', 'r', 'g', 'e', ' ', 'N', 'e', 'u', 'r', 'a', 'l', ' ', 'N', 'e', 't', 'w', 'o', 'r', 'k', '|', 'n', 'e', 'u', 'r', 'a', 'l', ' ', 'n', 'e', 't', 'w', 'o', 'r', 'k', 's', ' ', 'a', 'r', 'e', ' ', 'l', 'o', 'w', ' ', 'b', 'i', 'a', 's', ' ', 'm', 'a', 'c', 'h', 'i', 'n', 'e', 's', '.', ' ', 'I', 'f', ' ', 'y', 'o', 'u', ' ', 'm', 'a', 'k', 'e', ' ', 'y', 'o', 'u', 'r', ' ', 'n', 'e', 'u', 'r', 'a', 'l', ' ', 'n', 'e', 't', 'w', 'o', 'r', 'k', ' ', 'l', 'a', 'r', 'g', 'e', ' ', 'e', 'n', 'o', 'u', 'g', 'h', ' ', 'y', 'o', 'u', ' ', 'c', 'a', 'n', ' ', 'a', 'l', 'm', 'o', 's', 't', ' ', 'a', 'l', 'w', 'a', 'y', 's', ' ', 'f', 'i', 't', ' ', 'y', 'o', 'u', 'r', ' ', 't', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ' ', 's', 'e', 't', ' ', 'w', 'e', 'l', 'l', '.', ' ', '#', '#', '#', '#', ' ', 'R', 'e', 'c', 'i', 'p', 'e', ' ', 'f', 'o', 'r', ' ', 'd', 'e', 'c', 'r', 'e', 'a', 's', 'i', 'n', 'g', ' ', 'b', 'i', 'a', 's', ' ', 'w', 'i', 't', 'h', ' ', 'a', ' ', 'n', 'e', 'u', 'r', 'a', 'l', ' ', 'n', 'e', 't', 'w', 'o', 'r', 'k', ' ', '1', '.', ' ', 'T', 'r', 'a', 'i', 'n', ' ', 'a', ' ', 'n', 'e', 'u', 'r', 'a', 'l', ' ', 'n', 'e', 't', 'w', 'o', 'r', 'k', ' ', '2', '.', ' ', ' ', 'I', 'f', ' ', 't', 'h', 'e', ' ', 't', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ' ', 's', 'e', 't', ' ', 'e', 'r', 'r', 'o', 'r', ' ', '$', 'J', '_', '{', 't', 'r', 'a', 'i', 'n', '}', '(', '\\\\', 'v', 'e', 'c', '{', 'w', '}', ',', 'b', ')', '$', ' ', 'i', 's', ' ', 'h', 'i', 'g', 'h', ' ', 'r', 'e', 'l', 'a', 't', 'i', 'v', 'e', ' ', 't', 'o', ' ', 'y', 'o', 'u', 'r', ' ', 'b', 'a', 's', 'e', 'l', 'i', 'n', 'e', ',', ' ', 'i', 'n', 'c', 'r', 'e', 'a', 's', 'e', ' ', 't', 'h', 'e', ' ', 's', 'i', 'z', 'e', ' ', 'o', 'f', ' ', 't', 'h', 'e', ' ', 'n', 'e', 'u', 'r', 'a', 'l', ' ', 'n', 'e', 't', 'w', 'o', 'r', 'k', ' ', 'b', 'y', ' ', 'a', 'd', 'd', 'i', 'n', 'g', ' ', 'h', 'i', 'd', 'd', 'e', 'n', ' ', 'l', 'a', 'y', 'e', 'r', 's', '.', ' ', '3', '.', ' ', 'O', 'n', 'c', 'e', ' ', '$', 'J', '_', '{', 't', 'r', 'a', 'i', 'n', '}', '(', '\\\\', 'v', 'e', 'c', '{', 'w', '}', ',', 'b', ')', '$', ' ', 'i', 's', ' ', 'l', 'o', 'w', ' ', 'e', 'n', 'o', 'u', 'g', 'h', ',', ' ', 's', 'e', 'e', ' ', 'i', 'f', ' ', 'i', 't', ' ', 'd', 'o', 'e', 's', ' ', 'w', 'e', 'l', 'l', ' ', 'o', 'n', ' ', 't', 'h', 'e', ' ', 'c', 'r', 'o', 's', 's', ' ', 'v', 'a', 'l', 'i', 'd', 'a', 't', 'i', 'o', 'n', ' ', 's', 'e', 't', ' ', '4', '.', ' ', ' ', 'I', 'f', ' ', 't', 'h', 'e', ' ', 'c', 'r', 'o', 's', 's', ' ', 'v', 'a', 'l', 'i', 'd', 'a', 't', 'i', 'o', 'n', ' ', 's', 'e', 't', ' ', '$', 'J', '_', '{', 'c', 'v', '}', '(', '\\\\', 'v', 'e', 'c', '{', 'w', '}', ',', 'b', ')', '$', ' ', 'i', 's', ' ', 't', 'o', 'o', ' ', 'h', 'i', 'g', 'h', ',', ' ', 'a', 'd', 'd', ' ', 'm', 'o', 'r', 'e', ' ', 'd', 'a', 't', 'a', ',', ' ', 't', 'h', 'e', 'n', ' ', 't', 'e', 's', 't', ' ', 'a', 'g', 'a', 'i', 'n', ' ', 'f', 'r', 'o', 'm', ' ', 's', 't', 'e', 'p', ' ', '2', '.', ' ', '5', '.', ' ', 'R', 'e', 'p', 'e', 'a', 't', ' ', 'u', 'n', 't', 'i', 'l', ' ', '$', 'J', '_', '{', 'c', 'v', '}', '(', '\\\\', 'v', 'e', 'c', '{', 'w', '}', ',', 'b', ')', '$', ' ', ' ', 'i', 's', ' ', 'l', 'o', 'w', ' ', 'e', 'n', 'o', 'u', 'g', 'h', ' ', 'f', 'o', 'r', ' ', 'y', 'o', 'u', 'r', ' ', 'l', 'i', 'k', 'i', 'n', 'g', '.', ' ', '#', '#', '#', '#', '#', ' ', 'S', 'l', 'i', 'd', 'e', ' ', 'I', 'l', 'l', 'u', 's', 't', 'r', 'a', 't', 'i', 'o', 'n', ' ', '!', 'S', 'c', 'r', 'e', 'e', 'n', 's', 'h', 'o', 't', ' ', '2', '0', '2', '3', '-', '1', '0', '-', '1', '7', ' ', 'a', 't', ' ', '6', '.', '3', '8', '.', '4', '6', ' ', 'P', 'M', '.', 'p', 'n', 'g', ' ', '#', '#', '#', ' ', 'L', 'i', 'm', 'i', 't', 'a', 't', 'i', 'o', 'n', 's', ' ', 'a', 'n', 'd', ' ', 'N', 'o', 't', 'e', 's', ' ', 'B', 'i', 'g', 'g', 'e', 'r', ' ', 'n', 'e', 't', 'w', 'o', 'r', 'k', 's', ' ', 'a', 'r', 'e', ' ', 'r', 'e', 's', 't', 'r', 'i', 'c', 't', 'e', 'd', ' ', 'b', 'y', ' ', 'y', 'o', 'u', 'r', ' ', 'c', 'o', 'm', 'p', 'u', 't', 'i', 'n', 'g', ' ', 'p', 'o', 'w', 'e', 'r', ',', ' ', 'd', 'a', 't', 'a', ' ', 'i', 's', ' ', 'r', 'e', 's', 't', 'r', 'i', 'c', 't', 'e', 'd', ' ', 't', 'o', ' ', 't', 'h', 'e', ' ', 'a', 'm', 'o', 'u', 'n', 't', ' ', 'o', 'f', ' ', 'd', 'a', 't', 'a', ' ', 'y', 'o', 'u', ' ', 'h', 'a', 'v', 'e', '.', ' ', ' ', 'S', 'o', 'm', 'e', 't', 'i', 'm', 'e', 's', ' ', 'y', 'o', 'u', ' ', 'w', 'i', 'l', 'l', ' ', 'p', 'i', 'n', 'g', 'p', 'o', 'n', 'g', ' ', 'b', 'a', 'c', 'k', ' ', 'b', 'e', 't', 'w', 'e', 'e', 'n', ' ', 'h', 'i', 'g', 'h', ' ', 'b', 'i', 'a', 's', ' ', 'a', 'n', 'd', ' ', 'h', 'i', 'g', 'h', ' ', 'v', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', 'a', 's', ' ', 'y', 'o', 'u', ' ', 'm', 'o', 'v', 'e', ' ', 't', 'h', 'r', 'o', 'u', 'g', 'h', ' ', 't', 'h', 'i', 's', ' ', 'r', 'e', 'c', 'i', 'p', 'e', ' ', 'a', 'n', 'd', ' ', 'd', 'e', 'v', 'e', 'l', 'o', 'p', ' ', 'a', ' ', 'm', 'a', 'c', 'h', 'i', 'n', 'e', ' ', 'l', 'e', 'a', 'r', 'n', 'i', 'n', 'g', ' ', 'a', 'l', 'g', 'o', 'r', 'i', 't', 'h', 'm', '.', ' ', 'U', 's', 'e', ' ', 't', 'h', 'e', 's', 'e', ' ', 'o', 'b', 's', 'e', 'r', 'v', 'a', 't', 'i', 'o', 'n', 's', ' ', 't', 'o', ' ', 's', 'h', 'a', 'p', 'e', ' ', 'w', 'h', 'a', 't', ' ', 'y', 'o', 'u', ' ', 'd', 'o', ' ', 'n', 'e', 'x', 't', ' ', 'i', 'n', ' ', 't', 'h', 'e', ' ', 'p', 'r', 'o', 'c', 'e', 's', 's', '.', ' ', ' ', '#', '#', '#', ' ', 'N', 'e', 'u', 'r', 'a', 'l', ' ', 'N', 'e', 't', 'w', 'o', 'r', 'k', 's', ' ', 'a', 'n', 'd', ' ', 'R', 'e', 'g', 'u', 'l', 'a', 'r', 'i', 'z', 'a', 't', 'i', 'o', 'n', ' ', 'A', ' ', 'l', 'a', 'r', 'g', 'e', ' ', 'n', 'e', 'u', 'r', 'a', 'l', ' ', 'n', 'e', 't', 'w', 'o', 'r', 'k', ' ', 'w', 'i', 'l', 'l', ' ', 'u', 's', 'u', 'a', 'l', 'l', 'y', ' ', 'd', 'o', ' ', 'a', 's', ' ', 'w', 'e', 'l', 'l', ' ', 'o', 'r', ' ', 'b', 'e', 't', 't', 'e', 'r', ' ', 't', 'h', 'a', 'n', ' ', 'a', ' ', 's', 'm', 'a', 'l', 'l', 'e', 'r', ' ', 'o', 'n', 'e', ' ', 's', 'o', ' ', 'l', 'o', 'n', 'g', ' ', 'a', 's', ' ', 'r', 'e', 'g', 'u', 'l', 'a', 'r', 'i', 'z', 'a', 't', 'i', 'o', 'n', ' ', 'i', 's', ' ', 'c', 'h', 'o', 's', 'e', 'n', ' ', 'a', 'p', 'p', 'r', 'o', 'p', 'r', 'i', 'a', 't', 'e', 'l', 'y', '.', ' ', 'O', 'f', ' ', 'c', 'o', 'u', 'r', 's', 'e', ',', ' ', 'l', 'a', 'r', 'g', 'e', 'r', ' ', 'n', 'e', 'u', 'r', 'a', 'l', ' ', 'n', 'e', 't', 'w', 'o', 'r', 'k', 's', ' ', 'a', 'r', 'e', ' ', 'm', 'o', 'r', 'e', ' ', 'c', 'o', 'm', 'p', 'u', 't', 'a', 't', 'i', 'o', 'n', 'a', 'l', 'l', 'y', ' ', 'e', 'x', 'p', 'e', 'n', 's', 'i', 'v', 'e', '.', ' ', '#', '#', '#', ' ', 'N', 'e', 'u', 'r', 'a', 'l', ' ', 'N', 'e', 't', 'w', 'o', 'r', 'k', ' ', 'R', 'e', 'g', 'u', 'l', 'a', 'r', 'i', 'z', 'a', 't', 'i', 'o', 'n', ' ', '#', 'i', 'm', 'p', 'o', 'r', 't', 'a', 'n', 't', ' ', ' ', '#', 'f', 'u', 'n', 'c', 't', 'i', 'o', 'n', ' ', 'a', 'n', 'd', ' ', 'T', 'e', 'n', 's', 'o', 'r', 'F', 'l', 'o', 'w', ' ', 'i', 'm', 'p', 'l', 'e', 'm', 'e', 'n', 't', 'a', 't', 'i', 'o', 'n', ':', ' ', '!', 'S', 'c', 'r', 'e', 'e', 'n', 's', 'h', 'o', 't', ' ', '2', '0', '2', '3', '-', '1', '0', '-', '1', '7', ' ', 'a', 't', ' ', '6', '.', '4', '6', '.', '2', '8', ' ', 'P', 'M', '.', 'p', 'n', 'g', '*', 'N', 'o', 't', 'e', ' ', 'u', 's', 'u', 'a', 'l', 'l', 'y', ' ', 'd', 'o', 'n', \"'\", 't', ' ', 'r', 'e', 'g', 'u', 'l', 'a', 'r', 'i', 'z', 'e', ' ', 'B', ',', ' ', 'i', 't', ' ', 'd', 'o', 'e', 's', 'n', \"'\", 't', ' ', 'r', 'e', 'a', 'l', 'l', 'y', ' ', 'a', 'f', 'f', 'e', 'c', 't', ' ', 'a', 'n', 'y', 't', 'h', 'i', 'n', 'g', '*', ' ', '#', ' ', 'P', 'y', 't', 'h', 'o', 'n', ' ', 'N', 'o', 't', 'e', 's', '#', '#', ' ', 'K', 'e', 'y', 'w', 'o', 'r', 'd', 's', ' ', 'F', 'r', 'o', 'm', ' ', 'c', 'o', 'd', 'e', 'c', 'a', 'd', 'e', 'm', 'y', ' ', '-', ' ', 'C', 'o', 'n', 't', 'i', 'n', 'u', 'e', ' ', 'K', 'e', 'y', 'w', 'o', 'r', 'd', ':', ' ', 'u', 's', 'e', 'd', ' ', 'i', 'n', 's', 'i', 'd', 'e', ' ', 'a', ' ', 'l', 'o', 'o', 'p', ' ', 't', 'o', ' ', 's', 'k', 'i', 'p', ' ', 't', 'h', 'e', ' ', 'r', 'e', 'm', 'a', 'i', 'n', 'i', 'n', 'g', ' ', 'l', 'o', 'o', 'p', ' ', 'c', 'o', 'd', 'e', ' ', 'b', 'l', 'o', 'c', 'k', ' ', 'a', 'n', 'd', ' ', 'b', 'e', 'g', 'i', 'n', ' ', 't', 'h', 'e', ' ', 'n', 'e', 'x', 't', ' ', 'l', 'o', 'o', 'p', ' ', 'i', 't', 'e', 'r', 'a', 't', 'i', 'o', 'n', '.', ' ', '-', ' ', 'B', 'r', 'e', 'a', 'k', ' ', 'k', 'e', 'y', 'w', 'o', 'r', 'd', ' ', 'e', 's', 'c', 'a', 'p', 'e', 's', ' ', 't', 'h', 'e', ' ', 'l', 'o', 'o', 'p', ',', ' ', 'r', 'e', 'g', 'a', 'r', 'd', 'l', 'e', 's', 's', ' ', 'o', 'f', ' ', 't', 'h', 'e', ' ', 'i', 't', 'e', 'r', 'a', 't', 'i', 'o', 'n', ' ', 'n', 'u', 'm', 'b', 'e', 'r', '.', ' ', 'O', 'n', 'c', 'e', ' ', 'b', 'r', 'e', 'a', 'k', ' ', 'e', 'x', 'e', 'c', 'u', 't', 'e', 's', ' ', 't', 'h', 'e', ' ', 'p', 'r', 'o', 'g', 'r', 'a', 'm', ' ', 'w', 'i', 'l', 'l', ' ', 'c', 'o', 'n', 't', 'i', 'n', 'u', 'e', ' ', 't', 'o', ' ', 'e', 'x', 'e', 'c', 'u', 't', 'e', ' ', 'a', 'f', 't', 'e', 'r', ' ', 't', 'h', 'e', ' ', 'l', 'o', 'o', 'p', '.', '#', '#', '#', ' ', 'D', 'i', 'a', 'g', 'n', 'o', 's', 'i', 'n', 'g', ' ', 'B', 'i', 'a', 's', '/', 'V', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', 'S', 'e', 'e', ' ', 'B', 'i', 'a', 's', ' ', 'a', 'n', 'd', ' ', 'V', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', ' ', 'I', 'f', ' ', 'y', 'o', 'u', ' ', 'h', 'a', 'v', 'e', ' ', 'm', 'o', 'r', 'e', ' ', 'f', 'e', 'a', 't', 'u', 'r', 'e', 's', ',', ' ', 'y', 'o', 'u', ' ', 'c', 'a', 'n', \"'\", 't', ' ', 'v', 'i', 's', 'u', 'a', 'l', 'i', 'z', 'e', ' ', 'B', 'i', 'a', 's', ' ', 'a', 'n', 'd', ' ', 'V', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', ' ', 'A', ' ', 'm', 'o', 'r', 'e', ' ', 's', 'y', 's', 't', 'e', 'm', 'a', 't', 'i', 'c', ' ', 'w', 'a', 'y', ' ', 't', 'o', ' ', 's', 'e', 'e', ' ', 'i', 'f', ' ', 'y', 'o', 'u', ' ', 'h', 'a', 'v', 'e', ' ', 'h', 'i', 'g', 'h', ' ', 'b', 'i', 'a', 's', ' ', 'o', 'r', ' ', 'h', 'i', 'g', 'h', ' ', 'v', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', 'i', 's', ' ', 't', 'o', ' ', 'l', 'o', 'o', 'k', ' ', 'a', 't', ' ', 't', 'h', 'e', ' ', 'p', 'e', 'r', 'f', 'o', 'r', 'm', 'a', 'n', 'c', 'e', ' ', 'o', 'f', ' ', 't', 'h', 'e', ' ', 'a', 'l', 'g', 'o', 'r', 'i', 't', 'h', 'm', ' ', 'o', 'n', ' ', 't', 'h', 'e', ' ', 'T', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ',', ' ', 'T', 'e', 's', 't', ',', ' ', 'a', 'n', 'd', ' ', 'D', 'e', 'v', ' ', 'S', 'e', 't', 's', '|', 't', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ' ', 's', 'e', 't', ' ', 'a', 'n', 'd', ' ', 'd', 'e', 'v', ' ', 's', 'e', 't', ' ', ' ', 'A', ' ', 'c', 'h', 'a', 'r', 'a', 'c', 't', 'e', 'r', 'i', 's', 't', 'i', 'c', ' ', 'o', 'f', ' ', 'a', ' ', 'H', 'i', 'g', 'h', ' ', 'B', 'i', 'a', 's', ' ', '(', 'u', 'n', 'd', 'e', 'r', '-', 'f', 'i', 't', ')', ' ', 'm', 'o', 'd', 'e', 'l', ' ', 'i', 's', ' ', 't', 'h', 'a', 't', ' ', '$', 'J', 't', 'r', 'a', 'i', 'n', '$', ' ', '(', 'c', 'o', 's', 't', ' ', 'o', 'f', ' ', 't', 'h', 'e', ' ', 'T', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ',', ' ', 'T', 'e', 's', 't', ',', ' ', 'a', 'n', 'd', ' ', 'D', 'e', 'v', ' ', 'S', 'e', 't', 's', '|', 't', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ' ', 's', 'e', 't', ')', ' ', 'i', 's', ' ', 'h', 'i', 'g', 'h', '.', ' ', 'A', ' ', 'c', 'h', 'a', 'r', 'a', 'c', 't', 'e', 'r', 'i', 's', 't', 'i', 'c', ' ', 'o', 'f', ' ', 'a', ' ', 'H', 'i', 'g', 'h', ' ', 'V', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', '(', 'o', 'v', 'e', 'r', 'f', 'i', 't', ')', ' ', 'm', 'o', 'd', 'e', 'l', ' ', 'i', 's', ' ', 't', 'h', 'a', 't', ' ', '$', 'J', '_', '{', 'c', 'v', '}', '$', ' ', 'i', 's', ' ', 'h', 'i', 'g', 'h', ' ', 'b', 'u', 't', ' ', '$', 'J', '_', '{', 't', 'r', 'a', 'i', 'n', '}', '$', ' ', 'i', 's', ' ', 'l', 'o', 'w', '.', ' ', '#', '#', '#', '#', ' ', 'S', 'l', 'i', 'd', 'e', ' ', '!', 'S', 'c', 'r', 'e', 'e', 'n', 's', 'h', 'o', 't', ' ', '2', '0', '2', '3', '-', '1', '0', '-', '1', '7', ' ', 'a', 't', ' ', '1', '.', '2', '6', '.', '0', '0', ' ', 'P', 'M', '.', 'p', 'n', 'g', ' ', '#', '#', '#', ' ', 'U', 'n', 'd', 'e', 'r', 's', 't', 'a', 'n', 'd', 'i', 'n', 'g', ' ', 'B', 'i', 'a', 's', ' ', 'a', 'n', 'd', ' ', 'V', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', 'T', 'a', 'k', 'e', ' ', 'd', 'e', 'g', 'r', 'e', 'e', ' ', 'o', 'f', ' ', 'p', 'o', 'l', 'y', 'n', 'o', 'm', 'i', 'a', 'l', ' ', '$', 'd', '$', ' ', 'A', 's', ' ', '$', 'd', '$', ' ', 'i', 'n', 'c', 'r', 'e', 'a', 's', 'e', 's', ',', ' ', 't', 'h', 'a', 't', ' ', 'i', 's', ' ', 't', 'o', ' ', 's', 'a', 'y', ' ', 'w', 'e', ' ', 'a', 'd', 'd', ' ', 'd', 'e', 'g', 'r', 'e', 'e', 's', ' ', 't', 'o', ' ', 't', 'h', 'e', ' ', 'p', 'o', 'l', 'y', 'n', 'o', 'm', 'i', 'a', 'l', ' ', '$', 'J', '_', '{', 't', 'r', 'a', 'i', 'n', '}', '(', '{', '\\\\', 'v', 'e', 'c', '{', 'w', '}', ',', 'b', '}', ')', '$', ' ', ' ', 'w', 'i', 'l', 'l', ' ', 'd', 'e', 'c', 'r', 'e', 'a', 's', 'e', '.', ' ', '$', 'J', '_', '{', 'c', 'v', '}', '(', '\\\\', 'v', 'e', 'c', '{', 'w', '}', ',', 'b', ')', '$', ' ', 'd', 'i', 'p', 's', ' ', 't', 'o', 'w', 'a', 'r', 'd', 's', ' ', 't', 'h', 'e', ' ', 'm', 'i', 'd', 'd', 'l', 'e', ',', ' ', 's', 'h', 'o', 'w', 'i', 'n', 'g', ' ', 't', 'h', 'e', ' ', 'i', 'd', 'e', 'a', 'l', ' ', 'd', 'e', 'g', 'r', 'e', 'e', ' ', 'f', 'o', 'r', ' ', 't', 'h', 'e', ' ', 'p', 'o', 'l', 'y', 'n', 'o', 'm', 'i', 'a', 'l', '.', ' ', ' ', '#', '#', '#', '#', ' ', 'S', 'l', 'i', 'd', 'e', ' ', '!', 'S', 'c', 'r', 'e', 'e', 'n', 's', 'h', 'o', 't', ' ', '2', '0', '2', '3', '-', '1', '0', '-', '1', '7', ' ', 'a', 't', ' ', '1', '.', '2', '9', '.', '2', '1', ' ', 'P', 'M', '.', 'p', 'n', 'g', ' ', '#', '#', '#', ' ', 'T', 'e', 'c', 'h', 'n', 'i', 'q', 'u', 'e', ' ', 'f', 'o', 'r', ' ', 'D', 'i', 'a', 'g', 'n', 'o', 's', 'i', 'n', 'g', ' ', 'B', 'i', 'a', 's', ' ', 'a', 'n', 'd', ' ', 'V', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', 'H', 'o', 'w', ' ', 'd', 'o', ' ', 'y', 'o', 'u', ' ', 't', 'e', 'l', 'l', ' ', 'i', 'f', ' ', 'y', 'o', 'u', 'r', ' ', 'a', 'l', 'g', 'o', 'r', 'i', 't', 'h', 'm', ' ', 'h', 'a', 's', ' ', 'a', ' ', 'h', 'i', 'g', 'h', ' ', 'b', 'i', 'a', 's', ' ', 'o', 'r', ' ', 'v', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', 'p', 'r', 'o', 'b', 'l', 'e', 'm', '?', ' ', ' ', 'O', 'n', 'e', ' ', 'k', 'e', 'y', ' ', 't', 'a', 'k', 'e', 'a', 'w', 'a', 'y', ' ', 'i', 's', ' ', 't', 'h', 'a', 't', ' ', 'h', 'i', 'g', 'h', ' ', 'b', 'i', 'a', 's', ' ', 'm', 'e', 'a', 'n', 's', ' ', 'i', 't', ' ', 'i', 's', ' ', 'n', 'o', 't', ' ', 'e', 'v', 'e', 'n', ' ', 'd', 'o', 'i', 'n', 'g', ' ', 'w', 'e', 'l', 'l', ' ', 'o', 'n', ' ', 't', 'h', 'e', ' ', 't', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ' ', 's', 'e', 't', ' ', 'a', 'n', 'd', ' ', 'h', 'i', 'g', 'h', ' ', 'v', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', 'm', 'e', 'a', 'n', 's', ' ', 'i', 't', ' ', 'd', 'o', 'e', 's', ' ', 'm', 'u', 'c', 'h', ' ', 'w', 'o', 'r', 's', 'e', ' ', 'o', 'n', ' ', 't', 'h', 'e', ' ', 'c', 'r', 'o', 's', 's', ' ', 'v', 'a', 'l', 'i', 'd', 'a', 't', 'i', 'o', 'n', ' ', 's', 'e', 't', ' ', 'a', 'n', 'd', ' ', 't', 'h', 'e', ' ', 't', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ' ', 's', 'e', 't', '.', ' ', '#', '#', '#', '#', ' ', 'H', 'i', 'g', 'h', ' ', 'B', 'i', 'a', 's', ' ', '(', 'u', 'n', 'd', 'e', 'r', '-', 'f', 'i', 't', ')', ' ', 'I', 'n', ' ', 'a', 'n', ' ', 'u', 'n', 'd', 'e', 'r', '-', 'f', 'i', 't', ' ', 'm', 'o', 'd', 'e', 'l', ' ', '$', 'J', '_', '{', 't', 'r', 'a', 'i', 'n', '}', '$', ' ', 'w', 'i', 'l', 'l', ' ', 'b', 'e', ' ', 'h', 'i', 'g', 'h', ' ', 'a', 'n', 'd', ' ', '$', 'J', '_', '{', 't', 'r', 'a', 'i', 'n', '}', ' ', '\\\\', 'a', 'p', 'p', 'r', 'o', 'x', ' ', 'J', '_', '{', 'c', 'v', '}', '$', ' ', ' ', '#', '#', '#', '#', ' ', 'H', 'i', 'g', 'h', ' ', 'V', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', '(', 'o', 'v', 'e', 'r', 'f', 'i', 't', ')', ' ', 'I', 'n', ' ', 'a', 'n', ' ', 'o', 'v', 'e', 'r', '-', 'f', 'i', 't', ' ', 'm', 'o', 'd', 'e', 'l', ' ', '$', 'J', '_', '{', 'c', 'v', '}', ' ', '\\\\', 'g', 'g', ' ', 'J', '_', '{', 't', 'r', 'a', 'i', 'n', '}', '$', ' ', 'a', 'n', 'd', ' ', '$', 'J', '_', '{', 't', 'r', 'a', 'i', 'n', '}', '$', ' ', 'w', 'i', 'l', 'l', ' ', 'b', 'e', ' ', 'l', 'o', 'w', '.', ' ', '#', '#', '#', '#', ' ', 'H', 'i', 'g', 'h', ' ', 'B', 'i', 'a', 's', ' ', 'a', 'n', 'd', ' ', 'H', 'i', 'g', 'h', ' ', 'V', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', 'I', 'n', ' ', 'a', ' ', 'm', 'o', 'd', 'e', 'l', ' ', 'w', 'i', 't', 'h', ' ', 'b', 'o', 't', 'h', ' ', 'h', 'i', 'g', 'h', ' ', 'b', 'i', 'a', 's', ' ', 'a', 'n', 'd', ' ', 'h', 'i', 'g', 'h', ' ', 'v', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', '$', 'J', '_', '{', 't', 'r', 'a', 'i', 'n', '}', '$', ' ', 'w', 'i', 'l', 'l', ' ', 'b', 'e', ' ', 'h', 'i', 'g', 'h', ' ', 'a', 'n', 'd', ' ', '$', 'J', '_', '{', 'c', 'v', '}', ' ', '\\\\', 'g', 'g', ' ', 'J', '_', '{', 't', 'r', 'a', 'i', 'n', '}', '$', ' ', ' ', 'T', 'h', 'i', 's', ' ', 'i', 's', ' ', 'r', 'a', 'r', 'e', ',', ' ', 'a', 'n', 'd', ' ', 'd', 'o', 'e', 's', 'n', \"'\", 't', ' ', 'r', 'e', 'a', 'l', 'l', 'y', ' ', 'h', 'a', 'p', 'p', 'e', 'n', ' ', 'f', 'o', 'r', ' ', 'l', 'i', 'n', 'e', 'a', 'r', ' ', 'm', 'o', 'd', 'e', 'l', 's', ' ', 'w', 'i', 't', 'h', ' ', 'o', 'n', 'e', ' ', '$', 'd', '$', ',', ' ', 'b', 'u', 't', ' ', 'i', 't', ' ', 'd', 'o', 'e', 's', ' ', 'h', 'a', 'p', 'p', 'e', 'n', '.', ' ', 'T', 'h', 'i', 's', ' ', 'i', 's', ' ', 'w', 'h', 'e', 'n', ' ', 'i', 't', ' ', 'o', 'v', 'e', 'r', 'f', 'i', 't', 's', ' ', 'f', 'o', 'r', ' ', 's', 'o', 'm', 'e', ' ', 'p', 'a', 'r', 't', ' ', 'o', 'f', ' ', 't', 'h', 'e', ' ', 'i', 'n', 'p', 'u', 't', ' ', 'a', 'n', 'd', ' ', 'u', 'n', 'd', 'e', 'r', '-', 'f', 'i', 't', 's', ' ', 'f', 'o', 'r', ' ', 'a', 'n', 'o', 't', 'h', 'e', 'r', ' ', 'p', 'a', 'r', 't', ' ', 'o', 'f', ' ', 't', 'h', 'e', ' ', 'i', 'n', 'p', 'u', 't', '.', ' ', '#', '#', '#', '#', ' ', 'S', 'l', 'i', 'd', 'e', ' ', '!', 'S', 'c', 'r', 'e', 'e', 'n', 's', 'h', 'o', 't', ' ', '2', '0', '2', '3', '-', '1', '0', '-', '1', '7', ' ', 'a', 't', ' ', '1', '.', '3', '8', '.', '5', '1', ' ', 'P', 'M', '.', 'p', 'n', 'g', ' ', ' ', '#', '#', ' ', 'R', 'e', 'g', 'u', 'l', 'a', 'r', 'i', 'z', 'a', 't', 'i', 'o', 'n', ' ', 'a', 'n', 'd', ' ', 'b', 'i', 'a', 's', '/', 'v', 'a', 'r', 'i', 'a', 'n', 'c', 'e', '#', '#', '#', ' ', 'L', 'i', 'n', 'e', 'a', 'r', ' ', 'R', 'e', 'g', 'r', 'e', 's', 's', 'i', 'o', 'n', ' ', 'w', 'i', 't', 'h', ' ', 'R', 'e', 'g', 'u', 'l', 'a', 'r', 'i', 'z', 'a', 't', 'i', 'o', 'n', ' ', 'R', 'e', 'g', 'u', 'l', 'a', 'r', 'i', 'z', 'e', 'd', ' ', 'L', 'i', 'n', 'e', 'a', 'r', ' ', 'R', 'e', 'g', 'r', 'e', 's', 's', 'i', 'o', 'n', ' ', 'm', 'o', 'd', 'e', 'l', ':', ' ', 'L', 'a', 'm', 'b', 'd', 'a', ' ', '$', '\\\\', 'l', 'a', 'm', 'b', 'd', 'a', '$', ' ', 'i', 's', ' ', 't', 'h', 'e', ' ', 'r', 'e', 'g', 'u', 'l', 'a', 'r', 'i', 'z', 'a', 't', 'i', 'o', 'n', ' ', 'p', 'a', 'r', 'a', 'm', 'e', 't', 'e', 'r', ' ', '(', 'B', 'i', 'g', ' ', '#', 'm', 'e', 'r', 'g', 'e', ' ', 'a', 'n', 'd', ' ', 'c', 'l', 'e', 'a', 'n', 'u', 'p', ' ', 'n', 'e', 'e', 'd', 'e', 'd', ' ', 'h', 'e', 'r', 'e', ' ', 'i', 'n', ' ', 't', 'h', 'e', ' ', 'r', 'e', 'g', 'u', 'l', 'a', 'r', 'i', 'z', 'a', 't', 'i', 'o', 'n', ',', ' ', 'R', 'e', 'g', 'u', 'l', 'a', 'r', 'i', 'z', 'e', 'd', ' ', 'L', 'i', 'n', 'e', 'a', 'r', ' ', 'R', 'e', 'g', 'r', 'e', 's', 's', 'i', 'o', 'n', ',', ' ', 'c', 'o', 's', 't', ',', ' ', 'C', 'o', 's', 't', ' ', 'a', 'n', 'd', ' ', 'L', 'o', 's', 's', ',', ' ', 'l', 'o', 's', 's', ',', ' ', 'L', 'o', 's', 's', ' ', 'F', 'u', 'n', 'c', 't', 'i', 'o', 'n', ',', ' ', 'e', 't', 'c', '.', ' ', 'A', 'l', 'i', 'a', 's', 'e', 's', ' ', 'a', 'r', 'e', ' ', 'n', 'e', 'e', 'd', 'e', 'd', ' ', 'i', 'n', ' ', 'm', 'a', 'n', 'y', ' ', 'o', 'f', ' ', 't', 'h', 'e', 's', 'e', ')', ' ', ' ', 'T', 'a', 'k', 'e', ' ', 't', 'h', 'i', 's', ' ', 'm', 'o', 'd', 'e', 'l', ':', ' ', '$', '$', 'f', '_', '{', '\\\\', 'v', 'e', 'c', '{', 'w', '}', ',', 'b', '}', '(', 'x', ')', '=', 'w', '_', '1', '+', 'w', '_', '2', 'x', '^', '2', ' ', '+', ' ', 'w', '_', '4', 'x', '^', '4', '+', 'b', '$', '$', ' ', 'W', 'i', 't', 'h', ' ', 't', 'h', 'i', 's', ' ', 'r', 'e', 'g', 'u', 'l', 'a', 'r', 'i', 'z', 'a', 't', 'i', 'o', 'n', '|', 'r', 'e', 'g', 'u', 'l', 'a', 'r', 'i', 'z', 'e', 'd', ' ', 'C', 'o', 's', 't', ' ', 'a', 'n', 'd', ' ', 'L', 'o', 's', 's', ' ', '$', '$', 'J', '(', '\\\\', 'v', 'e', 'c', '{', 'w', '}', ',', 'b', ')', '=', '\\\\', 'f', 'r', 'a', 'c', '{', '1', '}', '{', '2', 'm', '}', ' ', '\\\\', 's', 'u', 'm', '_', '{', 'i', '=', '1', '}', '^', '{', 'm', '}', '(', 'f', '_', '{', '\\\\', 'v', 'e', 'c', '{', 'w', '}', ',', '{', 'b', '}', '}', '(', '\\\\', 'v', 'e', 'c', '{', 'x', '}', '^', '{', '(', 'i', ')', '}', '-', 'y', '^', '{', '(', 'i', ')', '}', ')', '^', '2', '+', '\\\\', 'f', 'r', 'a', 'c', '{', '\\\\', 'l', 'a', 'm', 'b', 'd', 'a', '}', '{', '2', 'm', '}', ' ', '\\\\', 's', 'u', 'm', '_', '{', 'j', '=', '1', '}', '^', 'n', 'w', '_', 'j', '^', '2', '$', '$', ' ', ' ', 'I', 'f', ' ', 'l', 'a', 'm', 'b', 'd', 'a', ' ', 'i', 's', ' ', 'l', 'a', 'r', 'g', 'e', ',', ' ', 's', 'a', 'y', ' ', '$', '\\\\', 'l', 'a', 'm', 'b', 'd', 'a', ' ', '=', ' ', '1', '0', ',', '0', '0', '0', '$', ' ', 't', 'h', 'a', 'n', ' ', '$', 'w', '_', '1', ' ', '\\\\', 'a', 'p', 'p', 'r', 'o', 'x', ' ', '0', ',', ' ', 'W', '_', '2', ' ', '\\\\', 'a', 'p', 'p', 'r', 'o', 'x', ' ', '0', '$', ' ', 'a', 'n', 'd', ' ', '$', 'f', '_', '{', '\\\\', 'v', 'e', 'c', '{', 'x', '}', ',', 'b', '}', '\\\\', 'v', 'e', 'c', '{', 'x', '}', ' ', '\\\\', 'a', 'p', 'p', 'r', 'o', 'x', ' ', 'b', '$', ',', ' ', 't', 'h', 'u', 's', ' ', 't', 'h', 'e', ' ', 'm', 'o', 'd', 'e', 'l', ' ', 'c', 'r', 'e', 'a', 't', 'e', 's', ' ', 'a', ' ', 'f', 'l', 'a', 't', ' ', 'l', 'i', 'n', 'e', '.', ' ', 'T', 'h', 'i', 's', ' ', 'i', 's', ' ', 'u', 'n', 'd', 'e', 'r', 'f', 'i', 't', ',', ' ', '$', 'J', '_', '{', 't', 'r', 'a', 'i', 'n', '}', '(', ')', ' ', ' ', 'O', 'n', ' ', 't', 'h', 'e', ' ', 'o', 't', 'h', 'e', 'r', ' ', 'h', 'a', 'n', 'd', ',', ' ', 'i', 'f', ' ', 'w', 'e', ' ', 's', 'e', 't', ' ', '$', '\\\\', 'l', 'a', 'm', 'b', 'd', 'a', '=', '0', '$', ',', ' ', 't', 'h', 'e', 'n', ' ', 'w', 'e', ' ', 'h', 'a', 'v', 'e', ' ', 'a', ' ', 'f', 'o', 'r', 't', 'h', ' ', 'o', 'r', 'd', 'e', 'r', ' ', 'p', 'o', 'l', 'y', 'n', 'o', 'm', 'i', 'a', 'l', ' ', 'w', 'i', 't', 'h', ' ', 'n', 'o', ' ', 'r', 'e', 'g', 'u', 'l', 'a', 'r', 'i', 'z', 'a', 't', 'i', 'o', 'n', '.', ' ', 'W', 'e', ' ', 'e', 'n', 'd', ' ', 'u', 'p', ' ', 'w', 'i', 't', 'h', ' ', 'a', ' ', 'v', 'e', 'r', 'y', ' ', 'o', 'v', 'e', 'r', 'f', 'i', 't', ' ', 'c', 'u', 'r', 'v', 'e', '.', ' ', ' ', 'S', 'o', ',', ' ', 'h', 'o', 'w', ' ', 'd', 'o', ' ', 'w', 'e', ' ', 'f', 'i', 'n', 'd', ' ', 'a', ' ', 'g', 'o', 'o', 'd', ' ', 'v', 'a', 'l', 'u', 'e', ' ', 'f', 'o', 'r', ' ', '$', '\\\\', 'l', 'a', 'm', 'b', 'd', 'a', '$', '?', ' ', '#', '#', '#', '#', '#', ' ', 'S', 'l', 'i', 'd', 'e', ' ', '!', 'S', 'c', 'r', 'e', 'e', 'n', 's', 'h', 'o', 't', ' ', '2', '0', '2', '3', '-', '1', '0', '-', '1', '7', ' ', 'a', 't', ' ', '3', '.', '5', '6', '.', '5', '3', ' ', 'P', 'M', '.', 'p', 'n', 'g', '#', '#', '#', ' ', 'C', 'h', 'o', 'o', 's', 'i', 'n', 'g', ' ', 't', 'h', 'e', ' ', 'R', 'e', 'g', 'u', 'l', 'a', 'r', 'i', 'z', 'a', 't', 'i', 'o', 'n', ' ', 'p', 'a', 'r', 'a', 'm', 'e', 't', 'e', 'r', ' ', '$', '\\\\', 'l', 'a', 'm', 'b', 'd', 'a', '$', ' ', ' ', ' ', 'T', 'h', 'i', 's', ' ', 'w', 'i', 'l', 'l', ' ', 'b', 'e', ' ', 's', 'i', 'm', 'i', 'l', 'a', 'r', ' ', 't', 'o', ' ', 'c', 'h', 'o', 'o', 's', 'i', 'n', 'g', ' ', '$', 'd', '$', ' ', 'w', 'i', 't', 'h', ' ', 'c', 'r', 'o', 's', 's', ' ', 'v', 'a', 'l', 'i', 'd', 'a', 't', 'i', 'o', 'n', ' ', 'T', 'r', 'y', ' ', 'm', 'u', 'l', 't', 'i', 'p', 'l', 'e', ' ', 'v', 'a', 'l', 'u', 'e', 's', ' ', 'f', 'o', 'r', ' ', '$', '\\\\', 'l', 'a', 'm', 'b', 'd', 'a', '$', ' ', 'a', 'n', 'd', ' ', 't', 'h', 'e', 'n', ' ', 'c', 'h', 'o', 'o', 's', 'e', ' ', 't', 'h', 'e', ' ', 'o', 'p', 't', 'i', 'o', 'n', ' ', 'w', 'i', 't', 'h', ' ', 't', 'h', 'e', ' ', 'l', 'o', 'w', 'e', 's', 't', ' ', 'c', 'o', 's', 't', '.', '#', '#', '#', ' ', 'S', 'l', 'i', 'd', 'e', ' ', '!', 'S', 'c', 'r', 'e', 'e', 'n', 's', 'h', 'o', 't', ' ', '2', '0', '2', '3', '-', '1', '0', '-', '1', '7', ' ', 'a', 't', ' ', '4', '.', '1', '5', '.', '2', '7', ' ', 'P', 'M', '.', 'p', 'n', 'g', '#', '#', '#', ' ', 'B', 'i', 'a', 's', ' ', 'a', 'n', 'd', ' ', 'V', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', 'a', 's', ' ', 'a', ' ', 'F', 'u', 'n', 'c', 't', 'i', 'o', 'n', ' ', 'o', 'f', ' ', 'r', 'e', 'g', 'u', 'l', 'a', 'r', 'i', 'z', 'a', 't', 'i', 'o', 'n', ' ', 'p', 'a', 'r', 'a', 'm', 'e', 't', 'e', 'r', ' ', '$', '\\\\', 'l', 'a', 'm', 'b', 'd', 'a', '$', ' ', ' ', 'C', 'r', 'o', 's', 's', ' ', 'V', 'a', 'l', 'i', 'd', 'a', 't', 'i', 'o', 'n', ' ', 't', 'r', 'i', 'e', 's', ' ', 'o', 'u', 't', ' ', 'm', 'a', 'n', 'y', ' ', 'v', 'e', 'r', 's', 'i', 'o', 'n', 's', ' ', 'o', 'f', ' ', '$', '\\\\', 'l', 'a', 'm', 'b', 'd', 'a', '$', ' ', 'a', 'n', 'd', ' ', 't', 'h', 'e', 'n', ' ', 'c', 'h', 'o', 'o', 's', 'e', 's', ' ', 't', 'h', 'e', ' ', 'o', 'n', 'e', ' ', 'w', 'i', 't', 'h', ' ', 't', 'h', 'e', ' ', 'l', 'o', 'w', 'e', 's', 't', ' ', 'c', 'o', 's', 't', '.', ' ', 'C', 'r', 'o', 's', 's', ' ', 'v', 'a', 'l', 'i', 'd', 'a', 't', 'i', 'o', 'n', ' ', 'w', 'i', 'l', 'l', ' ', 'h', 'e', 'l', 'p', ' ', 'f', 'i', 'n', 'd', ' ', 'u', 's', ' ', 'a', ' ', 'a', ' ', 'g', 'o', 'o', 'd', ' ', 'v', 'a', 'l', 'u', 'e', ' ', 'o', 'f', ' ', '$', 'd', '$', ' ', 'a', 's', ' ', 'w', 'e', 'l', 'l', ' ', 'a', 's', ' ', '$', '\\\\', 'l', 'a', 'm', 'b', 'd', 'a', '$', ' ', ' ', '#', '#', '#', '#', ' ', 'S', 'l', 'i', 'd', 'e', ' ', '!', 'S', 'c', 'r', 'e', 'e', 'n', 's', 'h', 'o', 't', ' ', '2', '0', '2', '3', '-', '1', '0', '-', '1', '7', ' ', 'a', 't', ' ', '4', '.', '1', '4', '.', '0', '7', ' ', 'P', 'M', '.', 'p', 'n', 'g', ' ', ' ', ' ', '#', '#', ' ', 'E', 's', 't', 'a', 'b', 'l', 'i', 's', 'h', 'i', 'n', 'g', ' ', 'A', ' ', 'B', 'a', 's', 'e', 'l', 'i', 'n', 'e', ' ', 'L', 'e', 'v', 'e', 'l', ' ', 'o', 'f', ' ', 'P', 'e', 'r', 'f', 'o', 'r', 'm', 'a', 'n', 'c', 'e', '#', '#', '#', ' ', 'S', 'p', 'e', 'e', 'c', 'h', ' ', 'R', 'e', 'c', 'o', 'g', 'n', 'i', 't', 'i', 'o', 'n', ' ', 'e', 'x', 'a', 'm', 'p', 'l', 'e', ' ', 'J', 'o', 'b', ' ', 'i', 's', ' ', 't', 'o', ' ', 't', 'a', 'k', 'e', ' ', 'i', 'n', ' ', 'a', 'u', 'd', 'i', 'o', ' ', 'a', 'n', 'd', ' ', 'o', 'u', 't', 'p', 'u', 't', ' ', 't', 'h', 'e', ' ', 't', 'e', 'x', 't', ' ', 'o', 'f', ' ', 'w', 'h', 'a', 't', ' ', 'a', ' ', 'p', 'e', 'r', 's', 'o', 'n', ' ', 'i', 's', ' ', 's', 'a', 'y', 'i', 'n', 'g', ' ', ' ', 'T', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ' ', 'e', 'r', 'r', 'o', 'r', ' ', '$', 'J', '_', '{', 't', 'r', 'a', 'i', 'n', '}', '$', ' ', 'i', 's', ' ', 'p', 'e', 'r', 'c', 'e', 'n', 't', 'a', 'g', 'e', ' ', 'o', 'f', ' ', 'a', 'u', 'd', 'i', 'o', ' ', 'c', 'l', 'i', 'p', 's', ' ', 't', 'h', 'a', 't', ' ', 't', 'h', 'e', ' ', 'p', 'r', 'o', 'g', 'r', 'a', 'm', ' ', 'd', 'o', 'e', 's', ' ', 'n', 'o', 't', ' ', 't', 'r', 'a', 'n', 's', 'c', 'r', 'i', 'b', 'e', ' ', 'c', 'o', 'r', 'r', 'e', 'c', 't', 'l', 'y', ' ', 'i', 'n', ' ', 'i', 't', \"'\", 's', ' ', 'e', 'n', 't', 'i', 'r', 'e', 't', 'y', '.', ' ', 'L', 'e', 't', 's', ' ', 's', 'a', 'y', ':', ' ', '\\t', 'H', 'u', 'm', 'a', 'n', ' ', 'L', 'e', 'v', 'e', 'l', ' ', 'P', 'e', 'r', 'f', 'o', 'r', 'm', 'a', 'n', 'c', 'e', ':', ' ', '1', '0', '.', '6', ' ', '\\t', '$', 'J', '_', '{', 't', 'r', 'a', 'i', 'n', '}', '$', ':', ' ', '1', '0', '.', '8', '%', ' ', '\\t', '$', 'J', '_', '{', 'c', 'v', '}', '$', ':', ' ', '1', '4', '.', '8', '%', ' ', ' ', 'W', 'h', 'y', ' ', 'i', 's', ' ', 'h', 'u', 'm', 'a', 'n', ' ', 'l', 'e', 'v', 'e', 'l', ' ', 'e', 'r', 'r', 'o', 'r', ' ', 's', 'o', ' ', 'h', 'i', 'g', 'h', '?', ' ', 'T', 'h', 'e', 'r', 'e', ' ', 'i', 's', ' ', 'l', 'o', 't', 's', ' ', 'o', 'f', ' ', 'n', 'o', 'i', 's', 'e', ' ', 'i', 'n', ' ', 't', 'h', 'e', ' ', 'a', 'u', 'd', 'i', 'o', '.', ' ', 'I', 't', ' ', 's', 'e', 'e', 'm', 's', ' ', 'u', 'n', 'f', 'a', 'i', 'r', ' ', 't', 'o', ' ', 'e', 'x', 'p', 'e', 'c', 't', ' ', 'a', ' ', 'l', 'e', 'a', 'r', 'n', 'i', 'n', 'g', ' ', 'a', 'l', 'g', 'o', 'r', 'i', 't', 'h', 'm', ' ', 't', 'o', ' ', 'd', 'o', ' ', 'm', 'u', 'c', 'h', ' ', 'b', 'e', 't', 't', 'e', 'r', '.', ' ', 'I', 't', ' ', 'i', 's', ' ', 't', 'h', 'u', 's', ' ', 'i', 's', ' ', 'm', 'o', 'r', 'e', ' ', 'u', 's', 'e', 'f', 'u', 'l', ' ', 't', 'o', ' ', 'm', 'e', 'a', 's', 'u', 'r', 'e', ' ', 't', 'h', 'e', ' ', 't', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ' ', 'e', 'r', 'r', 'o', 'r', ' ', 'a', 'g', 'a', 'i', 'n', 's', 't', ' ', 't', 'h', 'e', ' ', 'h', 'u', 'm', 'a', 'n', ' ', 'e', 'r', 'r', 'o', 'r', '.', ' ', 'S', 'o', ',', ' ', 'l', 'o', 'o', 'k', 'i', 'n', 'g', ' ', 'a', 't', ' ', 't', 'h', 'e', 's', 'e', ' ', 'r', 'e', 's', 'u', 'l', 't', 's', ',', ' ', '$', 'J', '_', '{', 't', 'r', 'a', 'i', 'n', '}', '$', ' ', 'i', 's', ' ', 'o', 'n', 'l', 'y', ' ', '0', '.', '2', '$', ' ', 'h', 'i', 'g', 'h', 'e', 'r', ' ', 't', 'h', 'a', 'n', ' ', 't', 'h', 'e', ' ', 'h', 'u', 'm', 'a', 'n', ' ', 'l', 'e', 'v', 'e', 'l', ' ', 'p', 'e', 'r', 'f', 'o', 'r', 'm', 'a', 'n', 'c', 'e', ' ', 'w', 'h', 'e', 'r', 'e', 'a', 's', ' ', '$', 'J', '_', '{', 'c', 'v', '}', '$', ' ', 'i', 's', ' ', 'a', ' ', 'f', 'u', 'l', 'l', ' ', '4', '.', '2', '%', ' ', 'h', 'i', 'g', 'h', 'e', 'r', '.', ' ', 'W', 'e', ' ', 'c', 'a', 'n', ' ', 't', 'h', 'u', 's', ' ', 'c', 'o', 'n', 'c', 'l', 'u', 'd', 'e', ' ', 't', 'h', 'a', 't', ' ', 't', 'h', 'i', 's', ' ', 'a', 'l', 'g', 'o', 'r', 'i', 't', 'h', 'm', ' ', 'h', 'a', 's', ' ', 'm', 'o', 'r', 'e', ' ', 'o', 'f', ' ', 'a', ' ', 'v', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', 'p', 'r', 'o', 'b', 'l', 'e', 'm', ' ', 't', 'h', 'a', 'n', ' ', 'a', ' ', 'b', 'i', 'a', 's', ' ', 'p', 'r', 'o', 'b', 'l', 'e', 'm', '.', '#', '#', '#', ' ', 'E', 's', 't', 'a', 'b', 'l', 'i', 's', 'h', 'i', 'n', 'g', ' ', 'a', ' ', 'b', 'a', 's', 'e', 'l', 'i', 'n', 'e', ' ', 'l', 'e', 'v', 'e', 'l', ' ', 'o', 'f', ' ', 'p', 'e', 'r', 'f', 'o', 'r', 'm', 'a', 'n', 'c', 'e', ' ', 'W', 'h', 'a', 't', ' ', 'i', 's', ' ', 't', 'h', 'e', ' ', 'l', 'e', 'v', 'e', 'l', ' ', 'o', 'f', ' ', 'e', 'r', 'r', 'o', 'r', ' ', 'y', 'o', 'u', ' ', 'c', 'a', 'n', ' ', 'r', 'e', 'a', 's', 'o', 'n', 'a', 'b', 'l', 'y', ' ', 'h', 'o', 'p', 'e', ' ', 't', 'o', ' ', 'g', 'e', 't', ' ', 't', 'o', '?', ' ', '-', ' ', 'H', 'u', 'm', 'a', 'n', ' ', 'l', 'e', 'v', 'e', 'l', ' ', 'p', 'e', 'r', 'f', 'o', 'r', 'm', 'a', 'n', 'c', 'e', ' ', '-', ' ', 'C', 'o', 'm', 'p', 'e', 't', 'i', 'n', 'g', ' ', 'a', 'l', 'g', 'o', 'r', 'i', 't', 'h', 'm', 's', ' ', 'p', 'e', 'r', 'f', 'o', 'r', 'm', 'a', 'n', 'c', 'e', ' ', '-', ' ', 'G', 'u', 'e', 's', 's', ' ', 'b', 'a', 's', 'e', 'd', ' ', 'o', 'n', ' ', 'e', 'x', 'p', 'e', 'r', 'i', 'e', 'n', 'c', 'e', '#', '#', '#', ' ', 'B', 'i', 'a', 's', ' ', 'V', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', 'E', 'x', 'a', 'm', 'p', 'l', 'e', 's', ' ', 'G', 'a', 'p', ' ', 'b', 'e', 't', 'w', 'e', 'e', 'n', ' ', 'b', 'a', 's', 'e', 'l', 'i', 'n', 'e', ' ', 'a', 'n', 'd', ' ', 't', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ' ', 'e', 'r', 'r', 'o', 'r', ' ', 's', 'h', 'o', 'w', 's', ' ', 'h', 'i', 'g', 'h', ' ', 'b', 'i', 'a', 's', '.', ' ', 'A', ' ', 'g', 'a', 'p', ' ', 'b', 'e', 't', 'w', 'e', 'e', 'n', ' ', 't', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ' ', 'e', 'r', 'r', 'o', 'r', ' ', 'a', 'n', 'd', ' ', 'c', 'r', 'o', 's', 's', ' ', 'v', 'a', 'l', 'i', 'd', 'a', 't', 'i', 'o', 'n', ' ', 'e', 'r', 'r', 'o', 'r', ' ', 's', 'h', 'o', 'w', 's', ' ', 'h', 'i', 'g', 'h', ' ', 'v', 'a', 'r', 'i', 'a', 'n', 'c', 'e', '.', ' ', ' ', 'I', 'f', ' ', 'y', 'o', 'u', 'r', ' ', 'g', 'o', 'a', 'l', ' ', 'i', 's', ' ', 'p', 'e', 'r', 'f', 'e', 'c', 't', 'i', 'o', 'n', ',', ' ', 't', 'h', 'e', ' ', 'b', 'a', 's', 'e', 'l', 'i', 'n', 'e', ' ', 'w', 'o', 'u', 'l', 'd', ' ', 'b', 'e', ' ', 'z', 'e', 'r', 'o', '.', ' ', 'B', 'u', 't', ' ', 'f', 'o', 'r', ' ', 'a', ' ', 'l', 'o', 't', ' ', 'o', 'f', ' ', 'r', 'e', 'a', 'l', ' ', 'w', 'o', 'r', 'l', 'd', ' ', 'e', 'x', 'a', 'm', 'p', 'l', 'e', 's', ',', ' ', 'l', 'i', 'k', 'e', ' ', 'a', 'u', 'd', 'i', 'o', ' ', 'r', 'e', 'c', 'o', 'g', 'n', 'i', 't', 'i', 'o', 'n', ',', ' ', 't', 'h', 'e', 'r', 'e', ' ', 'i', 's', ' ', 'a', ' ', 'l', 'o', 't', ' ', 'o', 'f', ' ', 'n', 'o', 'i', 's', 'e', ' ', 'i', 'n', ' ', 't', 'h', 'e', ' ', 'd', 'a', 't', 'a', ',', ' ', 's', 'o', ' ', 'y', 'o', 'u', ' ', 'n', 'e', 'e', 'd', ' ', 'a', ' ', 'h', 'i', 'g', 'h', 'e', 'r', ' ', 'b', 'a', 's', 'e', 'l', 'i', 'n', 'e', '.', ' ', ' ', 'I', 'f', ' ', 't', 'h', 'e', 'r', 'e', ' ', 'i', 's', ' ', 'a', ' ', 'g', 'a', 'p', ' ', 'b', 'e', 't', 'w', 'e', 'e', 'n', ' ', 'a', 'l', 'l', ' ', 't', 'h', 'r', 'e', 'e', ' ', 'i', 's', ' ', 'h', 'i', 'g', 'h', ' ', 'y', 'o', 'u', ' ', 'h', 'a', 'v', 'e', ' ', 'b', 'o', 't', 'h', ' ', 'h', 'i', 'g', 'h', ' ', 'b', 'i', 'a', 's', ' ', 'a', 'n', 'd', ' ', 'h', 'i', 'g', 'h', ' ', 'v', 'a', 'r', 'i', 'a', 'n', 'c', 'e', '.', ' ', '#', '#', '#', '#', ' ', 'S', 'l', 'i', 'd', 'e', ' ', '!', 'S', 'c', 'r', 'e', 'e', 'n', 's', 'h', 'o', 't', ' ', '2', '0', '2', '3', '-', '1', '0', '-', '1', '7', ' ', 'a', 't', ' ', '4', '.', '4', '2', '.', '4', '9', ' ', 'P', 'M', '.', 'p', 'n', 'g', ' ', ' ', ' ', '#', '#', ' ', 'L', 'e', 'a', 'r', 'n', 'i', 'n', 'g', ' ', 'C', 'u', 'r', 'v', 'e', 's', ' ', ' ', 'N', 'o', 't', 'e', 'd', ' ', 'i', 'n', ' ', 'L', 'e', 'a', 'r', 'n', 'i', 'n', 'g', ' ', 'C', 'u', 'r', 'v', 'e', 's', '#', '#', '#', ' ', 'O', 'v', 'e', 'r', 'v', 'i', 'e', 'w', ' ', 'L', 'e', 'a', 'r', 'n', 'i', 'n', 'g', ' ', 'c', 'u', 'r', 'v', 'e', 's', ' ', 'h', 'e', 'l', 'p', ' ', 'u', 'n', 'd', 'e', 'r', 's', 't', 'a', 'n', 'd', ' ', 'h', 'o', 'w', ' ', 'y', 'o', 'u', 'r', ' ', 'l', 'e', 'a', 'r', 'n', 'i', 'n', 'g', ' ', 'a', 'l', 'g', 'o', 'r', 'i', 't', 'h', 'm', ' ', 'i', 's', ' ', 'd', 'o', 'i', 'n', 'g', ' ', 'a', 's', ' ', 'a', ' ', 'f', 'u', 'n', 'c', 't', 'i', 'o', 'n', ' ', 'o', 'f', ' ', 't', 'h', 'e', ' ', 'a', 'm', 'o', 'u', 'n', 't', ' ', 'o', 'f', ' ', 'e', 'x', 'p', 'e', 'r', 'i', 'e', 'n', 'c', 'e', ' ', 'i', 't', ' ', 'h', 'a', 's', '.', ' ', 'E', 'x', 'p', 'e', 'r', 'i', 'e', 'n', 'c', 'e', ' ', 'b', 'e', 'i', 'n', 'g', ' ', 't', 'h', 'e', ' ', 'n', 'u', 'm', 'b', 'e', 'r', ' ', 'o', 'f', ' ', 't', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ' ', 'e', 'x', 'a', 'm', 'p', 'l', 'e', 's', ' ', 'i', 't', ' ', 'h', 'a', 's', '.', ' ', ' ', 'T', 'h', 'e', ' ', 'b', 'i', 'g', 'g', 'e', 'r', ' ', 't', 'h', 'e', ' ', 't', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ' ', 's', 'e', 't', ' ', 't', 'h', 'e', ' ', 'h', 'a', 'r', 'd', 'e', 'r', ' ', 'i', 't', ' ', 'i', 's', ' ', 't', 'o', ' ', 'f', 'i', 't', ' ', 'a', 'l', 'l', ' ', 'e', 'x', 'a', 'm', 'p', 'l', 'e', 's', ' ', 'p', 'e', 'r', 'f', 'e', 'c', 't', 'l', 'y', '.', ' ', 'T', 'h', 'u', 's', ';', ' ', 'a', 's', ' ', 't', 'h', 'e', ' ', 't', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ' ', 's', 'e', 't', ' ', 'i', 'n', 'c', 'r', 'e', 'a', 's', 'e', 's', ' ', 's', 'o', ' ', 'd', 'o', 'e', 's', ' ', 't', 'h', 'e', ' ', 't', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ' ', 'e', 'r', 'r', 'o', 'r', ' ', '$', 'J', '_', '{', 't', 'r', 'a', 'i', 'n', '}', '(', '\\\\', 'v', 'e', 'c', '{', 'w', '}', ',', 'b', ')', '$', ' ', ' ', ' ', 'P', 'l', 'o', 't', 't', 'i', 'n', 'g', ' ', 'a', ' ', 'l', 'e', 'a', 'r', 'n', 'i', 'n', 'g', ' ', 'c', 'u', 'r', 'v', 'e', ' ', 'b', 'y', ' ', 't', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ' ', 'd', 'i', 'f', 'f', 'e', 'r', 'e', 'n', 't', ' ', 'm', 'o', 'd', 'e', 'l', 's', ' ', 'b', 'a', 's', 'e', 'd', ' ', 'o', 'n', ' ', 'd', 'i', 'f', 'f', 'e', 'r', 'e', 'n', 't', ' ', 's', 'u', 'b', 's', 'e', 't', 's', ' ', 'o', 'f', ' ', 't', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ' ', 'd', 'a', 't', 'a', ' ', 'i', 's', ' ', 'c', 'o', 'm', 'p', 'u', 't', 'a', 't', 'i', 'o', 'n', 'a', 'l', 'l', 'y', ' ', 'e', 'x', 'p', 'e', 'n', 's', 'i', 'v', 'e', ',', ' ', 's', 'o', ' ', 'i', 'n', ' ', 'p', 'r', 'a', 'c', 't', 'i', 'c', 'e', ' ', 'i', 't', ' ', 'i', 's', 'n', \"'\", 't', ' ', 'd', 'o', 'n', 'e', ' ', 't', 'h', 'a', 't', ' ', 'o', 'f', 't', 'e', 'n', '.', ' ', 'B', 'u', 't', ',', ' ', 'i', 't', \"'\", 's', ' ', 'a', ' ', 'g', 'o', 'o', 'd', ' ', 'm', 'e', 'n', 't', 'a', 'l', ' ', 'v', 'i', 's', 'u', 'a', 'l', 'i', 'z', 'a', 't', 'i', 'o', 'n', '.', ' ', '#', '#', '#', '#', ' ', 'S', 'l', 'i', 'd', 'e', 's', ' ', '!', 'S', 'c', 'r', 'e', 'e', 'n', 's', 'h', 'o', 't', ' ', '2', '0', '2', '3', '-', '1', '0', '-', '1', '7', ' ', 'a', 't', ' ', '5', '.', '4', '0', '.', '0', '4', ' ', 'P', 'M', '.', 'p', 'n', 'g', ' ', ' ', ' ', '#', '#', '#', ' ', 'H', 'i', 'g', 'h', ' ', 'B', 'i', 'a', 's', ' ', 'E', 'x', 'a', 'm', 'p', 'l', 'e', ' ', 'I', 'f', ' ', 'a', ' ', 'l', 'e', 'a', 'r', 'n', 'i', 'n', 'g', ' ', 'a', 'l', 'g', 'o', 'r', 'i', 't', 'h', 'm', ' ', 's', 'u', 'f', 'f', 'e', 'r', 's', ' ', 'f', 'r', 'o', 'm', ' ', 'h', 'i', 'g', 'h', ' ', 'b', 'i', 'a', 's', ',', ' ', 'g', 'e', 't', 't', 'i', 'n', 'g', ' ', 'm', 'o', 'r', 'e', ' ', 't', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ' ', 'd', 'a', 't', 'a', ' ', 'w', 'i', 'l', 'l', ' ', 'n', 'o', 't', ' ', '(', 'b', 'y', ' ', 'i', 't', 's', 'e', 'l', 'f', ')', ' ', 'h', 'e', 'l', 'p', ' ', 't', 'h', 'a', 't', ' ', 'm', 'u', 'c', 'h', '.', ' ', '#', '#', '#', '#', ' ', 'S', 'l', 'i', 'd', 'e', ' ', 'f', 'o', 'r', ' ', 'H', 'i', 'g', 'h', ' ', 'B', 'i', 'a', 's', ' ', '!', 'S', 'c', 'r', 'e', 'e', 'n', 's', 'h', 'o', 't', ' ', '2', '0', '2', '3', '-', '1', '0', '-', '1', '7', ' ', 'a', 't', ' ', '5', '.', '4', '4', '.', '2', '9', ' ', 'P', 'M', '.', 'p', 'n', 'g', ' ', '#', '#', '#', ' ', 'H', 'i', 'g', 'h', ' ', 'V', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', 'E', 'x', 'a', 'm', 'p', 'l', 'e', ' ', 'I', 'f', ' ', 'a', ' ', 'l', 'e', 'a', 'r', 'n', 'i', 'n', 'g', ' ', 'a', 'l', 'g', 'o', 'r', 'i', 't', 'h', 'm', ' ', 's', 'u', 'f', 'f', 'e', 'r', 's', ' ', 'f', 'r', 'o', 'm', ' ', 'h', 'i', 'g', 'h', ' ', 'v', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ',', ' ', 'g', 'e', 't', 't', 'i', 'n', 'g', ' ', 'm', 'o', 'r', 'e', ' ', 't', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ' ', 'd', 'a', 't', 'a', ' ', 'i', 's', ' ', 'l', 'i', 'k', 'e', 'l', 'y', ' ', 't', 'o', ' ', 'h', 'e', 'l', 'p', '.', ' ', '#', '#', '#', '#', ' ', 'S', 'l', 'i', 'd', 'e', ' ', 'f', 'o', 'r', ' ', 'H', 'i', 'g', 'h', ' ', 'V', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', '!', 'S', 'c', 'r', 'e', 'e', 'n', 's', 'h', 'o', 't', ' ', '2', '0', '2', '3', '-', '1', '0', '-', '1', '7', ' ', 'a', 't', ' ', '5', '.', '4', '9', '.', '4', '5', ' ', 'P', 'M', '.', 'p', 'n', 'g', ' ', ' ', ' ', ' ', '#', '#', ' ', 'D', 'e', 'c', 'i', 'd', 'i', 'n', 'g', ' ', 'w', 'h', 'a', 't', ' ', 't', 'o', ' ', 't', 'r', 'y', ' ', 'n', 'e', 'x', 't', ' ', 'r', 'e', 'v', 'i', 's', 'i', 't', 'e', 'd', '#', '#', '#', ' ', 'E', 'x', 'a', 'm', 'p', 'l', 'e', 's', ' ', 'w', 'h', 'e', 'n', ' ', 'D', 'e', 'b', 'u', 'g', 'g', 'i', 'n', 'g', ' ', 'a', 'n', ' ', 'A', 'l', 'g', 'o', 'r', 'i', 't', 'h', 'm', ':', ' ', 'G', 'e', 't', ' ', 'm', 'o', 'r', 'e', ' ', 't', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ' ', 'e', 'x', 'a', 'm', 'p', 'l', 'e', 's', ':', ' ', 'f', 'i', 'x', 'e', 's', ' ', 'h', 'i', 'g', 'h', ' ', 'v', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', 'T', 'r', 'y', ' ', 's', 'm', 'a', 'l', 'l', 'e', 'r', ' ', 's', 'e', 't', ' ', 'o', 'f', ' ', 'f', 'e', 'a', 't', 'u', 'r', 'e', 's', ':', ' ', 'f', 'i', 'x', 'e', 's', ' ', 'h', 'i', 'g', 'h', ' ', 'v', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', 'T', 'r', 'y', ' ', 'g', 'e', 't', 't', 'i', 'n', 'g', ' ', 'a', 'd', 'd', 'i', 't', 'i', 'o', 'n', 'a', 'l', ' ', 'f', 'e', 'a', 't', 'u', 'r', 'e', 's', ':', ' ', 'f', 'i', 'x', 'e', 's', ' ', 'h', 'i', 'g', 'h', ' ', 'b', 'i', 'a', 's', ' ', '\\t', 'E', 'x', 'a', 'm', 'p', 'l', 'e', 's', ';', ' ', 'a', 'n', ' ', 'a', 'l', 'g', 'o', 'r', 'i', 't', 'h', 'm', ' ', 't', 'h', 'a', 't', ' ', 'l', 'a', 'c', 'k', 's', ' ', 'i', 'n', 'f', 'o', 'r', 'm', 'a', 't', 'i', 'o', 'n', ' ', 'w', 'o', 'n', 't', ' ', 'e', 'v', 'e', 'n', ' ', 'd', 'o', ' ', 'w', 'e', 'l', 'l', ' ', 'o', 'n', ' ', 't', 'h', 'e', ' ', 't', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ' ', 's', 'e', 't', ' ', 'A', 'd', 'd', 'i', 'n', 'g', ' ', 'p', 'o', 'l', 'y', 'n', 'o', 'm', 'i', 'a', 'l', ' ', 'f', 'e', 'a', 't', 'u', 'r', 'e', 's', ' ', '(', '$', 'x', '_', '1', '^', '2', ',', 'X', '_', '2', '^', '2', ',', ' ', 'e', 't', 'c', '$', ')', ':', ' ', 'F', 'i', 'x', 'e', 's', ' ', 'h', 'i', 'g', 'h', ' ', 'b', 'i', 'a', 's', ' ', 'D', 'e', 'c', 'r', 'e', 'a', 's', 'i', 'n', 'g', ' ', '$', '\\\\', 'l', 'a', 'm', 'b', 'd', 'a', '$', ' ', 'F', 'i', 'x', 'e', 's', ' ', 'h', 'i', 'g', 'h', ' ', 'b', 'i', 'a', 's', ' ', 'I', 'n', 'c', 'r', 'e', 'a', 's', 'i', 'n', 'g', ' ', '$', '\\\\', 'l', 'a', 'm', 'b', 'd', 'a', '$', ' ', 'F', 'i', 'x', 'e', 's', ' ', 'h', 'i', 'g', 'h', ' ', 'v', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', '\\t', 'F', 'o', 'r', 'c', 'e', 's', ' ', 't', 'h', 'e', ' ', 'a', 'l', 'g', 'o', 'r', 'i', 't', 'h', 'm', ' ', 't', 'o', ' ', 'f', 'o', 'r', 'c', 'e', ' ', 'a', ' ', 's', 'm', 'o', 'o', 't', 'h', 'e', 'r', ' ', 'f', 'u', 'n', 'c', 't', 'i', 'o', 'n', '.', ' ', ' ', '*', '*', 'N', 'o', 't', 'e', '!', '*', '*', ' ', 'D', 'o', 'n', \"'\", 't', ' ', 'r', 'a', 'n', 'd', 'o', 'm', 'l', 'y', ' ', 't', 'h', 'r', 'o', 'w', ' ', 'a', 'w', 'a', 'y', ' ', 't', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ' ', 'e', 'x', 'a', 'm', 'p', 'l', 'e', 's', ' ', 'j', 'u', 's', 't', ' ', 't', 'o', ' ', 'f', 'i', 'x', ' ', 'a', ' ', 'h', 'i', 'g', 'h', ' ', 'b', 'i', 'a', 's', ' ', 'p', 'r', 'o', 'b', 'l', 'e', 'm', '.', '#', '#', '#', ' ', 'T', 'a', 'k', 'e', 'a', 'w', 'a', 'y', ' ', '#', 'm', 'e', 'r', 'g', 'e', ' ', 'w', 'i', 't', 'h', ' ', 'B', 'i', 'a', 's', ' ', 'a', 'n', 'd', ' ', 'V', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', '?', ' ', 'I', 'f', ' ', 'y', 'o', 'u', 'r', ' ', 'a', 'l', 'g', 'o', 'r', 'i', 't', 'h', 'm', ' ', 'h', 'a', 's', ' ', 'h', 'i', 'g', 'h', ' ', 'v', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ',', ' ', 't', 'r', 'y', ' ', 's', 'i', 'm', 'p', 'l', 'i', 'f', 'y', 'i', 'n', 'g', ' ', 'y', 'o', 'u', 'r', ' ', 'm', 'o', 'd', 'e', 'l', ' ', 'o', 'r', ' ', 'g', 'e', 't', 't', 'i', 'n', 'g', ' ', 'm', 'o', 'r', 'e', ' ', 't', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ' ', 'd', 'a', 't', 'a', '.', ' ', 'S', 'i', 'm', 'p', 'l', 'i', 'f', 'i', 'c', 'a', 't', 'i', 'o', 'n', ' ', 'c', 'a', 'n', ' ', 'm', 'e', 'a', 'n', ' ', 'a', ' ', 's', 'm', 'a', 'l', 'l', 'e', 'r', ' ', 's', 'e', 't', ' ', 'o', 'f', ' ', 'f', 'e', 'a', 't', 'u', 'r', 'e', 's', ' ', 'o', 'r', ' ', 'a', 'n', ' ', 'i', 'n', 'c', 'r', 'e', 'a', 's', 'e', 'd', ' ', 'r', 'e', 'g', 'u', 'l', 'a', 'r', 'i', 'z', 'a', 't', 'i', 'o', 'n', ' ', 'I', 'f', ' ', 'y', 'o', 'u', 'r', ' ', 'a', 'l', 'g', 'o', 'r', 'i', 't', 'h', 'm', ' ', 'h', 'a', 's', ' ', 'h', 'i', 'g', 'h', ' ', 'b', 'i', 'a', 's', ',', ' ', 't', 'h', 'a', 't', ' ', 'i', 's', ' ', 't', 'o', ' ', 's', 'a', 'y', ' ', 'i', 't', 's', ' ', 'n', 'o', 't', ' ', 'e', 'v', 'e', 'n', ' ', 'd', 'o', 'i', 'n', 'g', ' ', 'w', 'e', 'l', 'l', ' ', 'o', 'n', ' ', 't', 'h', 'e', ' ', 't', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ' ', 's', 'e', 't', ',', ' ', 'y', 'o', 'u', ' ', 'm', 'a', 'i', 'n', 'l', 'y', ' ', 'n', 'e', 'e', 'd', ' ', 't', 'o', ' ', 'm', 'a', 'k', 'e', ' ', 'y', 'o', 'u', 'r', ' ', 'm', 'o', 'd', 'e', 'l', ' ', 'm', 'o', 'r', 'e', ' ', 'p', 'o', 'w', 'e', 'r', 'f', 'u', 'l', ' ', 'a', 'n', 'd', ' ', 'f', 'l', 'e', 'x', 'i', 'b', 'l', 'e', ' ', 't', 'o', ' ', 'f', 'i', 't', ' ', 'm', 'o', 'r', 'e', ' ', 'c', 'o', 'm', 'p', 'l', 'e', 'x', ' ', 'f', 'u', 'n', 'c', 't', 'i', 'o', 'n', 's', '.', ' ', 'T', 'o', ' ', 'd', 'o', ' ', 's', 'o', ' ', 'y', 'o', 'u', ' ', 'c', 'a', 'n', ' ', 'g', 'i', 'v', 'e', ' ', 'i', 't', ' ', 'a', 'd', 'd', 'i', 't', 'i', 'o', 'n', 'a', 'l', ' ', 'f', 'e', 'a', 't', 'u', 'r', 'e', 's', ',', ' ', 'a', 'd', 'd', ' ', 'p', 'o', 'l', 'y', 'n', 'o', 'm', 'i', 'a', 'l', ' ', 'f', 'e', 'a', 't', 'u', 'r', 'e', 's', ',', ' ', 'o', 'r', ' ', 'd', 'e', 'c', 'r', 'e', 'a', 's', 'e', ' ', '$', '\\\\', 'l', 'a', 'm', 'b', 'd', 'a', '$', ' ', ' ', ' ', '#', '#', ' ', 'B', 'i', 'a', 's', '/', 'V', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', 'a', 'n', 'd', ' ', 'N', 'e', 'u', 'r', 'a', 'l', ' ', 'N', 'e', 't', 'w', 'o', 'r', 'k', 's', '#', '#', '#', ' ', 'T', 'h', 'e', ' ', 'b', 'i', 'a', 's', ' ', 'v', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', 't', 'r', 'a', 'd', 'e', 'o', 'f', 'f', ' ', 'S', 'i', 'm', 'p', 'l', 'e', ' ', 'm', 'o', 'd', 'e', 'l', ' ', '=', ' ', 'h', 'i', 'g', 'h', ' ', 'b', 'i', 'a', 's', ' ', 'C', 'o', 'm', 'p', 'l', 'e', 'x', ' ', 'm', 'o', 'd', 'e', 'l', ' ', '=', ' ', 'h', 'i', 'g', 'h', ' ', 'v', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', 'B', 'e', 'f', 'o', 'r', 'e', ' ', 'n', 'e', 'u', 'r', 'a', 'l', ' ', 'n', 'e', 't', 'w', 'o', 'r', 'k', 's', ',', ' ', 'w', 'e', ' ', 'h', 'a', 'd', ' ', 't', 'o', ' ', 'w', 'o', 'r', 'r', 'y', ' ', 'a', 'b', 'o', 'u', 't', ' ', 'b', 'a', 'l', 'a', 'n', 'c', 'i', 'n', 'g', ' ', 't', 'h', 'i', 's', ' ', 'c', 'o', 'm', 'p', 'l', 'e', 'x', 'i', 't', 'y', ' ', 'b', 'e', 't', 'w', 'e', 'e', 'n', ' ', 'b', 'i', 'a', 's', ' ', 'a', 'n', 'd', ' ', 'v', 'a', 'r', 'i', 'a', 'n', 'c', 'e', '.', ' ', 'W', 'i', 't', 'h', ' ', 'n', 'e', 'u', 'r', 'a', 'l', ' ', 'n', 'e', 't', 'w', 'o', 'r', 'k', 's', ' ', 'w', 'e', ' ', 'n', 'o', 'w', ' ', 'a', 'r', 'e', ' ', 'm', 'o', 's', 't', 'l', 'y', ' ', 'w', 'o', 'r', 'r', 'i', 'e', 'd', ' ', 'a', 'b', 'o', 'u', 't', ' ', 'h', 'i', 'g', 'h', ' ', 'v', 'a', 'r', 'i', 'a', 'n', 'c', 'e', '.', ' ', '#', '#', '#', ' ', 'N', 'e', 'u', 'r', 'a', 'l', ' ', 'N', 'e', 't', 'w', 'o', 'r', 'k', 's', ' ', 'a', 'n', 'd', ' ', 'b', 'i', 'a', 's', ' ', 'v', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', 'L', 'a', 'r', 'g', 'e', ' ', 'N', 'e', 'u', 'r', 'a', 'l', ' ', 'N', 'e', 't', 'w', 'o', 'r', 'k', '|', 'n', 'e', 'u', 'r', 'a', 'l', ' ', 'n', 'e', 't', 'w', 'o', 'r', 'k', 's', ' ', 'a', 'r', 'e', ' ', 'l', 'o', 'w', ' ', 'b', 'i', 'a', 's', ' ', 'm', 'a', 'c', 'h', 'i', 'n', 'e', 's', '.', ' ', 'I', 'f', ' ', 'y', 'o', 'u', ' ', 'm', 'a', 'k', 'e', ' ', 'y', 'o', 'u', 'r', ' ', 'n', 'e', 'u', 'r', 'a', 'l', ' ', 'n', 'e', 't', 'w', 'o', 'r', 'k', ' ', 'l', 'a', 'r', 'g', 'e', ' ', 'e', 'n', 'o', 'u', 'g', 'h', ' ', 'y', 'o', 'u', ' ', 'c', 'a', 'n', ' ', 'a', 'l', 'm', 'o', 's', 't', ' ', 'a', 'l', 'w', 'a', 'y', 's', ' ', 'f', 'i', 't', ' ', 'y', 'o', 'u', 'r', ' ', 't', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ' ', 's', 'e', 't', ' ', 'w', 'e', 'l', 'l', '.', ' ', '#', '#', '#', '#', ' ', 'R', 'e', 'c', 'i', 'p', 'e', ' ', 'f', 'o', 'r', ' ', 'd', 'e', 'c', 'r', 'e', 'a', 's', 'i', 'n', 'g', ' ', 'b', 'i', 'a', 's', ' ', 'w', 'i', 't', 'h', ' ', 'a', ' ', 'n', 'e', 'u', 'r', 'a', 'l', ' ', 'n', 'e', 't', 'w', 'o', 'r', 'k', ' ', '1', '.', ' ', 'T', 'r', 'a', 'i', 'n', ' ', 'a', ' ', 'n', 'e', 'u', 'r', 'a', 'l', ' ', 'n', 'e', 't', 'w', 'o', 'r', 'k', ' ', '2', '.', ' ', ' ', 'I', 'f', ' ', 't', 'h', 'e', ' ', 't', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ' ', 's', 'e', 't', ' ', 'e', 'r', 'r', 'o', 'r', ' ', '$', 'J', '_', '{', 't', 'r', 'a', 'i', 'n', '}', '(', '\\\\', 'v', 'e', 'c', '{', 'w', '}', ',', 'b', ')', '$', ' ', 'i', 's', ' ', 'h', 'i', 'g', 'h', ' ', 'r', 'e', 'l', 'a', 't', 'i', 'v', 'e', ' ', 't', 'o', ' ', 'y', 'o', 'u', 'r', ' ', 'b', 'a', 's', 'e', 'l', 'i', 'n', 'e', ',', ' ', 'i', 'n', 'c', 'r', 'e', 'a', 's', 'e', ' ', 't', 'h', 'e', ' ', 's', 'i', 'z', 'e', ' ', 'o', 'f', ' ', 't', 'h', 'e', ' ', 'n', 'e', 'u', 'r', 'a', 'l', ' ', 'n', 'e', 't', 'w', 'o', 'r', 'k', ' ', 'b', 'y', ' ', 'a', 'd', 'd', 'i', 'n', 'g', ' ', 'h', 'i', 'd', 'd', 'e', 'n', ' ', 'l', 'a', 'y', 'e', 'r', 's', '.', ' ', '3', '.', ' ', 'O', 'n', 'c', 'e', ' ', '$', 'J', '_', '{', 't', 'r', 'a', 'i', 'n', '}', '(', '\\\\', 'v', 'e', 'c', '{', 'w', '}', ',', 'b', ')', '$', ' ', 'i', 's', ' ', 'l', 'o', 'w', ' ', 'e', 'n', 'o', 'u', 'g', 'h', ',', ' ', 's', 'e', 'e', ' ', 'i', 'f', ' ', 'i', 't', ' ', 'd', 'o', 'e', 's', ' ', 'w', 'e', 'l', 'l', ' ', 'o', 'n', ' ', 't', 'h', 'e', ' ', 'c', 'r', 'o', 's', 's', ' ', 'v', 'a', 'l', 'i', 'd', 'a', 't', 'i', 'o', 'n', ' ', 's', 'e', 't', ' ', '4', '.', ' ', ' ', 'I', 'f', ' ', 't', 'h', 'e', ' ', 'c', 'r', 'o', 's', 's', ' ', 'v', 'a', 'l', 'i', 'd', 'a', 't', 'i', 'o', 'n', ' ', 's', 'e', 't', ' ', '$', 'J', '_', '{', 'c', 'v', '}', '(', '\\\\', 'v', 'e', 'c', '{', 'w', '}', ',', 'b', ')', '$', ' ', 'i', 's', ' ', 't', 'o', 'o', ' ', 'h', 'i', 'g', 'h', ',', ' ', 'a', 'd', 'd', ' ', 'm', 'o', 'r', 'e', ' ', 'd', 'a', 't', 'a', ',', ' ', 't', 'h', 'e', 'n', ' ', 't', 'e', 's', 't', ' ', 'a', 'g', 'a', 'i', 'n', ' ', 'f', 'r', 'o', 'm', ' ', 's', 't', 'e', 'p', ' ', '2', '.', ' ', '5', '.', ' ', 'R', 'e', 'p', 'e', 'a', 't', ' ', 'u', 'n', 't', 'i', 'l', ' ', '$', 'J', '_', '{', 'c', 'v', '}', '(', '\\\\', 'v', 'e', 'c', '{', 'w', '}', ',', 'b', ')', '$', ' ', ' ', 'i', 's', ' ', 'l', 'o', 'w', ' ', 'e', 'n', 'o', 'u', 'g', 'h', ' ', 'f', 'o', 'r', ' ', 'y', 'o', 'u', 'r', ' ', 'l', 'i', 'k', 'i', 'n', 'g', '.', ' ', '#', '#', '#', '#', '#', ' ', 'S', 'l', 'i', 'd', 'e', ' ', 'I', 'l', 'l', 'u', 's', 't', 'r', 'a', 't', 'i', 'o', 'n', ' ', '!', 'S', 'c', 'r', 'e', 'e', 'n', 's', 'h', 'o', 't', ' ', '2', '0', '2', '3', '-', '1', '0', '-', '1', '7', ' ', 'a', 't', ' ', '6', '.', '3', '8', '.', '4', '6', ' ', 'P', 'M', '.', 'p', 'n', 'g', '#', '#', '#', ' ', 'L', 'i', 'm', 'i', 't', 'a', 't', 'i', 'o', 'n', 's', ' ', 'a', 'n', 'd', ' ', 'N', 'o', 't', 'e', 's', ' ', 'B', 'i', 'g', 'g', 'e', 'r', ' ', 'n', 'e', 't', 'w', 'o', 'r', 'k', 's', ' ', 'a', 'r', 'e', ' ', 'r', 'e', 's', 't', 'r', 'i', 'c', 't', 'e', 'd', ' ', 'b', 'y', ' ', 'y', 'o', 'u', 'r', ' ', 'c', 'o', 'm', 'p', 'u', 't', 'i', 'n', 'g', ' ', 'p', 'o', 'w', 'e', 'r', ',', ' ', 'd', 'a', 't', 'a', ' ', 'i', 's', ' ', 'r', 'e', 's', 't', 'r', 'i', 'c', 't', 'e', 'd', ' ', 't', 'o', ' ', 't', 'h', 'e', ' ', 'a', 'm', 'o', 'u', 'n', 't', ' ', 'o', 'f', ' ', 'd', 'a', 't', 'a', ' ', 'y', 'o', 'u', ' ', 'h', 'a', 'v', 'e', '.', ' ', ' ', 'S', 'o', 'm', 'e', 't', 'i', 'm', 'e', 's', ' ', 'y', 'o', 'u', ' ', 'w', 'i', 'l', 'l', ' ', 'p', 'i', 'n', 'g', 'p', 'o', 'n', 'g', ' ', 'b', 'a', 'c', 'k', ' ', 'b', 'e', 't', 'w', 'e', 'e', 'n', ' ', 'h', 'i', 'g', 'h', ' ', 'b', 'i', 'a', 's', ' ', 'a', 'n', 'd', ' ', 'h', 'i', 'g', 'h', ' ', 'v', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', 'a', 's', ' ', 'y', 'o', 'u', ' ', 'm', 'o', 'v', 'e', ' ', 't', 'h', 'r', 'o', 'u', 'g', 'h', ' ', 't', 'h', 'i', 's', ' ', 'r', 'e', 'c', 'i', 'p', 'e', ' ', 'a', 'n', 'd', ' ', 'd', 'e', 'v', 'e', 'l', 'o', 'p', ' ', 'a', ' ', 'm', 'a', 'c', 'h', 'i', 'n', 'e', ' ', 'l', 'e', 'a', 'r', 'n', 'i', 'n', 'g', ' ', 'a', 'l', 'g', 'o', 'r', 'i', 't', 'h', 'm', '.', ' ', 'U', 's', 'e', ' ', 't', 'h', 'e', 's', 'e', ' ', 'o', 'b', 's', 'e', 'r', 'v', 'a', 't', 'i', 'o', 'n', 's', ' ', 't', 'o', ' ', 's', 'h', 'a', 'p', 'e', ' ', 'w', 'h', 'a', 't', ' ', 'y', 'o', 'u', ' ', 'd', 'o', ' ', 'n', 'e', 'x', 't', ' ', 'i', 'n', ' ', 't', 'h', 'e', ' ', 'p', 'r', 'o', 'c', 'e', 's', 's', '.', ' ', '#', '#', '#', ' ', 'N', 'e', 'u', 'r', 'a', 'l', ' ', 'N', 'e', 't', 'w', 'o', 'r', 'k', 's', ' ', 'a', 'n', 'd', ' ', 'R', 'e', 'g', 'u', 'l', 'a', 'r', 'i', 'z', 'a', 't', 'i', 'o', 'n', ' ', 'A', ' ', 'l', 'a', 'r', 'g', 'e', ' ', 'n', 'e', 'u', 'r', 'a', 'l', ' ', 'n', 'e', 't', 'w', 'o', 'r', 'k', ' ', 'w', 'i', 'l', 'l', ' ', 'u', 's', 'u', 'a', 'l', 'l', 'y', ' ', 'd', 'o', ' ', 'a', 's', ' ', 'w', 'e', 'l', 'l', ' ', 'o', 'r', ' ', 'b', 'e', 't', 't', 'e', 'r', ' ', 't', 'h', 'a', 'n', ' ', 'a', ' ', 's', 'm', 'a', 'l', 'l', 'e', 'r', ' ', 'o', 'n', 'e', ' ', 's', 'o', ' ', 'l', 'o', 'n', 'g', ' ', 'a', 's', ' ', 'r', 'e', 'g', 'u', 'l', 'a', 'r', 'i', 'z', 'a', 't', 'i', 'o', 'n', ' ', 'i', 's', ' ', 'c', 'h', 'o', 's', 'e', 'n', ' ', 'a', 'p', 'p', 'r', 'o', 'p', 'r', 'i', 'a', 't', 'e', 'l', 'y', '.', ' ', 'O', 'f', ' ', 'c', 'o', 'u', 'r', 's', 'e', ',', ' ', 'l', 'a', 'r', 'g', 'e', 'r', ' ', 'n', 'e', 'u', 'r', 'a', 'l', ' ', 'n', 'e', 't', 'w', 'o', 'r', 'k', 's', ' ', 'a', 'r', 'e', ' ', 'm', 'o', 'r', 'e', ' ', 'c', 'o', 'm', 'p', 'u', 't', 'a', 't', 'i', 'o', 'n', 'a', 'l', 'l', 'y', ' ', 'e', 'x', 'p', 'e', 'n', 's', 'i', 'v', 'e', '.', '#', '#', '#', '#', ' ', 'S', 'l', 'i', 'd', 'e', ' ', '!', 'S', 'c', 'r', 'e', 'e', 'n', 's', 'h', 'o', 't', ' ', '2', '0', '2', '3', '-', '1', '0', '-', '1', '7', ' ', 'a', 't', ' ', '1', '.', '2', '6', '.', '0', '0', ' ', 'P', 'M', '.', 'p', 'n', 'g', ' ', ' ', '#', '#', '#', ' ', 'U', 'n', 'd', 'e', 'r', 's', 't', 'a', 'n', 'd', 'i', 'n', 'g', ' ', 'B', 'i', 'a', 's', ' ', 'a', 'n', 'd', ' ', 'V', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', 'T', 'a', 'k', 'e', ' ', 'd', 'e', 'g', 'r', 'e', 'e', ' ', 'o', 'f', ' ', 'p', 'o', 'l', 'y', 'n', 'o', 'm', 'i', 'a', 'l', ' ', '$', 'd', '$', ' ', 'A', 's', ' ', '$', 'd', '$', ' ', 'i', 'n', 'c', 'r', 'e', 'a', 's', 'e', 's', ',', ' ', 't', 'h', 'a', 't', ' ', 'i', 's', ' ', 't', 'o', ' ', 's', 'a', 'y', ' ', 'w', 'e', ' ', 'a', 'd', 'd', ' ', 'd', 'e', 'g', 'r', 'e', 'e', 's', ' ', 't', 'o', ' ', 't', 'h', 'e', ' ', 'p', 'o', 'l', 'y', 'n', 'o', 'm', 'i', 'a', 'l', ' ', '$', 'J', '_', '{', 't', 'r', 'a', 'i', 'n', '}', '(', '{', '\\\\', 'v', 'e', 'c', '{', 'w', '}', ',', 'b', '}', ')', '$', ' ', ' ', 'w', 'i', 'l', 'l', ' ', 'd', 'e', 'c', 'r', 'e', 'a', 's', 'e', '.', ' ', '$', 'J', '_', '{', 'c', 'v', '}', '(', '\\\\', 'v', 'e', 'c', '{', 'w', '}', ',', 'b', ')', '$', ' ', 'd', 'i', 'p', 's', ' ', 't', 'o', 'w', 'a', 'r', 'd', 's', ' ', 't', 'h', 'e', ' ', 'm', 'i', 'd', 'd', 'l', 'e', ',', ' ', 's', 'h', 'o', 'w', 'i', 'n', 'g', ' ', 't', 'h', 'e', ' ', 'i', 'd', 'e', 'a', 'l', ' ', 'd', 'e', 'g', 'r', 'e', 'e', ' ', 'f', 'o', 'r', ' ', 't', 'h', 'e', ' ', 'p', 'o', 'l', 'y', 'n', 'o', 'm', 'i', 'a', 'l', '.', ' ', '#', '#', '#', '#', ' ', 'S', 'l', 'i', 'd', 'e', ' ', '!', 'S', 'c', 'r', 'e', 'e', 'n', 's', 'h', 'o', 't', ' ', '2', '0', '2', '3', '-', '1', '0', '-', '1', '7', ' ', 'a', 't', ' ', '1', '.', '2', '9', '.', '2', '1', ' ', 'P', 'M', '.', 'p', 'n', 'g', ' ', ' ', '#', '#', '#', ' ', 'T', 'e', 'c', 'h', 'n', 'i', 'q', 'u', 'e', ' ', 'f', 'o', 'r', ' ', 'D', 'i', 'a', 'g', 'n', 'o', 's', 'i', 'n', 'g', ' ', 'B', 'i', 'a', 's', ' ', 'a', 'n', 'd', ' ', 'V', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', 'H', 'o', 'w', ' ', 'd', 'o', ' ', 'y', 'o', 'u', ' ', 't', 'e', 'l', 'l', ' ', 'i', 'f', ' ', 'y', 'o', 'u', 'r', ' ', 'a', 'l', 'g', 'o', 'r', 'i', 't', 'h', 'm', ' ', 'h', 'a', 's', ' ', 'a', ' ', 'h', 'i', 'g', 'h', ' ', 'b', 'i', 'a', 's', ' ', 'o', 'r', ' ', 'v', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', 'p', 'r', 'o', 'b', 'l', 'e', 'm', '?', ' ', ' ', 'O', 'n', 'e', ' ', 'k', 'e', 'y', ' ', 't', 'a', 'k', 'e', 'a', 'w', 'a', 'y', ' ', 'i', 's', ' ', 't', 'h', 'a', 't', ' ', 'h', 'i', 'g', 'h', ' ', 'b', 'i', 'a', 's', ' ', 'm', 'e', 'a', 'n', 's', ' ', 'i', 't', ' ', 'i', 's', ' ', 'n', 'o', 't', ' ', 'e', 'v', 'e', 'n', ' ', 'd', 'o', 'i', 'n', 'g', ' ', 'w', 'e', 'l', 'l', ' ', 'o', 'n', ' ', 't', 'h', 'e', ' ', 't', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ' ', 's', 'e', 't', ' ', 'a', 'n', 'd', ' ', 'h', 'i', 'g', 'h', ' ', 'v', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', 'm', 'e', 'a', 'n', 's', ' ', 'i', 't', ' ', 'd', 'o', 'e', 's', ' ', 'm', 'u', 'c', 'h', ' ', 'w', 'o', 'r', 's', 'e', ' ', 'o', 'n', ' ', 't', 'h', 'e', ' ', 'c', 'r', 'o', 's', 's', ' ', 'v', 'a', 'l', 'i', 'd', 'a', 't', 'i', 'o', 'n', ' ', 's', 'e', 't', ' ', 'a', 'n', 'd', ' ', 't', 'h', 'e', ' ', 't', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ' ', 's', 'e', 't', '.', '#', '#', '#', '#', ' ', 'H', 'i', 'g', 'h', ' ', 'B', 'i', 'a', 's', ' ', '(', 'u', 'n', 'd', 'e', 'r', '-', 'f', 'i', 't', ')', ' ', 'I', 'n', ' ', 'a', 'n', ' ', 'u', 'n', 'd', 'e', 'r', '-', 'f', 'i', 't', ' ', 'm', 'o', 'd', 'e', 'l', ' ', '$', 'J', '_', '{', 't', 'r', 'a', 'i', 'n', '}', '$', ' ', 'w', 'i', 'l', 'l', ' ', 'b', 'e', ' ', 'h', 'i', 'g', 'h', ' ', 'a', 'n', 'd', ' ', '$', 'J', '_', '{', 't', 'r', 'a', 'i', 'n', '}', ' ', '\\\\', 'a', 'p', 'p', 'r', 'o', 'x', ' ', 'J', '_', '{', 'c', 'v', '}', '$', ' ', '#', '#', '#', '#', ' ', 'H', 'i', 'g', 'h', ' ', 'V', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', '(', 'o', 'v', 'e', 'r', 'f', 'i', 't', ')', ' ', 'I', 'n', ' ', 'a', 'n', ' ', 'o', 'v', 'e', 'r', '-', 'f', 'i', 't', ' ', 'm', 'o', 'd', 'e', 'l', ' ', '$', 'J', '_', '{', 'c', 'v', '}', ' ', '\\\\', 'g', 'g', ' ', 'J', '_', '{', 't', 'r', 'a', 'i', 'n', '}', '$', ' ', 'a', 'n', 'd', ' ', '$', 'J', '_', '{', 't', 'r', 'a', 'i', 'n', '}', '$', ' ', 'w', 'i', 'l', 'l', ' ', 'b', 'e', ' ', 'l', 'o', 'w', '.', '#', '#', '#', '#', ' ', 'H', 'i', 'g', 'h', ' ', 'B', 'i', 'a', 's', ' ', 'a', 'n', 'd', ' ', 'H', 'i', 'g', 'h', ' ', 'V', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', 'I', 'n', ' ', 'a', ' ', 'm', 'o', 'd', 'e', 'l', ' ', 'w', 'i', 't', 'h', ' ', 'b', 'o', 't', 'h', ' ', 'h', 'i', 'g', 'h', ' ', 'b', 'i', 'a', 's', ' ', 'a', 'n', 'd', ' ', 'h', 'i', 'g', 'h', ' ', 'v', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', '$', 'J', '_', '{', 't', 'r', 'a', 'i', 'n', '}', '$', ' ', 'w', 'i', 'l', 'l', ' ', 'b', 'e', ' ', 'h', 'i', 'g', 'h', ' ', 'a', 'n', 'd', ' ', '$', 'J', '_', '{', 'c', 'v', '}', ' ', '\\\\', 'g', 'g', ' ', 'J', '_', '{', 't', 'r', 'a', 'i', 'n', '}', '$', ' ', ' ', 'T', 'h', 'i', 's', ' ', 'i', 's', ' ', 'r', 'a', 'r', 'e', ',', ' ', 'a', 'n', 'd', ' ', 'd', 'o', 'e', 's', 'n', \"'\", 't', ' ', 'r', 'e', 'a', 'l', 'l', 'y', ' ', 'h', 'a', 'p', 'p', 'e', 'n', ' ', 'f', 'o', 'r', ' ', 'l', 'i', 'n', 'e', 'a', 'r', ' ', 'm', 'o', 'd', 'e', 'l', 's', ' ', 'w', 'i', 't', 'h', ' ', 'o', 'n', 'e', ' ', '$', 'd', '$', ',', ' ', 'b', 'u', 't', ' ', 'i', 't', ' ', 'd', 'o', 'e', 's', ' ', 'h', 'a', 'p', 'p', 'e', 'n', '.', ' ', 'T', 'h', 'i', 's', ' ', 'i', 's', ' ', 'w', 'h', 'e', 'n', ' ', 'i', 't', ' ', 'o', 'v', 'e', 'r', 'f', 'i', 't', 's', ' ', 'f', 'o', 'r', ' ', 's', 'o', 'm', 'e', ' ', 'p', 'a', 'r', 't', ' ', 'o', 'f', ' ', 't', 'h', 'e', ' ', 'i', 'n', 'p', 'u', 't', ' ', 'a', 'n', 'd', ' ', 'u', 'n', 'd', 'e', 'r', '-', 'f', 'i', 't', 's', ' ', 'f', 'o', 'r', ' ', 'a', 'n', 'o', 't', 'h', 'e', 'r', ' ', 'p', 'a', 'r', 't', ' ', 'o', 'f', ' ', 't', 'h', 'e', ' ', 'i', 'n', 'p', 'u', 't', '.', '#', '#', '#', '#', ' ', 'S', 'l', 'i', 'd', 'e', ' ', '!', 'S', 'c', 'r', 'e', 'e', 'n', 's', 'h', 'o', 't', ' ', '2', '0', '2', '3', '-', '1', '0', '-', '1', '7', ' ', 'a', 't', ' ', '1', '.', '3', '8', '.', '5', '1', ' ', 'P', 'M', '.', 'p', 'n', 'g', ' ', ' ', '#', '#', ' ', 'R', 'e', 'g', 'u', 'l', 'a', 'r', 'i', 'z', 'a', 't', 'i', 'o', 'n', ' ', 'a', 'n', 'd', ' ', 'b', 'i', 'a', 's', '/', 'v', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', '#', '#', '#', ' ', 'L', 'i', 'n', 'e', 'a', 'r', ' ', 'R', 'e', 'g', 'r', 'e', 's', 's', 'i', 'o', 'n', ' ', 'w', 'i', 't', 'h', ' ', 'R', 'e', 'g', 'u', 'l', 'a', 'r', 'i', 'z', 'a', 't', 'i', 'o', 'n', ' ', 'R', 'e', 'g', 'u', 'l', 'a', 'r', 'i', 'z', 'e', 'd', ' ', 'L', 'i', 'n', 'e', 'a', 'r', ' ', 'R', 'e', 'g', 'r', 'e', 's', 's', 'i', 'o', 'n', ' ', 'm', 'o', 'd', 'e', 'l', ':', ' ', 'L', 'a', 'm', 'b', 'd', 'a', ' ', '$', '\\\\', 'l', 'a', 'm', 'b', 'd', 'a', '$', ' ', 'i', 's', ' ', 't', 'h', 'e', ' ', 'r', 'e', 'g', 'u', 'l', 'a', 'r', 'i', 'z', 'a', 't', 'i', 'o', 'n', ' ', 'p', 'a', 'r', 'a', 'm', 'e', 't', 'e', 'r', ' ', '(', 'B', 'i', 'g', ' ', '#', 'm', 'e', 'r', 'g', 'e', ' ', 'a', 'n', 'd', ' ', 'c', 'l', 'e', 'a', 'n', 'u', 'p', ' ', 'n', 'e', 'e', 'd', 'e', 'd', ' ', 'h', 'e', 'r', 'e', ' ', 'i', 'n', ' ', 't', 'h', 'e', ' ', 'r', 'e', 'g', 'u', 'l', 'a', 'r', 'i', 'z', 'a', 't', 'i', 'o', 'n', ',', ' ', 'R', 'e', 'g', 'u', 'l', 'a', 'r', 'i', 'z', 'e', 'd', ' ', 'L', 'i', 'n', 'e', 'a', 'r', ' ', 'R', 'e', 'g', 'r', 'e', 's', 's', 'i', 'o', 'n', ',', ' ', 'c', 'o', 's', 't', ',', ' ', 'C', 'o', 's', 't', ' ', 'a', 'n', 'd', ' ', 'L', 'o', 's', 's', ',', ' ', 'l', 'o', 's', 's', ',', ' ', 'L', 'o', 's', 's', ' ', 'F', 'u', 'n', 'c', 't', 'i', 'o', 'n', ',', ' ', 'e', 't', 'c', '.', ' ', 'A', 'l', 'i', 'a', 's', 'e', 's', ' ', 'a', 'r', 'e', ' ', 'n', 'e', 'e', 'd', 'e', 'd', ' ', 'i', 'n', ' ', 'm', 'a', 'n', 'y', ' ', 'o', 'f', ' ', 't', 'h', 'e', 's', 'e', ')', ' ', ' ', 'T', 'a', 'k', 'e', ' ', 't', 'h', 'i', 's', ' ', 'm', 'o', 'd', 'e', 'l', ':', ' ', '$', '$', 'f', '_', '{', '\\\\', 'v', 'e', 'c', '{', 'w', '}', ',', 'b', '}', '(', 'x', ')', '=', 'w', '_', '1', '+', 'w', '_', '2', 'x', '^', '2', ' ', '+', ' ', 'w', '_', '4', 'x', '^', '4', '+', 'b', '$', '$', ' ', 'W', 'i', 't', 'h', ' ', 't', 'h', 'i', 's', ' ', 'r', 'e', 'g', 'u', 'l', 'a', 'r', 'i', 'z', 'a', 't', 'i', 'o', 'n', '|', 'r', 'e', 'g', 'u', 'l', 'a', 'r', 'i', 'z', 'e', 'd', ' ', 'C', 'o', 's', 't', ' ', 'a', 'n', 'd', ' ', 'L', 'o', 's', 's', ' ', '$', '$', 'J', '(', '\\\\', 'v', 'e', 'c', '{', 'w', '}', ',', 'b', ')', '=', '\\\\', 'f', 'r', 'a', 'c', '{', '1', '}', '{', '2', 'm', '}', ' ', '\\\\', 's', 'u', 'm', '_', '{', 'i', '=', '1', '}', '^', '{', 'm', '}', '(', 'f', '_', '{', '\\\\', 'v', 'e', 'c', '{', 'w', '}', ',', '{', 'b', '}', '}', '(', '\\\\', 'v', 'e', 'c', '{', 'x', '}', '^', '{', '(', 'i', ')', '}', '-', 'y', '^', '{', '(', 'i', ')', '}', ')', '^', '2', '+', '\\\\', 'f', 'r', 'a', 'c', '{', '\\\\', 'l', 'a', 'm', 'b', 'd', 'a', '}', '{', '2', 'm', '}', ' ', '\\\\', 's', 'u', 'm', '_', '{', 'j', '=', '1', '}', '^', 'n', 'w', '_', 'j', '^', '2', '$', '$', ' ', ' ', 'I', 'f', ' ', 'l', 'a', 'm', 'b', 'd', 'a', ' ', 'i', 's', ' ', 'l', 'a', 'r', 'g', 'e', ',', ' ', 's', 'a', 'y', ' ', '$', '\\\\', 'l', 'a', 'm', 'b', 'd', 'a', ' ', '=', ' ', '1', '0', ',', '0', '0', '0', '$', ' ', 't', 'h', 'a', 'n', ' ', '$', 'w', '_', '1', ' ', '\\\\', 'a', 'p', 'p', 'r', 'o', 'x', ' ', '0', ',', ' ', 'W', '_', '2', ' ', '\\\\', 'a', 'p', 'p', 'r', 'o', 'x', ' ', '0', '$', ' ', 'a', 'n', 'd', ' ', '$', 'f', '_', '{', '\\\\', 'v', 'e', 'c', '{', 'x', '}', ',', 'b', '}', '\\\\', 'v', 'e', 'c', '{', 'x', '}', ' ', '\\\\', 'a', 'p', 'p', 'r', 'o', 'x', ' ', 'b', '$', ',', ' ', 't', 'h', 'u', 's', ' ', 't', 'h', 'e', ' ', 'm', 'o', 'd', 'e', 'l', ' ', 'c', 'r', 'e', 'a', 't', 'e', 's', ' ', 'a', ' ', 'f', 'l', 'a', 't', ' ', 'l', 'i', 'n', 'e', '.', ' ', 'T', 'h', 'i', 's', ' ', 'i', 's', ' ', 'u', 'n', 'd', 'e', 'r', 'f', 'i', 't', ',', ' ', '$', 'J', '_', '{', 't', 'r', 'a', 'i', 'n', '}', '(', ')', ' ', ' ', 'O', 'n', ' ', 't', 'h', 'e', ' ', 'o', 't', 'h', 'e', 'r', ' ', 'h', 'a', 'n', 'd', ',', ' ', 'i', 'f', ' ', 'w', 'e', ' ', 's', 'e', 't', ' ', '$', '\\\\', 'l', 'a', 'm', 'b', 'd', 'a', '=', '0', '$', ',', ' ', 't', 'h', 'e', 'n', ' ', 'w', 'e', ' ', 'h', 'a', 'v', 'e', ' ', 'a', ' ', 'f', 'o', 'r', 't', 'h', ' ', 'o', 'r', 'd', 'e', 'r', ' ', 'p', 'o', 'l', 'y', 'n', 'o', 'm', 'i', 'a', 'l', ' ', 'w', 'i', 't', 'h', ' ', 'n', 'o', ' ', 'r', 'e', 'g', 'u', 'l', 'a', 'r', 'i', 'z', 'a', 't', 'i', 'o', 'n', '.', ' ', 'W', 'e', ' ', 'e', 'n', 'd', ' ', 'u', 'p', ' ', 'w', 'i', 't', 'h', ' ', 'a', ' ', 'v', 'e', 'r', 'y', ' ', 'o', 'v', 'e', 'r', 'f', 'i', 't', ' ', 'c', 'u', 'r', 'v', 'e', '.', ' ', ' ', 'S', 'o', ',', ' ', 'h', 'o', 'w', ' ', 'd', 'o', ' ', 'w', 'e', ' ', 'f', 'i', 'n', 'd', ' ', 'a', ' ', 'g', 'o', 'o', 'd', ' ', 'v', 'a', 'l', 'u', 'e', ' ', 'f', 'o', 'r', ' ', '$', '\\\\', 'l', 'a', 'm', 'b', 'd', 'a', '$', '?', ' ', '#', '#', '#', '#', '#', ' ', 'S', 'l', 'i', 'd', 'e', ' ', '!', 'S', 'c', 'r', 'e', 'e', 'n', 's', 'h', 'o', 't', ' ', '2', '0', '2', '3', '-', '1', '0', '-', '1', '7', ' ', 'a', 't', ' ', '3', '.', '5', '6', '.', '5', '3', ' ', 'P', 'M', '.', 'p', 'n', 'g', ' ', '#', '#', '#', ' ', 'C', 'h', 'o', 'o', 's', 'i', 'n', 'g', ' ', 't', 'h', 'e', ' ', 'R', 'e', 'g', 'u', 'l', 'a', 'r', 'i', 'z', 'a', 't', 'i', 'o', 'n', ' ', 'p', 'a', 'r', 'a', 'm', 'e', 't', 'e', 'r', ' ', '$', '\\\\', 'l', 'a', 'm', 'b', 'd', 'a', '$', ' ', ' ', ' ', 'T', 'h', 'i', 's', ' ', 'w', 'i', 'l', 'l', ' ', 'b', 'e', ' ', 's', 'i', 'm', 'i', 'l', 'a', 'r', ' ', 't', 'o', ' ', 'c', 'h', 'o', 'o', 's', 'i', 'n', 'g', ' ', '$', 'd', '$', ' ', 'w', 'i', 't', 'h', ' ', 'c', 'r', 'o', 's', 's', ' ', 'v', 'a', 'l', 'i', 'd', 'a', 't', 'i', 'o', 'n', ' ', 'T', 'r', 'y', ' ', 'm', 'u', 'l', 't', 'i', 'p', 'l', 'e', ' ', 'v', 'a', 'l', 'u', 'e', 's', ' ', 'f', 'o', 'r', ' ', '$', '\\\\', 'l', 'a', 'm', 'b', 'd', 'a', '$', ' ', 'a', 'n', 'd', ' ', 't', 'h', 'e', 'n', ' ', 'c', 'h', 'o', 'o', 's', 'e', ' ', 't', 'h', 'e', ' ', 'o', 'p', 't', 'i', 'o', 'n', ' ', 'w', 'i', 't', 'h', ' ', 't', 'h', 'e', ' ', 'l', 'o', 'w', 'e', 's', 't', ' ', 'c', 'o', 's', 't', '.', ' ', '#', '#', '#', ' ', 'S', 'l', 'i', 'd', 'e', ' ', '!', 'S', 'c', 'r', 'e', 'e', 'n', 's', 'h', 'o', 't', ' ', '2', '0', '2', '3', '-', '1', '0', '-', '1', '7', ' ', 'a', 't', ' ', '4', '.', '1', '5', '.', '2', '7', ' ', 'P', 'M', '.', 'p', 'n', 'g', ' ', '#', '#', '#', ' ', 'B', 'i', 'a', 's', ' ', 'a', 'n', 'd', ' ', 'V', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', 'a', 's', ' ', 'a', ' ', 'F', 'u', 'n', 'c', 't', 'i', 'o', 'n', ' ', 'o', 'f', ' ', 'r', 'e', 'g', 'u', 'l', 'a', 'r', 'i', 'z', 'a', 't', 'i', 'o', 'n', ' ', 'p', 'a', 'r', 'a', 'm', 'e', 't', 'e', 'r', ' ', '$', '\\\\', 'l', 'a', 'm', 'b', 'd', 'a', '$', ' ', ' ', 'C', 'r', 'o', 's', 's', ' ', 'V', 'a', 'l', 'i', 'd', 'a', 't', 'i', 'o', 'n', ' ', 't', 'r', 'i', 'e', 's', ' ', 'o', 'u', 't', ' ', 'm', 'a', 'n', 'y', ' ', 'v', 'e', 'r', 's', 'i', 'o', 'n', 's', ' ', 'o', 'f', ' ', '$', '\\\\', 'l', 'a', 'm', 'b', 'd', 'a', '$', ' ', 'a', 'n', 'd', ' ', 't', 'h', 'e', 'n', ' ', 'c', 'h', 'o', 'o', 's', 'e', 's', ' ', 't', 'h', 'e', ' ', 'o', 'n', 'e', ' ', 'w', 'i', 't', 'h', ' ', 't', 'h', 'e', ' ', 'l', 'o', 'w', 'e', 's', 't', ' ', 'c', 'o', 's', 't', '.', ' ', 'C', 'r', 'o', 's', 's', ' ', 'v', 'a', 'l', 'i', 'd', 'a', 't', 'i', 'o', 'n', ' ', 'w', 'i', 'l', 'l', ' ', 'h', 'e', 'l', 'p', ' ', 'f', 'i', 'n', 'd', ' ', 'u', 's', ' ', 'a', ' ', 'a', ' ', 'g', 'o', 'o', 'd', ' ', 'v', 'a', 'l', 'u', 'e', ' ', 'o', 'f', ' ', '$', 'd', '$', ' ', 'a', 's', ' ', 'w', 'e', 'l', 'l', ' ', 'a', 's', ' ', '$', '\\\\', 'l', 'a', 'm', 'b', 'd', 'a', '$', ' ', '#', '#', '#', '#', ' ', 'S', 'l', 'i', 'd', 'e', ' ', '!', 'S', 'c', 'r', 'e', 'e', 'n', 's', 'h', 'o', 't', ' ', '2', '0', '2', '3', '-', '1', '0', '-', '1', '7', ' ', 'a', 't', ' ', '4', '.', '1', '4', '.', '0', '7', ' ', 'P', 'M', '.', 'p', 'n', 'g', ' ', ' ', ' ', '#', '#', ' ', 'E', 's', 't', 'a', 'b', 'l', 'i', 's', 'h', 'i', 'n', 'g', ' ', 'A', ' ', 'B', 'a', 's', 'e', 'l', 'i', 'n', 'e', ' ', 'L', 'e', 'v', 'e', 'l', ' ', 'o', 'f', ' ', 'P', 'e', 'r', 'f', 'o', 'r', 'm', 'a', 'n', 'c', 'e', ' ', '#', '#', '#', ' ', 'S', 'p', 'e', 'e', 'c', 'h', ' ', 'R', 'e', 'c', 'o', 'g', 'n', 'i', 't', 'i', 'o', 'n', ' ', 'e', 'x', 'a', 'm', 'p', 'l', 'e', ' ', 'J', 'o', 'b', ' ', 'i', 's', ' ', 't', 'o', ' ', 't', 'a', 'k', 'e', ' ', 'i', 'n', ' ', 'a', 'u', 'd', 'i', 'o', ' ', 'a', 'n', 'd', ' ', 'o', 'u', 't', 'p', 'u', 't', ' ', 't', 'h', 'e', ' ', 't', 'e', 'x', 't', ' ', 'o', 'f', ' ', 'w', 'h', 'a', 't', ' ', 'a', ' ', 'p', 'e', 'r', 's', 'o', 'n', ' ', 'i', 's', ' ', 's', 'a', 'y', 'i', 'n', 'g', ' ', ' ', 'T', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ' ', 'e', 'r', 'r', 'o', 'r', ' ', '$', 'J', '_', '{', 't', 'r', 'a', 'i', 'n', '}', '$', ' ', 'i', 's', ' ', 'p', 'e', 'r', 'c', 'e', 'n', 't', 'a', 'g', 'e', ' ', 'o', 'f', ' ', 'a', 'u', 'd', 'i', 'o', ' ', 'c', 'l', 'i', 'p', 's', ' ', 't', 'h', 'a', 't', ' ', 't', 'h', 'e', ' ', 'p', 'r', 'o', 'g', 'r', 'a', 'm', ' ', 'd', 'o', 'e', 's', ' ', 'n', 'o', 't', ' ', 't', 'r', 'a', 'n', 's', 'c', 'r', 'i', 'b', 'e', ' ', 'c', 'o', 'r', 'r', 'e', 'c', 't', 'l', 'y', ' ', 'i', 'n', ' ', 'i', 't', \"'\", 's', ' ', 'e', 'n', 't', 'i', 'r', 'e', 't', 'y', '.', ' ', 'L', 'e', 't', 's', ' ', 's', 'a', 'y', ':', ' ', '\\t', 'H', 'u', 'm', 'a', 'n', ' ', 'L', 'e', 'v', 'e', 'l', ' ', 'P', 'e', 'r', 'f', 'o', 'r', 'm', 'a', 'n', 'c', 'e', ':', ' ', '1', '0', '.', '6', ' ', '\\t', '$', 'J', '_', '{', 't', 'r', 'a', 'i', 'n', '}', '$', ':', ' ', '1', '0', '.', '8', '%', ' ', '\\t', '$', 'J', '_', '{', 'c', 'v', '}', '$', ':', ' ', '1', '4', '.', '8', '%', ' ', ' ', 'W', 'h', 'y', ' ', 'i', 's', ' ', 'h', 'u', 'm', 'a', 'n', ' ', 'l', 'e', 'v', 'e', 'l', ' ', 'e', 'r', 'r', 'o', 'r', ' ', 's', 'o', ' ', 'h', 'i', 'g', 'h', '?', ' ', 'T', 'h', 'e', 'r', 'e', ' ', 'i', 's', ' ', 'l', 'o', 't', 's', ' ', 'o', 'f', ' ', 'n', 'o', 'i', 's', 'e', ' ', 'i', 'n', ' ', 't', 'h', 'e', ' ', 'a', 'u', 'd', 'i', 'o', '.', ' ', 'I', 't', ' ', 's', 'e', 'e', 'm', 's', ' ', 'u', 'n', 'f', 'a', 'i', 'r', ' ', 't', 'o', ' ', 'e', 'x', 'p', 'e', 'c', 't', ' ', 'a', ' ', 'l', 'e', 'a', 'r', 'n', 'i', 'n', 'g', ' ', 'a', 'l', 'g', 'o', 'r', 'i', 't', 'h', 'm', ' ', 't', 'o', ' ', 'd', 'o', ' ', 'm', 'u', 'c', 'h', ' ', 'b', 'e', 't', 't', 'e', 'r', '.', ' ', 'I', 't', ' ', 'i', 's', ' ', 't', 'h', 'u', 's', ' ', 'i', 's', ' ', 'm', 'o', 'r', 'e', ' ', 'u', 's', 'e', 'f', 'u', 'l', ' ', 't', 'o', ' ', 'm', 'e', 'a', 's', 'u', 'r', 'e', ' ', 't', 'h', 'e', ' ', 't', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ' ', 'e', 'r', 'r', 'o', 'r', ' ', 'a', 'g', 'a', 'i', 'n', 's', 't', ' ', 't', 'h', 'e', ' ', 'h', 'u', 'm', 'a', 'n', ' ', 'e', 'r', 'r', 'o', 'r', '.', ' ', 'S', 'o', ',', ' ', 'l', 'o', 'o', 'k', 'i', 'n', 'g', ' ', 'a', 't', ' ', 't', 'h', 'e', 's', 'e', ' ', 'r', 'e', 's', 'u', 'l', 't', 's', ',', ' ', '$', 'J', '_', '{', 't', 'r', 'a', 'i', 'n', '}', '$', ' ', 'i', 's', ' ', 'o', 'n', 'l', 'y', ' ', '0', '.', '2', '$', ' ', 'h', 'i', 'g', 'h', 'e', 'r', ' ', 't', 'h', 'a', 'n', ' ', 't', 'h', 'e', ' ', 'h', 'u', 'm', 'a', 'n', ' ', 'l', 'e', 'v', 'e', 'l', ' ', 'p', 'e', 'r', 'f', 'o', 'r', 'm', 'a', 'n', 'c', 'e', ' ', 'w', 'h', 'e', 'r', 'e', 'a', 's', ' ', '$', 'J', '_', '{', 'c', 'v', '}', '$', ' ', 'i', 's', ' ', 'a', ' ', 'f', 'u', 'l', 'l', ' ', '4', '.', '2', '%', ' ', 'h', 'i', 'g', 'h', 'e', 'r', '.', ' ', 'W', 'e', ' ', 'c', 'a', 'n', ' ', 't', 'h', 'u', 's', ' ', 'c', 'o', 'n', 'c', 'l', 'u', 'd', 'e', ' ', 't', 'h', 'a', 't', ' ', 't', 'h', 'i', 's', ' ', 'a', 'l', 'g', 'o', 'r', 'i', 't', 'h', 'm', ' ', 'h', 'a', 's', ' ', 'm', 'o', 'r', 'e', ' ', 'o', 'f', ' ', 'a', ' ', 'v', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', 'p', 'r', 'o', 'b', 'l', 'e', 'm', ' ', 't', 'h', 'a', 'n', ' ', 'a', ' ', 'b', 'i', 'a', 's', ' ', 'p', 'r', 'o', 'b', 'l', 'e', 'm', '.', ' ', '#', '#', '#', ' ', 'E', 's', 't', 'a', 'b', 'l', 'i', 's', 'h', 'i', 'n', 'g', ' ', 'a', ' ', 'b', 'a', 's', 'e', 'l', 'i', 'n', 'e', ' ', 'l', 'e', 'v', 'e', 'l', ' ', 'o', 'f', ' ', 'p', 'e', 'r', 'f', 'o', 'r', 'm', 'a', 'n', 'c', 'e', ' ', 'W', 'h', 'a', 't', ' ', 'i', 's', ' ', 't', 'h', 'e', ' ', 'l', 'e', 'v', 'e', 'l', ' ', 'o', 'f', ' ', 'e', 'r', 'r', 'o', 'r', ' ', 'y', 'o', 'u', ' ', 'c', 'a', 'n', ' ', 'r', 'e', 'a', 's', 'o', 'n', 'a', 'b', 'l', 'y', ' ', 'h', 'o', 'p', 'e', ' ', 't', 'o', ' ', 'g', 'e', 't', ' ', 't', 'o', '?', ' ', '-', ' ', 'H', 'u', 'm', 'a', 'n', ' ', 'l', 'e', 'v', 'e', 'l', ' ', 'p', 'e', 'r', 'f', 'o', 'r', 'm', 'a', 'n', 'c', 'e', ' ', '-', ' ', 'C', 'o', 'm', 'p', 'e', 't', 'i', 'n', 'g', ' ', 'a', 'l', 'g', 'o', 'r', 'i', 't', 'h', 'm', 's', ' ', 'p', 'e', 'r', 'f', 'o', 'r', 'm', 'a', 'n', 'c', 'e', ' ', '-', ' ', 'G', 'u', 'e', 's', 's', ' ', 'b', 'a', 's', 'e', 'd', ' ', 'o', 'n', ' ', 'e', 'x', 'p', 'e', 'r', 'i', 'e', 'n', 'c', 'e', ' ', '#', '#', '#', ' ', 'B', 'i', 'a', 's', ' ', 'V', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', 'E', 'x', 'a', 'm', 'p', 'l', 'e', 's', ' ', 'G', 'a', 'p', ' ', 'b', 'e', 't', 'w', 'e', 'e', 'n', ' ', 'b', 'a', 's', 'e', 'l', 'i', 'n', 'e', ' ', 'a', 'n', 'd', ' ', 't', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ' ', 'e', 'r', 'r', 'o', 'r', ' ', 's', 'h', 'o', 'w', 's', ' ', 'h', 'i', 'g', 'h', ' ', 'b', 'i', 'a', 's', '.', ' ', 'A', ' ', 'g', 'a', 'p', ' ', 'b', 'e', 't', 'w', 'e', 'e', 'n', ' ', 't', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ' ', 'e', 'r', 'r', 'o', 'r', ' ', 'a', 'n', 'd', ' ', 'c', 'r', 'o', 's', 's', ' ', 'v', 'a', 'l', 'i', 'd', 'a', 't', 'i', 'o', 'n', ' ', 'e', 'r', 'r', 'o', 'r', ' ', 's', 'h', 'o', 'w', 's', ' ', 'h', 'i', 'g', 'h', ' ', 'v', 'a', 'r', 'i', 'a', 'n', 'c', 'e', '.', ' ', ' ', 'I', 'f', ' ', 'y', 'o', 'u', 'r', ' ', 'g', 'o', 'a', 'l', ' ', 'i', 's', ' ', 'p', 'e', 'r', 'f', 'e', 'c', 't', 'i', 'o', 'n', ',', ' ', 't', 'h', 'e', ' ', 'b', 'a', 's', 'e', 'l', 'i', 'n', 'e', ' ', 'w', 'o', 'u', 'l', 'd', ' ', 'b', 'e', ' ', 'z', 'e', 'r', 'o', '.', ' ', 'B', 'u', 't', ' ', 'f', 'o', 'r', ' ', 'a', ' ', 'l', 'o', 't', ' ', 'o', 'f', ' ', 'r', 'e', 'a', 'l', ' ', 'w', 'o', 'r', 'l', 'd', ' ', 'e', 'x', 'a', 'm', 'p', 'l', 'e', 's', ',', ' ', 'l', 'i', 'k', 'e', ' ', 'a', 'u', 'd', 'i', 'o', ' ', 'r', 'e', 'c', 'o', 'g', 'n', 'i', 't', 'i', 'o', 'n', ',', ' ', 't', 'h', 'e', 'r', 'e', ' ', 'i', 's', ' ', 'a', ' ', 'l', 'o', 't', ' ', 'o', 'f', ' ', 'n', 'o', 'i', 's', 'e', ' ', 'i', 'n', ' ', 't', 'h', 'e', ' ', 'd', 'a', 't', 'a', ',', ' ', 's', 'o', ' ', 'y', 'o', 'u', ' ', 'n', 'e', 'e', 'd', ' ', 'a', ' ', 'h', 'i', 'g', 'h', 'e', 'r', ' ', 'b', 'a', 's', 'e', 'l', 'i', 'n', 'e', '.', ' ', ' ', 'I', 'f', ' ', 't', 'h', 'e', 'r', 'e', ' ', 'i', 's', ' ', 'a', ' ', 'g', 'a', 'p', ' ', 'b', 'e', 't', 'w', 'e', 'e', 'n', ' ', 'a', 'l', 'l', ' ', 't', 'h', 'r', 'e', 'e', ' ', 'i', 's', ' ', 'h', 'i', 'g', 'h', ' ', 'y', 'o', 'u', ' ', 'h', 'a', 'v', 'e', ' ', 'b', 'o', 't', 'h', ' ', 'h', 'i', 'g', 'h', ' ', 'b', 'i', 'a', 's', ' ', 'a', 'n', 'd', ' ', 'h', 'i', 'g', 'h', ' ', 'v', 'a', 'r', 'i', 'a', 'n', 'c', 'e', '.', '#', '#', '#', '#', ' ', 'S', 'l', 'i', 'd', 'e', ' ', '!', 'S', 'c', 'r', 'e', 'e', 'n', 's', 'h', 'o', 't', ' ', '2', '0', '2', '3', '-', '1', '0', '-', '1', '7', ' ', 'a', 't', ' ', '4', '.', '4', '2', '.', '4', '9', ' ', 'P', 'M', '.', 'p', 'n', 'g', ' ', ' ', ' ', '#', '#', ' ', 'L', 'e', 'a', 'r', 'n', 'i', 'n', 'g', ' ', 'C', 'u', 'r', 'v', 'e', 's', ' ', ' ', 'N', 'o', 't', 'e', 'd', ' ', 'i', 'n', ' ', 'L', 'e', 'a', 'r', 'n', 'i', 'n', 'g', ' ', 'C', 'u', 'r', 'v', 'e', 's', ' ', '#', '#', '#', ' ', 'O', 'v', 'e', 'r', 'v', 'i', 'e', 'w', ' ', 'L', 'e', 'a', 'r', 'n', 'i', 'n', 'g', ' ', 'c', 'u', 'r', 'v', 'e', 's', ' ', 'h', 'e', 'l', 'p', ' ', 'u', 'n', 'd', 'e', 'r', 's', 't', 'a', 'n', 'd', ' ', 'h', 'o', 'w', ' ', 'y', 'o', 'u', 'r', ' ', 'l', 'e', 'a', 'r', 'n', 'i', 'n', 'g', ' ', 'a', 'l', 'g', 'o', 'r', 'i', 't', 'h', 'm', ' ', 'i', 's', ' ', 'd', 'o', 'i', 'n', 'g', ' ', 'a', 's', ' ', 'a', ' ', 'f', 'u', 'n', 'c', 't', 'i', 'o', 'n', ' ', 'o', 'f', ' ', 't', 'h', 'e', ' ', 'a', 'm', 'o', 'u', 'n', 't', ' ', 'o', 'f', ' ', 'e', 'x', 'p', 'e', 'r', 'i', 'e', 'n', 'c', 'e', ' ', 'i', 't', ' ', 'h', 'a', 's', '.', ' ', 'E', 'x', 'p', 'e', 'r', 'i', 'e', 'n', 'c', 'e', ' ', 'b', 'e', 'i', 'n', 'g', ' ', 't', 'h', 'e', ' ', 'n', 'u', 'm', 'b', 'e', 'r', ' ', 'o', 'f', ' ', 't', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ' ', 'e', 'x', 'a', 'm', 'p', 'l', 'e', 's', ' ', 'i', 't', ' ', 'h', 'a', 's', '.', ' ', ' ', 'T', 'h', 'e', ' ', 'b', 'i', 'g', 'g', 'e', 'r', ' ', 't', 'h', 'e', ' ', 't', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ' ', 's', 'e', 't', ' ', 't', 'h', 'e', ' ', 'h', 'a', 'r', 'd', 'e', 'r', ' ', 'i', 't', ' ', 'i', 's', ' ', 't', 'o', ' ', 'f', 'i', 't', ' ', 'a', 'l', 'l', ' ', 'e', 'x', 'a', 'm', 'p', 'l', 'e', 's', ' ', 'p', 'e', 'r', 'f', 'e', 'c', 't', 'l', 'y', '.', ' ', 'T', 'h', 'u', 's', ';', ' ', 'a', 's', ' ', 't', 'h', 'e', ' ', 't', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ' ', 's', 'e', 't', ' ', 'i', 'n', 'c', 'r', 'e', 'a', 's', 'e', 's', ' ', 's', 'o', ' ', 'd', 'o', 'e', 's', ' ', 't', 'h', 'e', ' ', 't', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ' ', 'e', 'r', 'r', 'o', 'r', ' ', '$', 'J', '_', '{', 't', 'r', 'a', 'i', 'n', '}', '(', '\\\\', 'v', 'e', 'c', '{', 'w', '}', ',', 'b', ')', '$', ' ', ' ', ' ', 'P', 'l', 'o', 't', 't', 'i', 'n', 'g', ' ', 'a', ' ', 'l', 'e', 'a', 'r', 'n', 'i', 'n', 'g', ' ', 'c', 'u', 'r', 'v', 'e', ' ', 'b', 'y', ' ', 't', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ' ', 'd', 'i', 'f', 'f', 'e', 'r', 'e', 'n', 't', ' ', 'm', 'o', 'd', 'e', 'l', 's', ' ', 'b', 'a', 's', 'e', 'd', ' ', 'o', 'n', ' ', 'd', 'i', 'f', 'f', 'e', 'r', 'e', 'n', 't', ' ', 's', 'u', 'b', 's', 'e', 't', 's', ' ', 'o', 'f', ' ', 't', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ' ', 'd', 'a', 't', 'a', ' ', 'i', 's', ' ', 'c', 'o', 'm', 'p', 'u', 't', 'a', 't', 'i', 'o', 'n', 'a', 'l', 'l', 'y', ' ', 'e', 'x', 'p', 'e', 'n', 's', 'i', 'v', 'e', ',', ' ', 's', 'o', ' ', 'i', 'n', ' ', 'p', 'r', 'a', 'c', 't', 'i', 'c', 'e', ' ', 'i', 't', ' ', 'i', 's', 'n', \"'\", 't', ' ', 'd', 'o', 'n', 'e', ' ', 't', 'h', 'a', 't', ' ', 'o', 'f', 't', 'e', 'n', '.', ' ', 'B', 'u', 't', ',', ' ', 'i', 't', \"'\", 's', ' ', 'a', ' ', 'g', 'o', 'o', 'd', ' ', 'm', 'e', 'n', 't', 'a', 'l', ' ', 'v', 'i', 's', 'u', 'a', 'l', 'i', 'z', 'a', 't', 'i', 'o', 'n', '.', '#', '#', '#', '#', ' ', 'S', 'l', 'i', 'd', 'e', 's', ' ', '!', 'S', 'c', 'r', 'e', 'e', 'n', 's', 'h', 'o', 't', ' ', '2', '0', '2', '3', '-', '1', '0', '-', '1', '7', ' ', 'a', 't', ' ', '5', '.', '4', '0', '.', '0', '4', ' ', 'P', 'M', '.', 'p', 'n', 'g', ' ', ' ', ' ', ' ', '#', '#', '#', ' ', 'H', 'i', 'g', 'h', ' ', 'B', 'i', 'a', 's', ' ', 'E', 'x', 'a', 'm', 'p', 'l', 'e', ' ', 'I', 'f', ' ', 'a', ' ', 'l', 'e', 'a', 'r', 'n', 'i', 'n', 'g', ' ', 'a', 'l', 'g', 'o', 'r', 'i', 't', 'h', 'm', ' ', 's', 'u', 'f', 'f', 'e', 'r', 's', ' ', 'f', 'r', 'o', 'm', ' ', 'h', 'i', 'g', 'h', ' ', 'b', 'i', 'a', 's', ',', ' ', 'g', 'e', 't', 't', 'i', 'n', 'g', ' ', 'm', 'o', 'r', 'e', ' ', 't', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ' ', 'd', 'a', 't', 'a', ' ', 'w', 'i', 'l', 'l', ' ', 'n', 'o', 't', ' ', '(', 'b', 'y', ' ', 'i', 't', 's', 'e', 'l', 'f', ')', ' ', 'h', 'e', 'l', 'p', ' ', 't', 'h', 'a', 't', ' ', 'm', 'u', 'c', 'h', '.', '#', '#', '#', '#', ' ', 'S', 'l', 'i', 'd', 'e', ' ', 'f', 'o', 'r', ' ', 'H', 'i', 'g', 'h', ' ', 'B', 'i', 'a', 's', ' ', '!', 'S', 'c', 'r', 'e', 'e', 'n', 's', 'h', 'o', 't', ' ', '2', '0', '2', '3', '-', '1', '0', '-', '1', '7', ' ', 'a', 't', ' ', '5', '.', '4', '4', '.', '2', '9', ' ', 'P', 'M', '.', 'p', 'n', 'g', ' ', ' ', '#', '#', '#', ' ', 'H', 'i', 'g', 'h', ' ', 'V', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', 'E', 'x', 'a', 'm', 'p', 'l', 'e', ' ', 'I', 'f', ' ', 'a', ' ', 'l', 'e', 'a', 'r', 'n', 'i', 'n', 'g', ' ', 'a', 'l', 'g', 'o', 'r', 'i', 't', 'h', 'm', ' ', 's', 'u', 'f', 'f', 'e', 'r', 's', ' ', 'f', 'r', 'o', 'm', ' ', 'h', 'i', 'g', 'h', ' ', 'v', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ',', ' ', 'g', 'e', 't', 't', 'i', 'n', 'g', ' ', 'm', 'o', 'r', 'e', ' ', 't', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ' ', 'd', 'a', 't', 'a', ' ', 'i', 's', ' ', 'l', 'i', 'k', 'e', 'l', 'y', ' ', 't', 'o', ' ', 'h', 'e', 'l', 'p', '.', '#', '#', '#', '#', ' ', 'S', 'l', 'i', 'd', 'e', ' ', 'f', 'o', 'r', ' ', 'H', 'i', 'g', 'h', ' ', 'V', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', '!', 'S', 'c', 'r', 'e', 'e', 'n', 's', 'h', 'o', 't', ' ', '2', '0', '2', '3', '-', '1', '0', '-', '1', '7', ' ', 'a', 't', ' ', '5', '.', '4', '9', '.', '4', '5', ' ', 'P', 'M', '.', 'p', 'n', 'g', ' ', ' ', ' ', ' ', '#', '#', ' ', 'D', 'e', 'c', 'i', 'd', 'i', 'n', 'g', ' ', 'w', 'h', 'a', 't', ' ', 't', 'o', ' ', 't', 'r', 'y', ' ', 'n', 'e', 'x', 't', ' ', 'r', 'e', 'v', 'i', 's', 'i', 't', 'e', 'd', ' ', '#', '#', '#', ' ', 'E', 'x', 'a', 'm', 'p', 'l', 'e', 's', ' ', 'w', 'h', 'e', 'n', ' ', 'D', 'e', 'b', 'u', 'g', 'g', 'i', 'n', 'g', ' ', 'a', 'n', ' ', 'A', 'l', 'g', 'o', 'r', 'i', 't', 'h', 'm', ':', ' ', 'G', 'e', 't', ' ', 'm', 'o', 'r', 'e', ' ', 't', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ' ', 'e', 'x', 'a', 'm', 'p', 'l', 'e', 's', ':', ' ', 'f', 'i', 'x', 'e', 's', ' ', 'h', 'i', 'g', 'h', ' ', 'v', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', 'T', 'r', 'y', ' ', 's', 'm', 'a', 'l', 'l', 'e', 'r', ' ', 's', 'e', 't', ' ', 'o', 'f', ' ', 'f', 'e', 'a', 't', 'u', 'r', 'e', 's', ':', ' ', 'f', 'i', 'x', 'e', 's', ' ', 'h', 'i', 'g', 'h', ' ', 'v', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', 'T', 'r', 'y', ' ', 'g', 'e', 't', 't', 'i', 'n', 'g', ' ', 'a', 'd', 'd', 'i', 't', 'i', 'o', 'n', 'a', 'l', ' ', 'f', 'e', 'a', 't', 'u', 'r', 'e', 's', ':', ' ', 'f', 'i', 'x', 'e', 's', ' ', 'h', 'i', 'g', 'h', ' ', 'b', 'i', 'a', 's', ' ', '\\t', 'E', 'x', 'a', 'm', 'p', 'l', 'e', 's', ';', ' ', 'a', 'n', ' ', 'a', 'l', 'g', 'o', 'r', 'i', 't', 'h', 'm', ' ', 't', 'h', 'a', 't', ' ', 'l', 'a', 'c', 'k', 's', ' ', 'i', 'n', 'f', 'o', 'r', 'm', 'a', 't', 'i', 'o', 'n', ' ', 'w', 'o', 'n', 't', ' ', 'e', 'v', 'e', 'n', ' ', 'd', 'o', ' ', 'w', 'e', 'l', 'l', ' ', 'o', 'n', ' ', 't', 'h', 'e', ' ', 't', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ' ', 's', 'e', 't', ' ', 'A', 'd', 'd', 'i', 'n', 'g', ' ', 'p', 'o', 'l', 'y', 'n', 'o', 'm', 'i', 'a', 'l', ' ', 'f', 'e', 'a', 't', 'u', 'r', 'e', 's', ' ', '(', '$', 'x', '_', '1', '^', '2', ',', 'X', '_', '2', '^', '2', ',', ' ', 'e', 't', 'c', '$', ')', ':', ' ', 'F', 'i', 'x', 'e', 's', ' ', 'h', 'i', 'g', 'h', ' ', 'b', 'i', 'a', 's', ' ', 'D', 'e', 'c', 'r', 'e', 'a', 's', 'i', 'n', 'g', ' ', '$', '\\\\', 'l', 'a', 'm', 'b', 'd', 'a', '$', ' ', 'F', 'i', 'x', 'e', 's', ' ', 'h', 'i', 'g', 'h', ' ', 'b', 'i', 'a', 's', ' ', 'I', 'n', 'c', 'r', 'e', 'a', 's', 'i', 'n', 'g', ' ', '$', '\\\\', 'l', 'a', 'm', 'b', 'd', 'a', '$', ' ', 'F', 'i', 'x', 'e', 's', ' ', 'h', 'i', 'g', 'h', ' ', 'v', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', '\\t', 'F', 'o', 'r', 'c', 'e', 's', ' ', 't', 'h', 'e', ' ', 'a', 'l', 'g', 'o', 'r', 'i', 't', 'h', 'm', ' ', 't', 'o', ' ', 'f', 'o', 'r', 'c', 'e', ' ', 'a', ' ', 's', 'm', 'o', 'o', 't', 'h', 'e', 'r', ' ', 'f', 'u', 'n', 'c', 't', 'i', 'o', 'n', '.', ' ', ' ', '*', '*', 'N', 'o', 't', 'e', '!', '*', '*', ' ', 'D', 'o', 'n', \"'\", 't', ' ', 'r', 'a', 'n', 'd', 'o', 'm', 'l', 'y', ' ', 't', 'h', 'r', 'o', 'w', ' ', 'a', 'w', 'a', 'y', ' ', 't', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ' ', 'e', 'x', 'a', 'm', 'p', 'l', 'e', 's', ' ', 'j', 'u', 's', 't', ' ', 't', 'o', ' ', 'f', 'i', 'x', ' ', 'a', ' ', 'h', 'i', 'g', 'h', ' ', 'b', 'i', 'a', 's', ' ', 'p', 'r', 'o', 'b', 'l', 'e', 'm', '.', ' ', '#', '#', '#', ' ', 'T', 'a', 'k', 'e', 'a', 'w', 'a', 'y', ' ', '#', 'm', 'e', 'r', 'g', 'e', ' ', 'w', 'i', 't', 'h', ' ', 'B', 'i', 'a', 's', ' ', 'a', 'n', 'd', ' ', 'V', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', '?', ' ', 'I', 'f', ' ', 'y', 'o', 'u', 'r', ' ', 'a', 'l', 'g', 'o', 'r', 'i', 't', 'h', 'm', ' ', 'h', 'a', 's', ' ', 'h', 'i', 'g', 'h', ' ', 'v', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ',', ' ', 't', 'r', 'y', ' ', 's', 'i', 'm', 'p', 'l', 'i', 'f', 'y', 'i', 'n', 'g', ' ', 'y', 'o', 'u', 'r', ' ', 'm', 'o', 'd', 'e', 'l', ' ', 'o', 'r', ' ', 'g', 'e', 't', 't', 'i', 'n', 'g', ' ', 'm', 'o', 'r', 'e', ' ', 't', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ' ', 'd', 'a', 't', 'a', '.', ' ', 'S', 'i', 'm', 'p', 'l', 'i', 'f', 'i', 'c', 'a', 't', 'i', 'o', 'n', ' ', 'c', 'a', 'n', ' ', 'm', 'e', 'a', 'n', ' ', 'a', ' ', 's', 'm', 'a', 'l', 'l', 'e', 'r', ' ', 's', 'e', 't', ' ', 'o', 'f', ' ', 'f', 'e', 'a', 't', 'u', 'r', 'e', 's', ' ', 'o', 'r', ' ', 'a', 'n', ' ', 'i', 'n', 'c', 'r', 'e', 'a', 's', 'e', 'd', ' ', 'r', 'e', 'g', 'u', 'l', 'a', 'r', 'i', 'z', 'a', 't', 'i', 'o', 'n', ' ', 'I', 'f', ' ', 'y', 'o', 'u', 'r', ' ', 'a', 'l', 'g', 'o', 'r', 'i', 't', 'h', 'm', ' ', 'h', 'a', 's', ' ', 'h', 'i', 'g', 'h', ' ', 'b', 'i', 'a', 's', ',', ' ', 't', 'h', 'a', 't', ' ', 'i', 's', ' ', 't', 'o', ' ', 's', 'a', 'y', ' ', 'i', 't', 's', ' ', 'n', 'o', 't', ' ', 'e', 'v', 'e', 'n', ' ', 'd', 'o', 'i', 'n', 'g', ' ', 'w', 'e', 'l', 'l', ' ', 'o', 'n', ' ', 't', 'h', 'e', ' ', 't', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ' ', 's', 'e', 't', ',', ' ', 'y', 'o', 'u', ' ', 'm', 'a', 'i', 'n', 'l', 'y', ' ', 'n', 'e', 'e', 'd', ' ', 't', 'o', ' ', 'm', 'a', 'k', 'e', ' ', 'y', 'o', 'u', 'r', ' ', 'm', 'o', 'd', 'e', 'l', ' ', 'm', 'o', 'r', 'e', ' ', 'p', 'o', 'w', 'e', 'r', 'f', 'u', 'l', ' ', 'a', 'n', 'd', ' ', 'f', 'l', 'e', 'x', 'i', 'b', 'l', 'e', ' ', 't', 'o', ' ', 'f', 'i', 't', ' ', 'm', 'o', 'r', 'e', ' ', 'c', 'o', 'm', 'p', 'l', 'e', 'x', ' ', 'f', 'u', 'n', 'c', 't', 'i', 'o', 'n', 's', '.', ' ', 'T', 'o', ' ', 'd', 'o', ' ', 's', 'o', ' ', 'y', 'o', 'u', ' ', 'c', 'a', 'n', ' ', 'g', 'i', 'v', 'e', ' ', 'i', 't', ' ', 'a', 'd', 'd', 'i', 't', 'i', 'o', 'n', 'a', 'l', ' ', 'f', 'e', 'a', 't', 'u', 'r', 'e', 's', ',', ' ', 'a', 'd', 'd', ' ', 'p', 'o', 'l', 'y', 'n', 'o', 'm', 'i', 'a', 'l', ' ', 'f', 'e', 'a', 't', 'u', 'r', 'e', 's', ',', ' ', 'o', 'r', ' ', 'd', 'e', 'c', 'r', 'e', 'a', 's', 'e', ' ', '$', '\\\\', 'l', 'a', 'm', 'b', 'd', 'a', '$', ' ', ' ', ' ', '#', '#', ' ', 'B', 'i', 'a', 's', '/', 'V', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', 'a', 'n', 'd', ' ', 'N', 'e', 'u', 'r', 'a', 'l', ' ', 'N', 'e', 't', 'w', 'o', 'r', 'k', 's', ' ', '#', '#', '#', ' ', 'T', 'h', 'e', ' ', 'b', 'i', 'a', 's', ' ', 'v', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', 't', 'r', 'a', 'd', 'e', 'o', 'f', 'f', ' ', 'S', 'i', 'm', 'p', 'l', 'e', ' ', 'm', 'o', 'd', 'e', 'l', ' ', '=', ' ', 'h', 'i', 'g', 'h', ' ', 'b', 'i', 'a', 's', ' ', 'C', 'o', 'm', 'p', 'l', 'e', 'x', ' ', 'm', 'o', 'd', 'e', 'l', ' ', '=', ' ', 'h', 'i', 'g', 'h', ' ', 'v', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', 'B', 'e', 'f', 'o', 'r', 'e', ' ', 'n', 'e', 'u', 'r', 'a', 'l', ' ', 'n', 'e', 't', 'w', 'o', 'r', 'k', 's', ',', ' ', 'w', 'e', ' ', 'h', 'a', 'd', ' ', 't', 'o', ' ', 'w', 'o', 'r', 'r', 'y', ' ', 'a', 'b', 'o', 'u', 't', ' ', 'b', 'a', 'l', 'a', 'n', 'c', 'i', 'n', 'g', ' ', 't', 'h', 'i', 's', ' ', 'c', 'o', 'm', 'p', 'l', 'e', 'x', 'i', 't', 'y', ' ', 'b', 'e', 't', 'w', 'e', 'e', 'n', ' ', 'b', 'i', 'a', 's', ' ', 'a', 'n', 'd', ' ', 'v', 'a', 'r', 'i', 'a', 'n', 'c', 'e', '.', ' ', 'W', 'i', 't', 'h', ' ', 'n', 'e', 'u', 'r', 'a', 'l', ' ', 'n', 'e', 't', 'w', 'o', 'r', 'k', 's', ' ', 'w', 'e', ' ', 'n', 'o', 'w', ' ', 'a', 'r', 'e', ' ', 'm', 'o', 's', 't', 'l', 'y', ' ', 'w', 'o', 'r', 'r', 'i', 'e', 'd', ' ', 'a', 'b', 'o', 'u', 't', ' ', 'h', 'i', 'g', 'h', ' ', 'v', 'a', 'r', 'i', 'a', 'n', 'c', 'e', '.', ' ', ' ', '#', '#', '#', ' ', 'N', 'e', 'u', 'r', 'a', 'l', ' ', 'N', 'e', 't', 'w', 'o', 'r', 'k', 's', ' ', 'a', 'n', 'd', ' ', 'b', 'i', 'a', 's', ' ', 'v', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', 'L', 'a', 'r', 'g', 'e', ' ', 'N', 'e', 'u', 'r', 'a', 'l', ' ', 'N', 'e', 't', 'w', 'o', 'r', 'k', '|', 'n', 'e', 'u', 'r', 'a', 'l', ' ', 'n', 'e', 't', 'w', 'o', 'r', 'k', 's', ' ', 'a', 'r', 'e', ' ', 'l', 'o', 'w', ' ', 'b', 'i', 'a', 's', ' ', 'm', 'a', 'c', 'h', 'i', 'n', 'e', 's', '.', ' ', 'I', 'f', ' ', 'y', 'o', 'u', ' ', 'm', 'a', 'k', 'e', ' ', 'y', 'o', 'u', 'r', ' ', 'n', 'e', 'u', 'r', 'a', 'l', ' ', 'n', 'e', 't', 'w', 'o', 'r', 'k', ' ', 'l', 'a', 'r', 'g', 'e', ' ', 'e', 'n', 'o', 'u', 'g', 'h', ' ', 'y', 'o', 'u', ' ', 'c', 'a', 'n', ' ', 'a', 'l', 'm', 'o', 's', 't', ' ', 'a', 'l', 'w', 'a', 'y', 's', ' ', 'f', 'i', 't', ' ', 'y', 'o', 'u', 'r', ' ', 't', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ' ', 's', 'e', 't', ' ', 'w', 'e', 'l', 'l', '.', '#', '#', '#', '#', '#', ' ', 'S', 'l', 'i', 'd', 'e', ' ', '!', 'S', 'c', 'r', 'e', 'e', 'n', 's', 'h', 'o', 't', ' ', '2', '0', '2', '3', '-', '1', '0', '-', '1', '7', ' ', 'a', 't', ' ', '3', '.', '5', '6', '.', '5', '3', ' ', 'P', 'M', '.', 'p', 'n', 'g', ' ', '#', '#', '#', ' ', 'C', 'h', 'o', 'o', 's', 'i', 'n', 'g', ' ', 't', 'h', 'e', ' ', 'R', 'e', 'g', 'u', 'l', 'a', 'r', 'i', 'z', 'a', 't', 'i', 'o', 'n', ' ', 'p', 'a', 'r', 'a', 'm', 'e', 't', 'e', 'r', ' ', '$', '\\\\', 'l', 'a', 'm', 'b', 'd', 'a', '$', ' ', ' ', ' ', 'T', 'h', 'i', 's', ' ', 'w', 'i', 'l', 'l', ' ', 'b', 'e', ' ', 's', 'i', 'm', 'i', 'l', 'a', 'r', ' ', 't', 'o', ' ', 'c', 'h', 'o', 'o', 's', 'i', 'n', 'g', ' ', '$', 'd', '$', ' ', 'w', 'i', 't', 'h', ' ', 'c', 'r', 'o', 's', 's', ' ', 'v', 'a', 'l', 'i', 'd', 'a', 't', 'i', 'o', 'n', ' ', 'T', 'r', 'y', ' ', 'm', 'u', 'l', 't', 'i', 'p', 'l', 'e', ' ', 'v', 'a', 'l', 'u', 'e', 's', ' ', 'f', 'o', 'r', ' ', '$', '\\\\', 'l', 'a', 'm', 'b', 'd', 'a', '$', ' ', 'a', 'n', 'd', ' ', 't', 'h', 'e', 'n', ' ', 'c', 'h', 'o', 'o', 's', 'e', ' ', 't', 'h', 'e', ' ', 'o', 'p', 't', 'i', 'o', 'n', ' ', 'w', 'i', 't', 'h', ' ', 't', 'h', 'e', ' ', 'l', 'o', 'w', 'e', 's', 't', ' ', 'c', 'o', 's', 't', '.', ' ', '#', '#', '#', ' ', 'S', 'l', 'i', 'd', 'e', ' ', '!', 'S', 'c', 'r', 'e', 'e', 'n', 's', 'h', 'o', 't', ' ', '2', '0', '2', '3', '-', '1', '0', '-', '1', '7', ' ', 'a', 't', ' ', '4', '.', '1', '5', '.', '2', '7', ' ', 'P', 'M', '.', 'p', 'n', 'g', ' ', '#', '#', '#', ' ', 'B', 'i', 'a', 's', ' ', 'a', 'n', 'd', ' ', 'V', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', 'a', 's', ' ', 'a', ' ', 'F', 'u', 'n', 'c', 't', 'i', 'o', 'n', ' ', 'o', 'f', ' ', 'r', 'e', 'g', 'u', 'l', 'a', 'r', 'i', 'z', 'a', 't', 'i', 'o', 'n', ' ', 'p', 'a', 'r', 'a', 'm', 'e', 't', 'e', 'r', ' ', '$', '\\\\', 'l', 'a', 'm', 'b', 'd', 'a', '$', ' ', ' ', 'C', 'r', 'o', 's', 's', ' ', 'V', 'a', 'l', 'i', 'd', 'a', 't', 'i', 'o', 'n', ' ', 't', 'r', 'i', 'e', 's', ' ', 'o', 'u', 't', ' ', 'm', 'a', 'n', 'y', ' ', 'v', 'e', 'r', 's', 'i', 'o', 'n', 's', ' ', 'o', 'f', ' ', '$', '\\\\', 'l', 'a', 'm', 'b', 'd', 'a', '$', ' ', 'a', 'n', 'd', ' ', 't', 'h', 'e', 'n', ' ', 'c', 'h', 'o', 'o', 's', 'e', 's', ' ', 't', 'h', 'e', ' ', 'o', 'n', 'e', ' ', 'w', 'i', 't', 'h', ' ', 't', 'h', 'e', ' ', 'l', 'o', 'w', 'e', 's', 't', ' ', 'c', 'o', 's', 't', '.', ' ', 'C', 'r', 'o', 's', 's', ' ', 'v', 'a', 'l', 'i', 'd', 'a', 't', 'i', 'o', 'n', ' ', 'w', 'i', 'l', 'l', ' ', 'h', 'e', 'l', 'p', ' ', 'f', 'i', 'n', 'd', ' ', 'u', 's', ' ', 'a', ' ', 'a', ' ', 'g', 'o', 'o', 'd', ' ', 'v', 'a', 'l', 'u', 'e', ' ', 'o', 'f', ' ', '$', 'd', '$', ' ', 'a', 's', ' ', 'w', 'e', 'l', 'l', ' ', 'a', 's', ' ', '$', '\\\\', 'l', 'a', 'm', 'b', 'd', 'a', '$', ' ', ' ', '#', '#', '#', '#', ' ', 'S', 'l', 'i', 'd', 'e', ' ', '!', 'S', 'c', 'r', 'e', 'e', 'n', 's', 'h', 'o', 't', ' ', '2', '0', '2', '3', '-', '1', '0', '-', '1', '7', ' ', 'a', 't', ' ', '4', '.', '1', '4', '.', '0', '7', ' ', 'P', 'M', '.', 'p', 'n', 'g', ' ', ' ', ' ', '#', '#', ' ', 'E', 's', 't', 'a', 'b', 'l', 'i', 's', 'h', 'i', 'n', 'g', ' ', 'A', ' ', 'B', 'a', 's', 'e', 'l', 'i', 'n', 'e', ' ', 'L', 'e', 'v', 'e', 'l', ' ', 'o', 'f', ' ', 'P', 'e', 'r', 'f', 'o', 'r', 'm', 'a', 'n', 'c', 'e', ' ', '#', '#', '#', ' ', 'S', 'p', 'e', 'e', 'c', 'h', ' ', 'R', 'e', 'c', 'o', 'g', 'n', 'i', 't', 'i', 'o', 'n', ' ', 'e', 'x', 'a', 'm', 'p', 'l', 'e', ' ', 'J', 'o', 'b', ' ', 'i', 's', ' ', 't', 'o', ' ', 't', 'a', 'k', 'e', ' ', 'i', 'n', ' ', 'a', 'u', 'd', 'i', 'o', ' ', 'a', 'n', 'd', ' ', 'o', 'u', 't', 'p', 'u', 't', ' ', 't', 'h', 'e', ' ', 't', 'e', 'x', 't', ' ', 'o', 'f', ' ', 'w', 'h', 'a', 't', ' ', 'a', ' ', 'p', 'e', 'r', 's', 'o', 'n', ' ', 'i', 's', ' ', 's', 'a', 'y', 'i', 'n', 'g', ' ', ' ', 'T', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ' ', 'e', 'r', 'r', 'o', 'r', ' ', '$', 'J', '_', '{', 't', 'r', 'a', 'i', 'n', '}', '$', ' ', 'i', 's', ' ', 'p', 'e', 'r', 'c', 'e', 'n', 't', 'a', 'g', 'e', ' ', 'o', 'f', ' ', 'a', 'u', 'd', 'i', 'o', ' ', 'c', 'l', 'i', 'p', 's', ' ', 't', 'h', 'a', 't', ' ', 't', 'h', 'e', ' ', 'p', 'r', 'o', 'g', 'r', 'a', 'm', ' ', 'd', 'o', 'e', 's', ' ', 'n', 'o', 't', ' ', 't', 'r', 'a', 'n', 's', 'c', 'r', 'i', 'b', 'e', ' ', 'c', 'o', 'r', 'r', 'e', 'c', 't', 'l', 'y', ' ', 'i', 'n', ' ', 'i', 't', \"'\", 's', ' ', 'e', 'n', 't', 'i', 'r', 'e', 't', 'y', '.', ' ', 'L', 'e', 't', 's', ' ', 's', 'a', 'y', ':', ' ', '\\t', 'H', 'u', 'm', 'a', 'n', ' ', 'L', 'e', 'v', 'e', 'l', ' ', 'P', 'e', 'r', 'f', 'o', 'r', 'm', 'a', 'n', 'c', 'e', ':', ' ', '1', '0', '.', '6', ' ', '\\t', '$', 'J', '_', '{', 't', 'r', 'a', 'i', 'n', '}', '$', ':', ' ', '1', '0', '.', '8', '%', ' ', '\\t', '$', 'J', '_', '{', 'c', 'v', '}', '$', ':', ' ', '1', '4', '.', '8', '%', ' ', ' ', 'W', 'h', 'y', ' ', 'i', 's', ' ', 'h', 'u', 'm', 'a', 'n', ' ', 'l', 'e', 'v', 'e', 'l', ' ', 'e', 'r', 'r', 'o', 'r', ' ', 's', 'o', ' ', 'h', 'i', 'g', 'h', '?', ' ', 'T', 'h', 'e', 'r', 'e', ' ', 'i', 's', ' ', 'l', 'o', 't', 's', ' ', 'o', 'f', ' ', 'n', 'o', 'i', 's', 'e', ' ', 'i', 'n', ' ', 't', 'h', 'e', ' ', 'a', 'u', 'd', 'i', 'o', '.', ' ', 'I', 't', ' ', 's', 'e', 'e', 'm', 's', ' ', 'u', 'n', 'f', 'a', 'i', 'r', ' ', 't', 'o', ' ', 'e', 'x', 'p', 'e', 'c', 't', ' ', 'a', ' ', 'l', 'e', 'a', 'r', 'n', 'i', 'n', 'g', ' ', 'a', 'l', 'g', 'o', 'r', 'i', 't', 'h', 'm', ' ', 't', 'o', ' ', 'd', 'o', ' ', 'm', 'u', 'c', 'h', ' ', 'b', 'e', 't', 't', 'e', 'r', '.', ' ', 'I', 't', ' ', 'i', 's', ' ', 't', 'h', 'u', 's', ' ', 'i', 's', ' ', 'm', 'o', 'r', 'e', ' ', 'u', 's', 'e', 'f', 'u', 'l', ' ', 't', 'o', ' ', 'm', 'e', 'a', 's', 'u', 'r', 'e', ' ', 't', 'h', 'e', ' ', 't', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ' ', 'e', 'r', 'r', 'o', 'r', ' ', 'a', 'g', 'a', 'i', 'n', 's', 't', ' ', 't', 'h', 'e', ' ', 'h', 'u', 'm', 'a', 'n', ' ', 'e', 'r', 'r', 'o', 'r', '.', ' ', 'S', 'o', ',', ' ', 'l', 'o', 'o', 'k', 'i', 'n', 'g', ' ', 'a', 't', ' ', 't', 'h', 'e', 's', 'e', ' ', 'r', 'e', 's', 'u', 'l', 't', 's', ',', ' ', '$', 'J', '_', '{', 't', 'r', 'a', 'i', 'n', '}', '$', ' ', 'i', 's', ' ', 'o', 'n', 'l', 'y', ' ', '0', '.', '2', '$', ' ', 'h', 'i', 'g', 'h', 'e', 'r', ' ', 't', 'h', 'a', 'n', ' ', 't', 'h', 'e', ' ', 'h', 'u', 'm', 'a', 'n', ' ', 'l', 'e', 'v', 'e', 'l', ' ', 'p', 'e', 'r', 'f', 'o', 'r', 'm', 'a', 'n', 'c', 'e', ' ', 'w', 'h', 'e', 'r', 'e', 'a', 's', ' ', '$', 'J', '_', '{', 'c', 'v', '}', '$', ' ', 'i', 's', ' ', 'a', ' ', 'f', 'u', 'l', 'l', ' ', '4', '.', '2', '%', ' ', 'h', 'i', 'g', 'h', 'e', 'r', '.', ' ', 'W', 'e', ' ', 'c', 'a', 'n', ' ', 't', 'h', 'u', 's', ' ', 'c', 'o', 'n', 'c', 'l', 'u', 'd', 'e', ' ', 't', 'h', 'a', 't', ' ', 't', 'h', 'i', 's', ' ', 'a', 'l', 'g', 'o', 'r', 'i', 't', 'h', 'm', ' ', 'h', 'a', 's', ' ', 'm', 'o', 'r', 'e', ' ', 'o', 'f', ' ', 'a', ' ', 'v', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', 'p', 'r', 'o', 'b', 'l', 'e', 'm', ' ', 't', 'h', 'a', 'n', ' ', 'a', ' ', 'b', 'i', 'a', 's', ' ', 'p', 'r', 'o', 'b', 'l', 'e', 'm', '.', ' ', '#', '#', '#', ' ', 'E', 's', 't', 'a', 'b', 'l', 'i', 's', 'h', 'i', 'n', 'g', ' ', 'a', ' ', 'b', 'a', 's', 'e', 'l', 'i', 'n', 'e', ' ', 'l', 'e', 'v', 'e', 'l', ' ', 'o', 'f', ' ', 'p', 'e', 'r', 'f', 'o', 'r', 'm', 'a', 'n', 'c', 'e', ' ', 'W', 'h', 'a', 't', ' ', 'i', 's', ' ', 't', 'h', 'e', ' ', 'l', 'e', 'v', 'e', 'l', ' ', 'o', 'f', ' ', 'e', 'r', 'r', 'o', 'r', ' ', 'y', 'o', 'u', ' ', 'c', 'a', 'n', ' ', 'r', 'e', 'a', 's', 'o', 'n', 'a', 'b', 'l', 'y', ' ', 'h', 'o', 'p', 'e', ' ', 't', 'o', ' ', 'g', 'e', 't', ' ', 't', 'o', '?', ' ', '-', ' ', 'H', 'u', 'm', 'a', 'n', ' ', 'l', 'e', 'v', 'e', 'l', ' ', 'p', 'e', 'r', 'f', 'o', 'r', 'm', 'a', 'n', 'c', 'e', ' ', '-', ' ', 'C', 'o', 'm', 'p', 'e', 't', 'i', 'n', 'g', ' ', 'a', 'l', 'g', 'o', 'r', 'i', 't', 'h', 'm', 's', ' ', 'p', 'e', 'r', 'f', 'o', 'r', 'm', 'a', 'n', 'c', 'e', ' ', '-', ' ', 'G', 'u', 'e', 's', 's', ' ', 'b', 'a', 's', 'e', 'd', ' ', 'o', 'n', ' ', 'e', 'x', 'p', 'e', 'r', 'i', 'e', 'n', 'c', 'e', ' ', '#', '#', '#', ' ', 'B', 'i', 'a', 's', ' ', 'V', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', 'E', 'x', 'a', 'm', 'p', 'l', 'e', 's', ' ', 'G', 'a', 'p', ' ', 'b', 'e', 't', 'w', 'e', 'e', 'n', ' ', 'b', 'a', 's', 'e', 'l', 'i', 'n', 'e', ' ', 'a', 'n', 'd', ' ', 't', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ' ', 'e', 'r', 'r', 'o', 'r', ' ', 's', 'h', 'o', 'w', 's', ' ', 'h', 'i', 'g', 'h', ' ', 'b', 'i', 'a', 's', '.', ' ', 'A', ' ', 'g', 'a', 'p', ' ', 'b', 'e', 't', 'w', 'e', 'e', 'n', ' ', 't', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ' ', 'e', 'r', 'r', 'o', 'r', ' ', 'a', 'n', 'd', ' ', 'c', 'r', 'o', 's', 's', ' ', 'v', 'a', 'l', 'i', 'd', 'a', 't', 'i', 'o', 'n', ' ', 'e', 'r', 'r', 'o', 'r', ' ', 's', 'h', 'o', 'w', 's', ' ', 'h', 'i', 'g', 'h', ' ', 'v', 'a', 'r', 'i', 'a', 'n', 'c', 'e', '.', ' ', ' ', 'I', 'f', ' ', 'y', 'o', 'u', 'r', ' ', 'g', 'o', 'a', 'l', ' ', 'i', 's', ' ', 'p', 'e', 'r', 'f', 'e', 'c', 't', 'i', 'o', 'n', ',', ' ', 't', 'h', 'e', ' ', 'b', 'a', 's', 'e', 'l', 'i', 'n', 'e', ' ', 'w', 'o', 'u', 'l', 'd', ' ', 'b', 'e', ' ', 'z', 'e', 'r', 'o', '.', ' ', 'B', 'u', 't', ' ', 'f', 'o', 'r', ' ', 'a', ' ', 'l', 'o', 't', ' ', 'o', 'f', ' ', 'r', 'e', 'a', 'l', ' ', 'w', 'o', 'r', 'l', 'd', ' ', 'e', 'x', 'a', 'm', 'p', 'l', 'e', 's', ',', ' ', 'l', 'i', 'k', 'e', ' ', 'a', 'u', 'd', 'i', 'o', ' ', 'r', 'e', 'c', 'o', 'g', 'n', 'i', 't', 'i', 'o', 'n', ',', ' ', 't', 'h', 'e', 'r', 'e', ' ', 'i', 's', ' ', 'a', ' ', 'l', 'o', 't', ' ', 'o', 'f', ' ', 'n', 'o', 'i', 's', 'e', ' ', 'i', 'n', ' ', 't', 'h', 'e', ' ', 'd', 'a', 't', 'a', ',', ' ', 's', 'o', ' ', 'y', 'o', 'u', ' ', 'n', 'e', 'e', 'd', ' ', 'a', ' ', 'h', 'i', 'g', 'h', 'e', 'r', ' ', 'b', 'a', 's', 'e', 'l', 'i', 'n', 'e', '.', ' ', ' ', 'I', 'f', ' ', 't', 'h', 'e', 'r', 'e', ' ', 'i', 's', ' ', 'a', ' ', 'g', 'a', 'p', ' ', 'b', 'e', 't', 'w', 'e', 'e', 'n', ' ', 'a', 'l', 'l', ' ', 't', 'h', 'r', 'e', 'e', ' ', 'i', 's', ' ', 'h', 'i', 'g', 'h', ' ', 'y', 'o', 'u', ' ', 'h', 'a', 'v', 'e', ' ', 'b', 'o', 't', 'h', ' ', 'h', 'i', 'g', 'h', ' ', 'b', 'i', 'a', 's', ' ', 'a', 'n', 'd', ' ', 'h', 'i', 'g', 'h', ' ', 'v', 'a', 'r', 'i', 'a', 'n', 'c', 'e', '.', ' ', '#', '#', '#', '#', ' ', 'S', 'l', 'i', 'd', 'e', ' ', '!', 'S', 'c', 'r', 'e', 'e', 'n', 's', 'h', 'o', 't', ' ', '2', '0', '2', '3', '-', '1', '0', '-', '1', '7', ' ', 'a', 't', ' ', '4', '.', '4', '2', '.', '4', '9', ' ', 'P', 'M', '.', 'p', 'n', 'g', ' ', ' ', ' ', '#', '#', ' ', 'L', 'e', 'a', 'r', 'n', 'i', 'n', 'g', ' ', 'C', 'u', 'r', 'v', 'e', 's', ' ', ' ', 'N', 'o', 't', 'e', 'd', ' ', 'i', 'n', ' ', 'L', 'e', 'a', 'r', 'n', 'i', 'n', 'g', ' ', 'C', 'u', 'r', 'v', 'e', 's', ' ', '#', '#', '#', ' ', 'O', 'v', 'e', 'r', 'v', 'i', 'e', 'w', ' ', 'L', 'e', 'a', 'r', 'n', 'i', 'n', 'g', ' ', 'c', 'u', 'r', 'v', 'e', 's', ' ', 'h', 'e', 'l', 'p', ' ', 'u', 'n', 'd', 'e', 'r', 's', 't', 'a', 'n', 'd', ' ', 'h', 'o', 'w', ' ', 'y', 'o', 'u', 'r', ' ', 'l', 'e', 'a', 'r', 'n', 'i', 'n', 'g', ' ', 'a', 'l', 'g', 'o', 'r', 'i', 't', 'h', 'm', ' ', 'i', 's', ' ', 'd', 'o', 'i', 'n', 'g', ' ', 'a', 's', ' ', 'a', ' ', 'f', 'u', 'n', 'c', 't', 'i', 'o', 'n', ' ', 'o', 'f', ' ', 't', 'h', 'e', ' ', 'a', 'm', 'o', 'u', 'n', 't', ' ', 'o', 'f', ' ', 'e', 'x', 'p', 'e', 'r', 'i', 'e', 'n', 'c', 'e', ' ', 'i', 't', ' ', 'h', 'a', 's', '.', ' ', 'E', 'x', 'p', 'e', 'r', 'i', 'e', 'n', 'c', 'e', ' ', 'b', 'e', 'i', 'n', 'g', ' ', 't', 'h', 'e', ' ', 'n', 'u', 'm', 'b', 'e', 'r', ' ', 'o', 'f', ' ', 't', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ' ', 'e', 'x', 'a', 'm', 'p', 'l', 'e', 's', ' ', 'i', 't', ' ', 'h', 'a', 's', '.', ' ', ' ', 'T', 'h', 'e', ' ', 'b', 'i', 'g', 'g', 'e', 'r', ' ', 't', 'h', 'e', ' ', 't', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ' ', 's', 'e', 't', ' ', 't', 'h', 'e', ' ', 'h', 'a', 'r', 'd', 'e', 'r', ' ', 'i', 't', ' ', 'i', 's', ' ', 't', 'o', ' ', 'f', 'i', 't', ' ', 'a', 'l', 'l', ' ', 'e', 'x', 'a', 'm', 'p', 'l', 'e', 's', ' ', 'p', 'e', 'r', 'f', 'e', 'c', 't', 'l', 'y', '.', ' ', 'T', 'h', 'u', 's', ';', ' ', 'a', 's', ' ', 't', 'h', 'e', ' ', 't', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ' ', 's', 'e', 't', ' ', 'i', 'n', 'c', 'r', 'e', 'a', 's', 'e', 's', ' ', 's', 'o', ' ', 'd', 'o', 'e', 's', ' ', 't', 'h', 'e', ' ', 't', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ' ', 'e', 'r', 'r', 'o', 'r', ' ', '$', 'J', '_', '{', 't', 'r', 'a', 'i', 'n', '}', '(', '\\\\', 'v', 'e', 'c', '{', 'w', '}', ',', 'b', ')', '$', ' ', ' ', ' ', 'P', 'l', 'o', 't', 't', 'i', 'n', 'g', ' ', 'a', ' ', 'l', 'e', 'a', 'r', 'n', 'i', 'n', 'g', ' ', 'c', 'u', 'r', 'v', 'e', ' ', 'b', 'y', ' ', 't', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ' ', 'd', 'i', 'f', 'f', 'e', 'r', 'e', 'n', 't', ' ', 'm', 'o', 'd', 'e', 'l', 's', ' ', 'b', 'a', 's', 'e', 'd', ' ', 'o', 'n', ' ', 'd', 'i', 'f', 'f', 'e', 'r', 'e', 'n', 't', ' ', 's', 'u', 'b', 's', 'e', 't', 's', ' ', 'o', 'f', ' ', 't', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ' ', 'd', 'a', 't', 'a', ' ', 'i', 's', ' ', 'c', 'o', 'm', 'p', 'u', 't', 'a', 't', 'i', 'o', 'n', 'a', 'l', 'l', 'y', ' ', 'e', 'x', 'p', 'e', 'n', 's', 'i', 'v', 'e', ',', ' ', 's', 'o', ' ', 'i', 'n', ' ', 'p', 'r', 'a', 'c', 't', 'i', 'c', 'e', ' ', 'i', 't', ' ', 'i', 's', 'n', \"'\", 't', ' ', 'd', 'o', 'n', 'e', ' ', 't', 'h', 'a', 't', ' ', 'o', 'f', 't', 'e', 'n', '.', ' ', 'B', 'u', 't', ',', ' ', 'i', 't', \"'\", 's', ' ', 'a', ' ', 'g', 'o', 'o', 'd', ' ', 'm', 'e', 'n', 't', 'a', 'l', ' ', 'v', 'i', 's', 'u', 'a', 'l', 'i', 'z', 'a', 't', 'i', 'o', 'n', '.', ' ', '#', '#', '#', '#', ' ', 'S', 'l', 'i', 'd', 'e', 's', ' ', '!', 'S', 'c', 'r', 'e', 'e', 'n', 's', 'h', 'o', 't', ' ', '2', '0', '2', '3', '-', '1', '0', '-', '1', '7', ' ', 'a', 't', ' ', '5', '.', '4', '0', '.', '0', '4', ' ', 'P', 'M', '.', 'p', 'n', 'g', ' ', ' ', ' ', ' ', '#', '#', '#', ' ', 'H', 'i', 'g', 'h', ' ', 'B', 'i', 'a', 's', ' ', 'E', 'x', 'a', 'm', 'p', 'l', 'e', ' ', 'I', 'f', ' ', 'a', ' ', 'l', 'e', 'a', 'r', 'n', 'i', 'n', 'g', ' ', 'a', 'l', 'g', 'o', 'r', 'i', 't', 'h', 'm', ' ', 's', 'u', 'f', 'f', 'e', 'r', 's', ' ', 'f', 'r', 'o', 'm', ' ', 'h', 'i', 'g', 'h', ' ', 'b', 'i', 'a', 's', ',', ' ', 'g', 'e', 't', 't', 'i', 'n', 'g', ' ', 'm', 'o', 'r', 'e', ' ', 't', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ' ', 'd', 'a', 't', 'a', ' ', 'w', 'i', 'l', 'l', ' ', 'n', 'o', 't', ' ', '(', 'b', 'y', ' ', 'i', 't', 's', 'e', 'l', 'f', ')', ' ', 'h', 'e', 'l', 'p', ' ', 't', 'h', 'a', 't', ' ', 'm', 'u', 'c', 'h', '.', ' ', '#', '#', '#', '#', ' ', 'S', 'l', 'i', 'd', 'e', ' ', 'f', 'o', 'r', ' ', 'H', 'i', 'g', 'h', ' ', 'B', 'i', 'a', 's', ' ', '!', 'S', 'c', 'r', 'e', 'e', 'n', 's', 'h', 'o', 't', ' ', '2', '0', '2', '3', '-', '1', '0', '-', '1', '7', ' ', 'a', 't', ' ', '5', '.', '4', '4', '.', '2', '9', ' ', 'P', 'M', '.', 'p', 'n', 'g', ' ', ' ', '#', '#', '#', ' ', 'H', 'i', 'g', 'h', ' ', 'V', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', 'E', 'x', 'a', 'm', 'p', 'l', 'e', ' ', 'I', 'f', ' ', 'a', ' ', 'l', 'e', 'a', 'r', 'n', 'i', 'n', 'g', ' ', 'a', 'l', 'g', 'o', 'r', 'i', 't', 'h', 'm', ' ', 's', 'u', 'f', 'f', 'e', 'r', 's', ' ', 'f', 'r', 'o', 'm', ' ', 'h', 'i', 'g', 'h', ' ', 'v', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ',', ' ', 'g', 'e', 't', 't', 'i', 'n', 'g', ' ', 'm', 'o', 'r', 'e', ' ', 't', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ' ', 'd', 'a', 't', 'a', ' ', 'i', 's', ' ', 'l', 'i', 'k', 'e', 'l', 'y', ' ', 't', 'o', ' ', 'h', 'e', 'l', 'p', '.', ' ', '#', '#', '#', '#', ' ', 'S', 'l', 'i', 'd', 'e', ' ', 'f', 'o', 'r', ' ', 'H', 'i', 'g', 'h', ' ', 'V', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', '!', 'S', 'c', 'r', 'e', 'e', 'n', 's', 'h', 'o', 't', ' ', '2', '0', '2', '3', '-', '1', '0', '-', '1', '7', ' ', 'a', 't', ' ', '5', '.', '4', '9', '.', '4', '5', ' ', 'P', 'M', '.', 'p', 'n', 'g', ' ', ' ', ' ', ' ', '#', '#', ' ', 'D', 'e', 'c', 'i', 'd', 'i', 'n', 'g', ' ', 'w', 'h', 'a', 't', ' ', 't', 'o', ' ', 't', 'r', 'y', ' ', 'n', 'e', 'x', 't', ' ', 'r', 'e', 'v', 'i', 's', 'i', 't', 'e', 'd', ' ', '#', '#', '#', ' ', 'E', 'x', 'a', 'm', 'p', 'l', 'e', 's', ' ', 'w', 'h', 'e', 'n', ' ', 'D', 'e', 'b', 'u', 'g', 'g', 'i', 'n', 'g', ' ', 'a', 'n', ' ', 'A', 'l', 'g', 'o', 'r', 'i', 't', 'h', 'm', ':', ' ', 'G', 'e', 't', ' ', 'm', 'o', 'r', 'e', ' ', 't', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ' ', 'e', 'x', 'a', 'm', 'p', 'l', 'e', 's', ':', ' ', 'f', 'i', 'x', 'e', 's', ' ', 'h', 'i', 'g', 'h', ' ', 'v', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', 'T', 'r', 'y', ' ', 's', 'm', 'a', 'l', 'l', 'e', 'r', ' ', 's', 'e', 't', ' ', 'o', 'f', ' ', 'f', 'e', 'a', 't', 'u', 'r', 'e', 's', ':', ' ', 'f', 'i', 'x', 'e', 's', ' ', 'h', 'i', 'g', 'h', ' ', 'v', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', 'T', 'r', 'y', ' ', 'g', 'e', 't', 't', 'i', 'n', 'g', ' ', 'a', 'd', 'd', 'i', 't', 'i', 'o', 'n', 'a', 'l', ' ', 'f', 'e', 'a', 't', 'u', 'r', 'e', 's', ':', ' ', 'f', 'i', 'x', 'e', 's', ' ', 'h', 'i', 'g', 'h', ' ', 'b', 'i', 'a', 's', ' ', '\\t', 'E', 'x', 'a', 'm', 'p', 'l', 'e', 's', ';', ' ', 'a', 'n', ' ', 'a', 'l', 'g', 'o', 'r', 'i', 't', 'h', 'm', ' ', 't', 'h', 'a', 't', ' ', 'l', 'a', 'c', 'k', 's', ' ', 'i', 'n', 'f', 'o', 'r', 'm', 'a', 't', 'i', 'o', 'n', ' ', 'w', 'o', 'n', 't', ' ', 'e', 'v', 'e', 'n', ' ', 'd', 'o', ' ', 'w', 'e', 'l', 'l', ' ', 'o', 'n', ' ', 't', 'h', 'e', ' ', 't', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ' ', 's', 'e', 't', ' ', 'A', 'd', 'd', 'i', 'n', 'g', ' ', 'p', 'o', 'l', 'y', 'n', 'o', 'm', 'i', 'a', 'l', ' ', 'f', 'e', 'a', 't', 'u', 'r', 'e', 's', ' ', '(', '$', 'x', '_', '1', '^', '2', ',', 'X', '_', '2', '^', '2', ',', ' ', 'e', 't', 'c', '$', ')', ':', ' ', 'F', 'i', 'x', 'e', 's', ' ', 'h', 'i', 'g', 'h', ' ', 'b', 'i', 'a', 's', ' ', 'D', 'e', 'c', 'r', 'e', 'a', 's', 'i', 'n', 'g', ' ', '$', '\\\\', 'l', 'a', 'm', 'b', 'd', 'a', '$', ' ', 'F', 'i', 'x', 'e', 's', ' ', 'h', 'i', 'g', 'h', ' ', 'b', 'i', 'a', 's', ' ', 'I', 'n', 'c', 'r', 'e', 'a', 's', 'i', 'n', 'g', ' ', '$', '\\\\', 'l', 'a', 'm', 'b', 'd', 'a', '$', ' ', 'F', 'i', 'x', 'e', 's', ' ', 'h', 'i', 'g', 'h', ' ', 'v', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', '\\t', 'F', 'o', 'r', 'c', 'e', 's', ' ', 't', 'h', 'e', ' ', 'a', 'l', 'g', 'o', 'r', 'i', 't', 'h', 'm', ' ', 't', 'o', ' ', 'f', 'o', 'r', 'c', 'e', ' ', 'a', ' ', 's', 'm', 'o', 'o', 't', 'h', 'e', 'r', ' ', 'f', 'u', 'n', 'c', 't', 'i', 'o', 'n', '.', ' ', ' ', '*', '*', 'N', 'o', 't', 'e', '!', '*', '*', ' ', 'D', 'o', 'n', \"'\", 't', ' ', 'r', 'a', 'n', 'd', 'o', 'm', 'l', 'y', ' ', 't', 'h', 'r', 'o', 'w', ' ', 'a', 'w', 'a', 'y', ' ', 't', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ' ', 'e', 'x', 'a', 'm', 'p', 'l', 'e', 's', ' ', 'j', 'u', 's', 't', ' ', 't', 'o', ' ', 'f', 'i', 'x', ' ', 'a', ' ', 'h', 'i', 'g', 'h', ' ', 'b', 'i', 'a', 's', ' ', 'p', 'r', 'o', 'b', 'l', 'e', 'm', '.', ' ', '#', '#', '#', ' ', 'T', 'a', 'k', 'e', 'a', 'w', 'a', 'y', ' ', '#', 'm', 'e', 'r', 'g', 'e', ' ', 'w', 'i', 't', 'h', ' ', 'B', 'i', 'a', 's', ' ', 'a', 'n', 'd', ' ', 'V', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', '?', ' ', 'I', 'f', ' ', 'y', 'o', 'u', 'r', ' ', 'a', 'l', 'g', 'o', 'r', 'i', 't', 'h', 'm', ' ', 'h', 'a', 's', ' ', 'h', 'i', 'g', 'h', ' ', 'v', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ',', ' ', 't', 'r', 'y', ' ', 's', 'i', 'm', 'p', 'l', 'i', 'f', 'y', 'i', 'n', 'g', ' ', 'y', 'o', 'u', 'r', ' ', 'm', 'o', 'd', 'e', 'l', ' ', 'o', 'r', ' ', 'g', 'e', 't', 't', 'i', 'n', 'g', ' ', 'm', 'o', 'r', 'e', ' ', 't', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ' ', 'd', 'a', 't', 'a', '.', ' ', 'S', 'i', 'm', 'p', 'l', 'i', 'f', 'i', 'c', 'a', 't', 'i', 'o', 'n', ' ', 'c', 'a', 'n', ' ', 'm', 'e', 'a', 'n', ' ', 'a', ' ', 's', 'm', 'a', 'l', 'l', 'e', 'r', ' ', 's', 'e', 't', ' ', 'o', 'f', ' ', 'f', 'e', 'a', 't', 'u', 'r', 'e', 's', ' ', 'o', 'r', ' ', 'a', 'n', ' ', 'i', 'n', 'c', 'r', 'e', 'a', 's', 'e', 'd', ' ', 'r', 'e', 'g', 'u', 'l', 'a', 'r', 'i', 'z', 'a', 't', 'i', 'o', 'n', ' ', 'I', 'f', ' ', 'y', 'o', 'u', 'r', ' ', 'a', 'l', 'g', 'o', 'r', 'i', 't', 'h', 'm', ' ', 'h', 'a', 's', ' ', 'h', 'i', 'g', 'h', ' ', 'b', 'i', 'a', 's', ',', ' ', 't', 'h', 'a', 't', ' ', 'i', 's', ' ', 't', 'o', ' ', 's', 'a', 'y', ' ', 'i', 't', 's', ' ', 'n', 'o', 't', ' ', 'e', 'v', 'e', 'n', ' ', 'd', 'o', 'i', 'n', 'g', ' ', 'w', 'e', 'l', 'l', ' ', 'o', 'n', ' ', 't', 'h', 'e', ' ', 't', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ' ', 's', 'e', 't', ',', ' ', 'y', 'o', 'u', ' ', 'm', 'a', 'i', 'n', 'l', 'y', ' ', 'n', 'e', 'e', 'd', ' ', 't', 'o', ' ', 'm', 'a', 'k', 'e', ' ', 'y', 'o', 'u', 'r', ' ', 'm', 'o', 'd', 'e', 'l', ' ', 'm', 'o', 'r', 'e', ' ', 'p', 'o', 'w', 'e', 'r', 'f', 'u', 'l', ' ', 'a', 'n', 'd', ' ', 'f', 'l', 'e', 'x', 'i', 'b', 'l', 'e', ' ', 't', 'o', ' ', 'f', 'i', 't', ' ', 'm', 'o', 'r', 'e', ' ', 'c', 'o', 'm', 'p', 'l', 'e', 'x', ' ', 'f', 'u', 'n', 'c', 't', 'i', 'o', 'n', 's', '.', ' ', 'T', 'o', ' ', 'd', 'o', ' ', 's', 'o', ' ', 'y', 'o', 'u', ' ', 'c', 'a', 'n', ' ', 'g', 'i', 'v', 'e', ' ', 'i', 't', ' ', 'a', 'd', 'd', 'i', 't', 'i', 'o', 'n', 'a', 'l', ' ', 'f', 'e', 'a', 't', 'u', 'r', 'e', 's', ',', ' ', 'a', 'd', 'd', ' ', 'p', 'o', 'l', 'y', 'n', 'o', 'm', 'i', 'a', 'l', ' ', 'f', 'e', 'a', 't', 'u', 'r', 'e', 's', ',', ' ', 'o', 'r', ' ', 'd', 'e', 'c', 'r', 'e', 'a', 's', 'e', ' ', '$', '\\\\', 'l', 'a', 'm', 'b', 'd', 'a', '$', ' ', ' ', ' ', '#', '#', ' ', 'B', 'i', 'a', 's', '/', 'V', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', 'a', 'n', 'd', ' ', 'N', 'e', 'u', 'r', 'a', 'l', ' ', 'N', 'e', 't', 'w', 'o', 'r', 'k', 's', ' ', '#', '#', '#', ' ', 'T', 'h', 'e', ' ', 'b', 'i', 'a', 's', ' ', 'v', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', 't', 'r', 'a', 'd', 'e', 'o', 'f', 'f', ' ', 'S', 'i', 'm', 'p', 'l', 'e', ' ', 'm', 'o', 'd', 'e', 'l', ' ', '=', ' ', 'h', 'i', 'g', 'h', ' ', 'b', 'i', 'a', 's', ' ', 'C', 'o', 'm', 'p', 'l', 'e', 'x', ' ', 'm', 'o', 'd', 'e', 'l', ' ', '=', ' ', 'h', 'i', 'g', 'h', ' ', 'v', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', 'B', 'e', 'f', 'o', 'r', 'e', ' ', 'n', 'e', 'u', 'r', 'a', 'l', ' ', 'n', 'e', 't', 'w', 'o', 'r', 'k', 's', ',', ' ', 'w', 'e', ' ', 'h', 'a', 'd', ' ', 't', 'o', ' ', 'w', 'o', 'r', 'r', 'y', ' ', 'a', 'b', 'o', 'u', 't', ' ', 'b', 'a', 'l', 'a', 'n', 'c', 'i', 'n', 'g', ' ', 't', 'h', 'i', 's', ' ', 'c', 'o', 'm', 'p', 'l', 'e', 'x', 'i', 't', 'y', ' ', 'b', 'e', 't', 'w', 'e', 'e', 'n', ' ', 'b', 'i', 'a', 's', ' ', 'a', 'n', 'd', ' ', 'v', 'a', 'r', 'i', 'a', 'n', 'c', 'e', '.', ' ', 'W', 'i', 't', 'h', ' ', 'n', 'e', 'u', 'r', 'a', 'l', ' ', 'n', 'e', 't', 'w', 'o', 'r', 'k', 's', ' ', 'w', 'e', ' ', 'n', 'o', 'w', ' ', 'a', 'r', 'e', ' ', 'm', 'o', 's', 't', 'l', 'y', ' ', 'w', 'o', 'r', 'r', 'i', 'e', 'd', ' ', 'a', 'b', 'o', 'u', 't', ' ', 'h', 'i', 'g', 'h', ' ', 'v', 'a', 'r', 'i', 'a', 'n', 'c', 'e', '.', ' ', ' ', '#', '#', '#', ' ', 'N', 'e', 'u', 'r', 'a', 'l', ' ', 'N', 'e', 't', 'w', 'o', 'r', 'k', 's', ' ', 'a', 'n', 'd', ' ', 'b', 'i', 'a', 's', ' ', 'v', 'a', 'r', 'i', 'a', 'n', 'c', 'e', ' ', 'L', 'a', 'r', 'g', 'e', ' ', 'N', 'e', 'u', 'r', 'a', 'l', ' ', 'N', 'e', 't', 'w', 'o', 'r', 'k', '|', 'n', 'e', 'u', 'r', 'a', 'l', ' ', 'n', 'e', 't', 'w', 'o', 'r', 'k', 's', ' ', 'a', 'r', 'e', ' ', 'l', 'o', 'w', ' ', 'b', 'i', 'a', 's', ' ', 'm', 'a', 'c', 'h', 'i', 'n', 'e', 's', '.', ' ', 'I', 'f', ' ', 'y', 'o', 'u', ' ', 'm', 'a', 'k', 'e', ' ', 'y', 'o', 'u', 'r', ' ', 'n', 'e', 'u', 'r', 'a', 'l', ' ', 'n', 'e', 't', 'w', 'o', 'r', 'k', ' ', 'l', 'a', 'r', 'g', 'e', ' ', 'e', 'n', 'o', 'u', 'g', 'h', ' ', 'y', 'o', 'u', ' ', 'c', 'a', 'n', ' ', 'a', 'l', 'm', 'o', 's', 't', ' ', 'a', 'l', 'w', 'a', 'y', 's', ' ', 'f', 'i', 't', ' ', 'y', 'o', 'u', 'r', ' ', 't', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ' ', 's', 'e', 't', ' ', 'w', 'e', 'l', 'l', '.', ' ', '#', '#', '#', '#', ' ', 'R', 'e', 'c', 'i', 'p', 'e', ' ', 'f', 'o', 'r', ' ', 'd', 'e', 'c', 'r', 'e', 'a', 's', 'i', 'n', 'g', ' ', 'b', 'i', 'a', 's', ' ', 'w', 'i', 't', 'h', ' ', 'a', ' ', 'n', 'e', 'u', 'r', 'a', 'l', ' ', 'n', 'e', 't', 'w', 'o', 'r', 'k', ' ', '1', '.', ' ', 'T', 'r', 'a', 'i', 'n', ' ', 'a', ' ', 'n', 'e', 'u', 'r', 'a', 'l', ' ', 'n', 'e', 't', 'w', 'o', 'r', 'k', ' ', '2', '.', ' ', ' ', 'I', 'f', ' ', 't', 'h', 'e', ' ', 't', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ' ', 's', 'e', 't', ' ', 'e', 'r', 'r', 'o', 'r', ' ', '$', 'J', '_', '{', 't', 'r', 'a', 'i', 'n', '}', '(', '\\\\', 'v', 'e', 'c', '{', 'w', '}', ',', 'b', ')', '$', ' ', 'i', 's', ' ', 'h', 'i', 'g', 'h', ' ', 'r', 'e', 'l', 'a', 't', 'i', 'v', 'e', ' ', 't', 'o', ' ', 'y', 'o', 'u', 'r', ' ', 'b', 'a', 's', 'e', 'l', 'i', 'n', 'e', ',', ' ', 'i', 'n', 'c', 'r', 'e', 'a', 's', 'e', ' ', 't', 'h', 'e', ' ', 's', 'i', 'z', 'e', ' ', 'o', 'f', ' ', 't', 'h', 'e', ' ', 'n', 'e', 'u', 'r', 'a', 'l', ' ', 'n', 'e', 't', 'w', 'o', 'r', 'k', ' ', 'b', 'y', ' ', 'a', 'd', 'd', 'i', 'n', 'g', ' ', 'h', 'i', 'd', 'd', 'e', 'n', ' ', 'l', 'a', 'y', 'e', 'r', 's', '.', ' ', '3', '.', ' ', 'O', 'n', 'c', 'e', ' ', '$', 'J', '_', '{', 't', 'r', 'a', 'i', 'n', '}', '(', '\\\\', 'v', 'e', 'c', '{', 'w', '}', ',', 'b', ')', '$', ' ', 'i', 's', ' ', 'l', 'o', 'w', ' ', 'e', 'n', 'o', 'u', 'g', 'h', ',', ' ', 's', 'e', 'e', ' ', 'i', 'f', ' ', 'i', 't', ' ', 'd', 'o', 'e', 's', ' ', 'w', 'e', 'l', 'l', ' ', 'o', 'n', ' ', 't', 'h', 'e', ' ', 'c', 'r', 'o', 's', 's', ' ', 'v', 'a', 'l', 'i', 'd', 'a', 't', 'i', 'o', 'n', ' ', 's', 'e', 't', ' ', '4', '.', ' ', ' ', 'I', 'f', ' ', 't', 'h', 'e', ' ', 'c', 'r', 'o', 's', 's', ' ', 'v', 'a', 'l', 'i', 'd', 'a', 't', 'i', 'o', 'n', ' ', 's', 'e', 't', ' ', '$', 'J', '_', '{', 'c', 'v', '}', '(', '\\\\', 'v', 'e', 'c', '{', 'w', '}', ',', 'b', ')', '$', ' ', 'i', 's', ' ', 't', 'o', 'o', ' ', 'h', 'i', 'g', 'h', ',', ' ', 'a', 'd', 'd', ' ', 'm', 'o', 'r', 'e', ' ', 'd', 'a', 't', 'a', ',', ' ', 't', 'h', 'e', 'n', ' ', 't', 'e', 's', 't', ' ', 'a', 'g', 'a', 'i', 'n', ' ', 'f', 'r', 'o', 'm', ' ', 's', 't', 'e', 'p', ' ', '2', '.', ' ', '5', '.', ' ', 'R', 'e', 'p', 'e', 'a', 't', ' ', 'u', 'n', 't', 'i', 'l', ' ', '$', 'J', '_', '{', 'c', 'v', '}', '(', '\\\\', 'v', 'e', 'c', '{', 'w', '}', ',', 'b', ')', '$', ' ', ' ', 'i', 's', ' ', 'l', 'o', 'w', ' ', 'e', 'n', 'o', 'u', 'g', 'h', ' ', 'f', 'o', 'r', ' ', 'y', 'o', 'u', 'r', ' ', 'l', 'i', 'k', 'i', 'n', 'g', '.']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# If the note is longer than max_tokens, split the note into each header\n",
    "summary = \"No summary yet... initialized value\"\n",
    "\n",
    "if num_tokens <= MAX_TOKENS:\n",
    "    print(f\"üòé All good:\\n{num_tokens} tokens less than {MAX_TOKENS} maximum\")\n",
    "    summary = summarize_string(prepared_note, summarizer, TOKENIZER)\n",
    "else:\n",
    "    print(f\"‚õëÔ∏è {num_tokens} is greater than {MAX_TOKENS}, we need to split up this note\")\n",
    "\n",
    "    note_sections = init_note_sections(prepared_note)\n",
    "    longest_header = find_longest_header(note_sections[1])\n",
    "    print(f\"üèÑ Longest header: üü©{longest_header}üü©\")\n",
    "    preamble = [str(note_sections[0])]\n",
    "    body = str(note_sections[1])\n",
    "    new_sections = split_at_header(body, longest_header[:-1])\n",
    "\n",
    "    print(f\"ü´Ä there are {len(new_sections)} new sections after splitting the body\")\n",
    "    all_sections = preamble\n",
    "    for section in new_sections:\n",
    "        all_sections.extend(section)\n",
    "    \n",
    "\n",
    "\n",
    "    subsection_summaries = []\n",
    "    print(\"SECTIONS:\")\n",
    "    print(all_sections)\n",
    "    # for i, section in enumerate(all_sections):\n",
    "    #     print(i)\n",
    "    #     print(section)\n",
    "#         subsection_summaries += [summarize_string(section, summarizer, TOKENIZER)]\n",
    "#     summary = \"\\n\".join(subsection_summaries)\n",
    "\n",
    "# print(\"üíÖ Final, summarized note:\\n\", summary)\n",
    "    \n",
    "    \n",
    "# if no headers are left and the note is longer than max_tokens, split at the middle line break\n",
    "# Keep splitting each sub-section at the middle line break until there is a single line\n",
    "# If the single line is longer than max_tokens, split the line at the middle char\n",
    "# Keep splitting each sub line at the middle char until it is less than max_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send each section to the summarizer\n",
    "# perhaps if the section is too small (like the hashtag preamble in some notes, just dont send at all)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
