{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "from functions import summarize_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_note = \"/Users/joachimpfefferkorn/repos/daily_note_organizer/test_media/2023-10-17.md\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_md(md_content: str):\n",
    "    clean_note = md_content\n",
    "    clean_note = clean_note.replace('- [x]', 'Completed:').replace('- [ ]','To Do:')\n",
    "    clean_note = clean_note.replace('[[', '').replace(']]','')\n",
    "    clean_note = clean_note.replace('![[', 'Image file:')\n",
    "    return clean_note\n",
    "\n",
    "def prepare_note(md_path):\n",
    "    with open(md_path, 'r') as note:\n",
    "        md_content = note.read()\n",
    "        cleaned_md = clean_md(md_content)\n",
    "        return cleaned_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_note = prepare_note(str(long_note))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joachimpfefferkorn/repos/daily_note_organizer/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"knkarthick/MEETING_SUMMARY\"\n",
    "summarizer = pipeline(\"summarization\", model=model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2718 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2718\n"
     ]
    }
   ],
   "source": [
    "class Section:\n",
    "    def __init__(self, content):\n",
    "        self.content = content\n",
    "        self.tokens = tokenizer(content)\n",
    "        self.num_tokens = len(self.tokens['input_ids']) #TODO there is a direct way to get these, slight hack for now\n",
    "\n",
    "print(Section(prepared_note).num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(section: Section, amount: int):\n",
    "    lines = np.asarray(section.content.split(\"\\n\"), dtype=str)\n",
    "\n",
    "    sub_arrays = np.array_split(lines, amount)\n",
    "\n",
    "    sections = []\n",
    "    #Make each sub array a string\n",
    "    for array in sub_arrays:\n",
    "        text = \"\\n\".join(array)\n",
    "    # then a section\n",
    "    #add each section to a list\n",
    "        sections.append(Section(text))\n",
    "    return sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.Section'>\n",
      "🐘 Original section must be split by 2! Section tokens: 2718 Max model length: 1024\n",
      "🪸 New sections updated with split subsections, 🔃 recursively split is recursing\n",
      "⚔️ Splitting subsection 0\n",
      "🐘 Original section must be split by 3! Section tokens: 1394 Max model length: 1024\n",
      "🪸 New sections updated with split subsections, 🔃 recursively split is recursing\n",
      "⚔️ Splitting subsection 0\n",
      "🦋 Section is small enough!\n",
      "🏄 Small enough section added to new_sections\n",
      "⚔️ Splitting subsection 1\n",
      "🦋 Section is small enough!\n",
      "🏄 Small enough section added to new_sections\n",
      "⚔️ Splitting subsection 2\n",
      "🦋 Section is small enough!\n",
      "🏄 Small enough section added to new_sections\n",
      "⚔️ Splitting subsection 1\n",
      "🐘 Original section must be split by 3! Section tokens: 1325 Max model length: 1024\n",
      "🪸 New sections updated with split subsections, 🔃 recursively split is recursing\n",
      "⚔️ Splitting subsection 0\n",
      "🦋 Section is small enough!\n",
      "🏄 Small enough section added to new_sections\n",
      "⚔️ Splitting subsection 1\n",
      "🦋 Section is small enough!\n",
      "🏄 Small enough section added to new_sections\n",
      "⚔️ Splitting subsection 2\n",
      "🦋 Section is small enough!\n",
      "🏄 Small enough section added to new_sections\n",
      "[<__main__.Section object at 0x132969a50>, <__main__.Section object at 0x131cedc10>, <__main__.Section object at 0x131cee110>, <__main__.Section object at 0x131cee090>, <__main__.Section object at 0x127e94dd0>, <__main__.Section object at 0x1317fbe10>]\n"
     ]
    }
   ],
   "source": [
    "def biggest_section(sections):\n",
    "    biggest_section = Section(\"\")\n",
    "    for section in sections:\n",
    "        if section.num_tokens > biggest_section.num_tokens:\n",
    "            biggest_section = section\n",
    "    return biggest_section\n",
    "\n",
    "output_sections = []\n",
    "og_section = Section(prepared_note)\n",
    "def recursive_split(input_section, split_amount):\n",
    "\n",
    "    if input_section.num_tokens > tokenizer.model_max_length and split_amount < 99:\n",
    "        split_amount += 1\n",
    "        print(f\"🐘 Original section must be split by {split_amount}! Section tokens: {input_section.num_tokens} Max model length: {tokenizer.model_max_length}\")\n",
    "        new_sections = split(og_section, split_amount)\n",
    "        print(f\"🪸 New sections updated with split subsections, 🔃 recursively split is recursing\")\n",
    "        for i, subsection in enumerate(new_sections):\n",
    "            print(f\"⚔️ Splitting subsection {i}\")\n",
    "            recursive_split(subsection, split_amount)\n",
    "    else:\n",
    "        print(f\"🦋 Section is small enough!\")\n",
    "        print(f\"🏄 Small enough section added to new_sections\")\n",
    "        output_sections.append(input_section)\n",
    "        return 0\n",
    "\n",
    "\n",
    "\n",
    "split_amount = 1\n",
    "input_sections = split(Section(prepared_note), split_amount)\n",
    "print(type(input_sections[0]))\n",
    "# for section in sections:\n",
    "#     print(section.content)\n",
    "\n",
    "recursive_split(og_section, split_amount)\n",
    "print(output_sections)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📇 Summarizing section 0\n",
      "📇 Summarizing section 1\n",
      "📇 Summarizing section 2\n",
      "📇 Summarizing section 3\n",
      "📇 Summarizing section 4\n",
      "📇 Summarizing section 5\n"
     ]
    }
   ],
   "source": [
    "summary = \"\"\n",
    "for i, section in enumerate(output_sections):\n",
    "    print(f\"📇 Summarizing section {i}\")\n",
    "    summary += summarize_string(section.content, summarizer, tokenizer)[0]['summary_text'] + \"\\n\"\n",
    "    #summary += summarize_string(section.content, summarizer, tokenizer)['summary_text'] + \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In Machine Learning Specialization Advanced Machine Learning Algorithms, Week 3, the author explains how to fix the first time the model doesn't work.\n",
      "If the regularization parameter is large, then the learning curve gets oversimplified. If it's low, the learning algorithm underfits the data. The gap between baseline and training error shows high bias. The human level performance is 10.6%, whereas the speech recognition program's is 14.8%.\n",
      "If a learning algorithm suffers from high bias, getting more training data will help. If it does not, you need to make it more powerful and flexible to fit more complex functions.\n",
      "In Machine Learning Specialization Advanced Machine Learning Algorithms, Week 3, the author explains how to fix the first time the model doesn't work.\n",
      "If the regularization parameter is large, then the learning curve gets oversimplified. If it's low, the learning algorithm underfits the data. The gap between baseline and training error shows high bias. The human level performance is 10.6%, whereas the speech recognition program's is 14.8%.\n",
      "If a learning algorithm suffers from high bias, getting more training data will help. If it does not, you need to make it more powerful and flexible to fit more complex functions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
